22-10-20 10:07:39.050 :   task: swinir_denoising_sr_x2_charbonnier
  model: plain
  gpu_ids: [0]
  dist: False
  scale: 2
  n_channels: 1
  path:[
    root: superresolution
    pretrained_netG: None
    pretrained_netE: None
    task: superresolution\swinir_denoising_sr_x2_charbonnier
    log: superresolution\swinir_denoising_sr_x2_charbonnier
    options: superresolution\swinir_denoising_sr_x2_charbonnier\options
    models: superresolution\swinir_denoising_sr_x2_charbonnier\models
    images: superresolution\swinir_denoising_sr_x2_charbonnier\images
    pretrained_optimizerG: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr
      dataroot_H: trainsets/trainH
      dataroot_L: trainsets/X4_lq_x2_lr
      H_size: 96
      dataloader_shuffle: True
      dataloader_num_workers: 12
      dataloader_batch_size: 1
      phase: train
      scale: 2
      n_channels: 1
    ]
    test:[
      name: test_dataset
      dataset_type: sr
      dataroot_H: testsets/set12
      dataroot_L: testsets/set12_valid_lq_x2_lr
      phase: test
      scale: 2
      n_channels: 1
    ]
  ]
  netG:[
    net_type: swinir
    upscale: 2
    in_chans: 1
    img_size: 48
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    upsampler: pixelshuffle
    resi_connection: 1conv
    init_type: default
    scale: 2
  ]
  train:[
    G_lossfn_type: charbonnier
    G_lossfn_weight: 1.0
    E_decay: 0.999
    G_optimizer_type: adam
    G_optimizer_lr: 0.0002
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_optimizer_reuse: True
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [250000, 400000, 450000, 475000, 500000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    G_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
  ]
  opt_path: options\swinir\train_swinir_sr_classical.json
  is_train: True
  merge_bn: False
  merge_bn_startpoint: -1
  find_unused_parameters: True
  num_gpu: 1
  rank: 0
  world_size: 1

22-10-20 10:07:39.069 : Number of train images: 800, iters: 800
22-10-20 10:07:40.938 : 
Networks name: SwinIR
Params number: 11748093
Net structure:
SwinIR(
  (conv_first): Conv2d(1, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (patch_embed): PatchEmbed(
    (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  )
  (patch_unembed): PatchUnEmbed()
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (1): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (2): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (3): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (4): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (5): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
  )
  (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  (conv_after_body): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_before_upsample): Sequential(
    (0): Conv2d(180, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
  )
  (upsample): Upsample(
    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): PixelShuffle(upscale_factor=2)
  )
  (conv_last): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

22-10-20 10:07:41.156 : 
 |  mean  |  min   |  max   |  std   || shape               
 |  0.007 | -0.332 |  0.333 |  0.193 | torch.Size([180, 1, 3, 3]) || conv_first.weight
 | -0.010 | -0.332 |  0.327 |  0.194 | torch.Size([180]) || conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.061 |  0.068 |  0.019 | torch.Size([225, 6]) || layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.084 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.077 |  0.086 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.090 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.090 |  0.082 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.087 |  0.085 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.079 |  0.079 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.077 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.089 |  0.078 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.091 |  0.087 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.bias
 |  0.001 | -0.057 |  0.071 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.088 |  0.095 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.083 |  0.074 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.084 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.082 |  0.096 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.083 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.085 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.075 |  0.083 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.101 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.082 |  0.090 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.bias
 |  0.000 | -0.075 |  0.069 |  0.019 | torch.Size([225, 6]) || layers.0.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.099 |  0.090 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.079 |  0.083 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.087 |  0.089 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.079 |  0.099 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.bias
 | -0.001 | -0.069 |  0.065 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.086 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.086 |  0.078 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.090 |  0.077 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.081 |  0.090 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.0.conv.weight
 | -0.001 | -0.025 |  0.025 |  0.014 | torch.Size([180]) || layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.062 |  0.056 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.083 |  0.081 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.084 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.090 |  0.093 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.099 |  0.089 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.bias
 | -0.000 | -0.067 |  0.065 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.086 |  0.083 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.084 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.091 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.092 |  0.087 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.067 |  0.073 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.090 |  0.092 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.075 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.094 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.084 |  0.093 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.064 |  0.058 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.087 |  0.081 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.091 |  0.085 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.086 |  0.091 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.081 |  0.098 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.bias
 |  0.000 | -0.063 |  0.063 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.086 |  0.092 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.082 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.095 |  0.089 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.092 |  0.089 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.066 |  0.078 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.086 |  0.082 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.079 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.079 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.090 |  0.090 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.1.conv.weight
 | -0.001 | -0.025 |  0.023 |  0.014 | torch.Size([180]) || layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.bias
 |  0.001 | -0.059 |  0.061 |  0.019 | torch.Size([225, 6]) || layers.2.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.084 |  0.078 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.098 |  0.076 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.084 |  0.093 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.084 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.bias
 |  0.001 | -0.064 |  0.059 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.085 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.079 |  0.091 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.080 |  0.079 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.090 |  0.079 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.067 |  0.063 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.100 |  0.093 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.085 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.077 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.086 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.069 |  0.072 |  0.019 | torch.Size([225, 6]) || layers.2.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.093 |  0.081 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.080 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.079 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.082 |  0.098 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.064 |  0.064 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.101 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.081 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.082 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.095 |  0.088 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.074 |  0.074 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.084 |  0.087 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.077 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.091 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.081 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.2.conv.weight
 | -0.000 | -0.024 |  0.024 |  0.014 | torch.Size([180]) || layers.2.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.062 |  0.059 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.092 |  0.093 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.074 |  0.093 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.091 |  0.079 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.077 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.bias
 | -0.000 | -0.059 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.077 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.090 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.083 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.087 |  0.096 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.063 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.087 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.078 |  0.089 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.081 |  0.085 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.090 |  0.095 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.057 |  0.058 |  0.019 | torch.Size([225, 6]) || layers.3.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.087 |  0.079 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.082 |  0.085 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.082 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.086 |  0.091 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.063 |  0.063 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.086 |  0.084 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.080 |  0.077 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.087 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.093 |  0.087 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.bias
 |  0.001 | -0.066 |  0.073 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.093 |  0.090 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.087 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.091 |  0.106 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.089 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.3.conv.weight
 |  0.001 | -0.025 |  0.025 |  0.015 | torch.Size([180]) || layers.3.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.bias
 | -0.001 | -0.067 |  0.057 |  0.019 | torch.Size([225, 6]) || layers.4.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.103 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.085 |  0.100 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.086 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.088 |  0.079 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.061 |  0.064 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.084 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.091 |  0.077 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.087 |  0.091 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.078 |  0.090 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.066 |  0.072 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.087 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.082 |  0.072 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.086 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.083 |  0.088 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.073 |  0.069 |  0.021 | torch.Size([225, 6]) || layers.4.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.090 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.079 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.082 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.084 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.071 |  0.070 |  0.021 | torch.Size([225, 6]) || layers.4.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.081 |  0.092 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.079 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.082 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.103 |  0.100 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.077 |  0.066 |  0.021 | torch.Size([225, 6]) || layers.4.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.084 |  0.083 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.083 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.090 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.078 |  0.082 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.4.conv.weight
 | -0.002 | -0.024 |  0.024 |  0.014 | torch.Size([180]) || layers.4.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.bias
 |  0.001 | -0.064 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.089 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.090 |  0.075 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.094 |  0.080 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.098 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.064 |  0.064 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.093 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.075 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.080 |  0.077 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.083 |  0.089 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.070 |  0.065 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.081 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.089 |  0.078 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.082 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.092 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.bias
 |  0.001 | -0.058 |  0.062 |  0.019 | torch.Size([225, 6]) || layers.5.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.089 |  0.080 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.082 |  0.085 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.088 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.083 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.bias
 |  0.001 | -0.077 |  0.063 |  0.019 | torch.Size([225, 6]) || layers.5.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.085 |  0.085 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.078 |  0.077 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.081 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.094 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.bias
 |  0.001 | -0.062 |  0.069 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.102 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.080 |  0.078 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.078 |  0.080 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.086 |  0.075 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.5.conv.weight
 | -0.002 | -0.024 |  0.024 |  0.014 | torch.Size([180]) || layers.5.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || norm.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || conv_after_body.weight
 | -0.001 | -0.025 |  0.025 |  0.014 | torch.Size([180]) || conv_after_body.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([64, 180, 3, 3]) || conv_before_upsample.0.weight
 | -0.001 | -0.025 |  0.025 |  0.015 | torch.Size([64]) || conv_before_upsample.0.bias
 |  0.000 | -0.042 |  0.042 |  0.024 | torch.Size([256, 64, 3, 3]) || upsample.0.weight
 |  0.000 | -0.042 |  0.041 |  0.024 | torch.Size([256]) || upsample.0.bias
 | -0.000 | -0.041 |  0.042 |  0.024 | torch.Size([1, 64, 3, 3]) || conv_last.weight
 |  0.030 |  0.030 |  0.030 |    nan | torch.Size([1]) || conv_last.bias

22-10-20 10:15:55.630 :   task: swinir_denoising_sr_x2_charbonnier
  model: plain
  gpu_ids: [0]
  dist: False
  scale: 2
  n_channels: 1
  path:[
    root: superresolution
    pretrained_netG: None
    pretrained_netE: None
    task: superresolution\swinir_denoising_sr_x2_charbonnier
    log: superresolution\swinir_denoising_sr_x2_charbonnier
    options: superresolution\swinir_denoising_sr_x2_charbonnier\options
    models: superresolution\swinir_denoising_sr_x2_charbonnier\models
    images: superresolution\swinir_denoising_sr_x2_charbonnier\images
    pretrained_optimizerG: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr
      dataroot_H: trainsets/trainH
      dataroot_L: trainsets/X4_lq_x2_lr
      H_size: 96
      dataloader_shuffle: True
      dataloader_num_workers: 12
      dataloader_batch_size: 1
      phase: train
      scale: 2
      n_channels: 1
    ]
    test:[
      name: test_dataset
      dataset_type: sr
      dataroot_H: testsets/set12
      dataroot_L: testsets/set12_valid_lq_x2_lr
      phase: test
      scale: 2
      n_channels: 1
    ]
  ]
  netG:[
    net_type: swinir
    upscale: 2
    in_chans: 1
    img_size: 48
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    upsampler: pixelshuffle
    resi_connection: 1conv
    init_type: default
    scale: 2
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    E_decay: 0.999
    G_optimizer_type: adam
    G_optimizer_lr: 0.0002
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_optimizer_reuse: True
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [250000, 400000, 450000, 475000, 500000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    G_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
  ]
  opt_path: options\swinir\train_swinir_sr_classical.json
  is_train: True
  merge_bn: False
  merge_bn_startpoint: -1
  find_unused_parameters: True
  num_gpu: 1
  rank: 0
  world_size: 1

22-10-20 10:15:55.644 : Number of train images: 800, iters: 800
22-10-20 10:15:57.344 : 
Networks name: SwinIR
Params number: 11748093
Net structure:
SwinIR(
  (conv_first): Conv2d(1, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (patch_embed): PatchEmbed(
    (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  )
  (patch_unembed): PatchUnEmbed()
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (1): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (2): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (3): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (4): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (5): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
  )
  (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  (conv_after_body): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_before_upsample): Sequential(
    (0): Conv2d(180, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
  )
  (upsample): Upsample(
    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): PixelShuffle(upscale_factor=2)
  )
  (conv_last): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

22-10-20 10:15:57.505 : 
 |  mean  |  min   |  max   |  std   || shape               
 |  0.004 | -0.333 |  0.333 |  0.189 | torch.Size([180, 1, 3, 3]) || conv_first.weight
 | -0.008 | -0.332 |  0.333 |  0.195 | torch.Size([180]) || conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.bias
 | -0.001 | -0.066 |  0.064 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.091 |  0.090 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.076 |  0.088 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.088 |  0.090 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.093 |  0.090 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.bias
 | -0.000 | -0.062 |  0.057 |  0.021 | torch.Size([225, 6]) || layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.087 |  0.092 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.088 |  0.077 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.077 |  0.080 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.093 |  0.088 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.061 |  0.070 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.084 |  0.092 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.082 |  0.086 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.091 |  0.079 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.096 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.067 |  0.069 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.091 |  0.082 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.077 |  0.077 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.083 |  0.080 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.088 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.bias
 |  0.000 | -0.067 |  0.071 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.091 |  0.083 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.082 |  0.077 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.091 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.085 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.bias
 |  0.001 | -0.071 |  0.056 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.090 |  0.085 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.078 |  0.077 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.090 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.089 |  0.082 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.0.conv.weight
 |  0.002 | -0.025 |  0.025 |  0.015 | torch.Size([180]) || layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.053 |  0.067 |  0.019 | torch.Size([225, 6]) || layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.091 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.083 |  0.075 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.083 |  0.092 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.084 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.bias
 |  0.001 | -0.062 |  0.070 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.085 |  0.092 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.080 |  0.076 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.083 |  0.089 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.082 |  0.088 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.066 |  0.055 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.085 |  0.083 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.083 |  0.085 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.086 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.084 |  0.093 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.067 |  0.073 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.089 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.094 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.086 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.084 |  0.079 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.062 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.092 |  0.085 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.093 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.080 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.086 |  0.089 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.064 |  0.063 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.094 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.083 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.079 |  0.080 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.080 |  0.093 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.1.conv.weight
 |  0.000 | -0.025 |  0.025 |  0.015 | torch.Size([180]) || layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.068 |  0.083 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.079 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.090 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.081 |  0.095 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.077 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.bias
 | -0.001 | -0.059 |  0.062 |  0.019 | torch.Size([225, 6]) || layers.2.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.089 |  0.080 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.076 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.087 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.079 |  0.087 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.061 |  0.071 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.084 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.082 |  0.087 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.088 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.085 |  0.087 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.071 |  0.069 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.101 |  0.090 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.077 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.085 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.084 |  0.093 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.068 |  0.069 |  0.021 | torch.Size([225, 6]) || layers.2.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.095 |  0.093 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.092 |  0.084 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.086 |  0.085 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.086 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.070 |  0.069 |  0.021 | torch.Size([225, 6]) || layers.2.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.090 |  0.083 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.076 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.078 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.083 |  0.093 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.2.conv.weight
 | -0.001 | -0.025 |  0.025 |  0.015 | torch.Size([180]) || layers.2.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.060 |  0.069 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.087 |  0.085 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.089 |  0.088 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.082 |  0.090 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.078 |  0.098 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.bias
 | -0.000 | -0.070 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.092 |  0.076 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.080 |  0.090 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.081 |  0.079 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.086 |  0.078 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.057 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.088 |  0.081 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.088 |  0.072 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.084 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.082 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.072 |  0.068 |  0.019 | torch.Size([225, 6]) || layers.3.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.086 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.079 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.088 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.094 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.059 |  0.072 |  0.021 | torch.Size([225, 6]) || layers.3.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.083 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.094 |  0.085 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.090 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.100 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.068 |  0.073 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.094 |  0.088 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.080 |  0.077 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.096 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.082 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.3.conv.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180]) || layers.3.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.063 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.084 |  0.084 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.075 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.089 |  0.085 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.092 |  0.088 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.bias
 | -0.000 | -0.080 |  0.064 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.088 |  0.095 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.081 |  0.083 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.094 |  0.085 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.082 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.076 |  0.069 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.085 |  0.095 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.074 |  0.089 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.084 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.087 |  0.082 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.085 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.088 |  0.092 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.081 |  0.084 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.085 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.085 |  0.087 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.bias
 |  0.000 | -0.069 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.088 |  0.096 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.085 |  0.087 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.089 |  0.078 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.088 |  0.079 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.060 |  0.055 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.083 |  0.088 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.084 |  0.083 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.091 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.082 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.4.conv.weight
 |  0.001 | -0.025 |  0.024 |  0.015 | torch.Size([180]) || layers.4.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.bias
 |  0.001 | -0.065 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.092 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.083 |  0.087 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.098 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.084 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.bias
 |  0.001 | -0.059 |  0.066 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.086 |  0.085 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.074 |  0.083 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.081 |  0.085 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.080 |  0.087 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.bias
 | -0.001 | -0.081 |  0.061 |  0.021 | torch.Size([225, 6]) || layers.5.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.090 |  0.083 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.082 |  0.087 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.088 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.086 |  0.087 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.bias
 |  0.001 | -0.060 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.085 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.077 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.088 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.102 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.065 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.077 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.084 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.086 |  0.092 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.081 |  0.079 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.065 |  0.063 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.081 |  0.095 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.084 |  0.091 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.085 |  0.085 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.086 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.5.conv.weight
 |  0.001 | -0.025 |  0.025 |  0.014 | torch.Size([180]) || layers.5.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || norm.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || conv_after_body.weight
 | -0.001 | -0.025 |  0.025 |  0.014 | torch.Size([180]) || conv_after_body.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([64, 180, 3, 3]) || conv_before_upsample.0.weight
 |  0.000 | -0.024 |  0.025 |  0.014 | torch.Size([64]) || conv_before_upsample.0.bias
 |  0.000 | -0.042 |  0.042 |  0.024 | torch.Size([256, 64, 3, 3]) || upsample.0.weight
 | -0.001 | -0.042 |  0.042 |  0.024 | torch.Size([256]) || upsample.0.bias
 | -0.000 | -0.042 |  0.042 |  0.023 | torch.Size([1, 64, 3, 3]) || conv_last.weight
 |  0.010 |  0.010 |  0.010 |    nan | torch.Size([1]) || conv_last.bias

22-10-20 10:16:56.510 : <epoch:  0, iter:     200, lr:2.000e-04> G_loss: 1.477e-01 
22-10-20 10:17:37.486 : <epoch:  0, iter:     400, lr:2.000e-04> G_loss: 1.378e-01 
22-10-20 10:18:18.525 : <epoch:  0, iter:     600, lr:2.000e-04> G_loss: 6.438e-02 
22-10-20 10:19:00.509 : <epoch:  0, iter:     800, lr:2.000e-04> G_loss: 1.143e-01 
22-10-20 10:19:59.649 : <epoch:  1, iter:   1,000, lr:2.000e-04> G_loss: 2.545e-02 
22-10-20 10:20:40.592 : <epoch:  1, iter:   1,200, lr:2.000e-04> G_loss: 9.677e-02 
22-10-20 10:21:23.765 : <epoch:  1, iter:   1,400, lr:2.000e-04> G_loss: 1.129e-01 
22-10-20 10:22:04.982 : <epoch:  1, iter:   1,600, lr:2.000e-04> G_loss: 9.673e-02 
22-10-20 10:23:02.957 : <epoch:  2, iter:   1,800, lr:2.000e-04> G_loss: 7.268e-02 
22-10-20 10:23:43.879 : <epoch:  2, iter:   2,000, lr:2.000e-04> G_loss: 7.249e-02 
22-10-20 10:24:25.726 : <epoch:  2, iter:   2,200, lr:2.000e-04> G_loss: 7.459e-02 
22-10-20 10:25:07.165 : <epoch:  2, iter:   2,400, lr:2.000e-04> G_loss: 9.287e-02 
22-10-20 10:26:06.056 : <epoch:  3, iter:   2,600, lr:2.000e-04> G_loss: 8.479e-02 
22-10-20 10:26:47.718 : <epoch:  3, iter:   2,800, lr:2.000e-04> G_loss: 8.640e-02 
22-10-20 10:27:30.262 : <epoch:  3, iter:   3,000, lr:2.000e-04> G_loss: 8.773e-02 
22-10-20 10:28:12.485 : <epoch:  3, iter:   3,200, lr:2.000e-04> G_loss: 7.950e-02 
22-10-20 10:29:13.799 : <epoch:  4, iter:   3,400, lr:2.000e-04> G_loss: 6.817e-02 
22-10-20 10:29:57.321 : <epoch:  4, iter:   3,600, lr:2.000e-04> G_loss: 1.338e-01 
22-10-20 10:30:40.329 : <epoch:  4, iter:   3,800, lr:2.000e-04> G_loss: 6.813e-02 
22-10-20 10:31:23.265 : <epoch:  4, iter:   4,000, lr:2.000e-04> G_loss: 5.571e-02 
22-10-20 10:32:24.302 : <epoch:  5, iter:   4,200, lr:2.000e-04> G_loss: 8.507e-02 
22-10-20 10:33:07.037 : <epoch:  5, iter:   4,400, lr:2.000e-04> G_loss: 8.311e-02 
22-10-20 10:33:49.912 : <epoch:  5, iter:   4,600, lr:2.000e-04> G_loss: 7.382e-02 
22-10-20 10:34:33.547 : <epoch:  5, iter:   4,800, lr:2.000e-04> G_loss: 7.578e-02 
22-10-20 10:35:36.652 : <epoch:  6, iter:   5,000, lr:2.000e-04> G_loss: 1.234e-01 
22-10-20 10:35:36.652 : Saving the model.
22-10-20 10:35:39.053 : ---1--> 01_0dB_x2_lr.png | 18.95dB
22-10-20 10:35:39.354 : ---2--> 02_0dB_x2_lr.png | 21.37dB
22-10-20 10:35:39.675 : ---3--> 03_0dB_x2_lr.png | 19.70dB
22-10-20 10:35:39.979 : ---4--> 04_0dB_x2_lr.png | 21.41dB
22-10-20 10:35:40.323 : ---5--> 05_0dB_x2_lr.png | 20.38dB
22-10-20 10:35:40.667 : ---6--> 06_0dB_x2_lr.png | 20.29dB
22-10-20 10:35:40.986 : ---7--> 07_0dB_x2_lr.png | 18.06dB
22-10-20 10:35:42.208 : ---8--> 08_0dB_x2_lr.png | 22.53dB
22-10-20 10:35:43.434 : ---9--> 09_0dB_x2_lr.png | 20.96dB
22-10-20 10:35:44.665 : --10--> 10_0dB_x2_lr.png | 20.23dB
22-10-20 10:35:45.895 : --11--> 11_0dB_x2_lr.png | 21.79dB
22-10-20 10:35:47.119 : --12--> 12_0dB_x2_lr.png | 20.66dB
22-10-20 10:35:47.377 : <epoch:  6, iter:   5,000, Average PSNR : 20.53dB

22-10-20 10:36:31.098 : <epoch:  6, iter:   5,200, lr:2.000e-04> G_loss: 8.126e-02 
22-10-20 10:37:14.909 : <epoch:  6, iter:   5,400, lr:2.000e-04> G_loss: 4.697e-02 
22-10-20 10:37:58.237 : <epoch:  6, iter:   5,600, lr:2.000e-04> G_loss: 8.618e-02 
22-10-20 10:39:00.839 : <epoch:  7, iter:   5,800, lr:2.000e-04> G_loss: 6.173e-02 
22-10-20 10:39:45.810 : <epoch:  7, iter:   6,000, lr:2.000e-04> G_loss: 5.398e-02 
22-10-20 10:40:30.230 : <epoch:  7, iter:   6,200, lr:2.000e-04> G_loss: 7.339e-02 
22-10-20 10:41:13.712 : <epoch:  7, iter:   6,400, lr:2.000e-04> G_loss: 7.108e-02 
22-10-20 10:42:16.837 : <epoch:  8, iter:   6,600, lr:2.000e-04> G_loss: 6.506e-02 
22-10-20 10:43:01.399 : <epoch:  8, iter:   6,800, lr:2.000e-04> G_loss: 5.678e-02 
22-10-20 10:43:45.242 : <epoch:  8, iter:   7,000, lr:2.000e-04> G_loss: 8.657e-02 
22-10-20 10:44:29.401 : <epoch:  8, iter:   7,200, lr:2.000e-04> G_loss: 6.979e-02 
22-10-20 10:45:31.751 : <epoch:  9, iter:   7,400, lr:2.000e-04> G_loss: 9.889e-02 
22-10-20 10:46:17.116 : <epoch:  9, iter:   7,600, lr:2.000e-04> G_loss: 1.005e-01 
22-10-20 10:47:01.233 : <epoch:  9, iter:   7,800, lr:2.000e-04> G_loss: 6.390e-02 
22-10-20 10:47:45.489 : <epoch:  9, iter:   8,000, lr:2.000e-04> G_loss: 1.281e-02 
22-10-20 10:48:49.481 : <epoch: 10, iter:   8,200, lr:2.000e-04> G_loss: 1.274e-01 
22-10-20 10:49:33.666 : <epoch: 10, iter:   8,400, lr:2.000e-04> G_loss: 4.946e-02 
22-10-20 10:50:17.898 : <epoch: 10, iter:   8,600, lr:2.000e-04> G_loss: 7.585e-02 
22-10-20 10:51:02.121 : <epoch: 10, iter:   8,800, lr:2.000e-04> G_loss: 6.342e-02 
22-10-20 10:52:05.703 : <epoch: 11, iter:   9,000, lr:2.000e-04> G_loss: 6.366e-02 
22-10-20 10:52:50.468 : <epoch: 11, iter:   9,200, lr:2.000e-04> G_loss: 7.194e-02 
22-10-20 10:53:35.192 : <epoch: 11, iter:   9,400, lr:2.000e-04> G_loss: 9.813e-02 
22-10-20 10:54:19.432 : <epoch: 11, iter:   9,600, lr:2.000e-04> G_loss: 7.039e-02 
22-10-20 10:55:23.137 : <epoch: 12, iter:   9,800, lr:2.000e-04> G_loss: 6.311e-02 
22-10-20 10:56:07.591 : <epoch: 12, iter:  10,000, lr:2.000e-04> G_loss: 4.119e-02 
22-10-20 10:56:07.591 : Saving the model.
22-10-20 10:56:09.866 : ---1--> 01_0dB_x2_lr.png | 20.87dB
22-10-20 10:56:10.143 : ---2--> 02_0dB_x2_lr.png | 18.90dB
22-10-20 10:56:10.418 : ---3--> 03_0dB_x2_lr.png | 21.58dB
22-10-20 10:56:10.697 : ---4--> 04_0dB_x2_lr.png | 18.13dB
22-10-20 10:56:10.972 : ---5--> 05_0dB_x2_lr.png | 17.25dB
22-10-20 10:56:11.240 : ---6--> 06_0dB_x2_lr.png | 21.16dB
22-10-20 10:56:11.516 : ---7--> 07_0dB_x2_lr.png | 21.03dB
22-10-20 10:56:12.700 : ---8--> 08_0dB_x2_lr.png | 20.35dB
22-10-20 10:56:13.877 : ---9--> 09_0dB_x2_lr.png | 18.97dB
22-10-20 10:56:15.051 : --10--> 10_0dB_x2_lr.png | 22.57dB
22-10-20 10:56:16.212 : --11--> 11_0dB_x2_lr.png | 18.49dB
22-10-20 10:56:17.385 : --12--> 12_0dB_x2_lr.png | 21.86dB
22-10-20 10:56:17.607 : <epoch: 12, iter:  10,000, Average PSNR : 20.10dB

22-10-20 10:57:00.915 : <epoch: 12, iter:  10,200, lr:2.000e-04> G_loss: 7.789e-02 
22-10-20 10:57:45.915 : <epoch: 12, iter:  10,400, lr:2.000e-04> G_loss: 6.263e-02 
22-10-20 10:58:49.311 : <epoch: 13, iter:  10,600, lr:2.000e-04> G_loss: 1.417e-01 
22-10-20 10:59:33.180 : <epoch: 13, iter:  10,800, lr:2.000e-04> G_loss: 1.768e-01 
22-10-20 11:00:17.313 : <epoch: 13, iter:  11,000, lr:2.000e-04> G_loss: 7.638e-02 
22-10-20 11:01:01.617 : <epoch: 13, iter:  11,200, lr:2.000e-04> G_loss: 7.254e-02 
22-10-20 11:02:05.573 : <epoch: 14, iter:  11,400, lr:2.000e-04> G_loss: 7.054e-02 
22-10-20 11:02:49.745 : <epoch: 14, iter:  11,600, lr:2.000e-04> G_loss: 6.965e-02 
22-10-20 11:03:34.420 : <epoch: 14, iter:  11,800, lr:2.000e-04> G_loss: 4.872e-02 
22-10-20 11:04:18.237 : <epoch: 14, iter:  12,000, lr:2.000e-04> G_loss: 6.283e-02 
22-10-20 11:05:20.803 : <epoch: 15, iter:  12,200, lr:2.000e-04> G_loss: 6.156e-02 
22-10-20 11:06:02.802 : <epoch: 15, iter:  12,400, lr:2.000e-04> G_loss: 1.109e-01 
22-10-20 11:06:44.238 : <epoch: 15, iter:  12,600, lr:2.000e-04> G_loss: 5.681e-02 
22-10-20 11:07:25.425 : <epoch: 15, iter:  12,800, lr:2.000e-04> G_loss: 1.121e-01 
22-10-20 11:08:24.056 : <epoch: 16, iter:  13,000, lr:2.000e-04> G_loss: 7.894e-02 
22-10-20 11:09:05.026 : <epoch: 16, iter:  13,200, lr:2.000e-04> G_loss: 1.017e-01 
22-10-20 11:09:47.262 : <epoch: 16, iter:  13,400, lr:2.000e-04> G_loss: 6.071e-02 
22-10-20 11:10:29.389 : <epoch: 16, iter:  13,600, lr:2.000e-04> G_loss: 6.728e-02 
22-10-20 11:11:28.748 : <epoch: 17, iter:  13,800, lr:2.000e-04> G_loss: 7.323e-02 
22-10-20 11:12:10.756 : <epoch: 17, iter:  14,000, lr:2.000e-04> G_loss: 6.589e-02 
22-10-20 11:12:52.796 : <epoch: 17, iter:  14,200, lr:2.000e-04> G_loss: 3.235e-02 
22-10-20 11:13:34.279 : <epoch: 17, iter:  14,400, lr:2.000e-04> G_loss: 9.623e-02 
22-10-20 11:14:31.873 : <epoch: 18, iter:  14,600, lr:2.000e-04> G_loss: 6.657e-02 
22-10-20 11:15:14.110 : <epoch: 18, iter:  14,800, lr:2.000e-04> G_loss: 7.567e-02 
22-10-20 11:15:56.462 : <epoch: 18, iter:  15,000, lr:2.000e-04> G_loss: 6.121e-02 
22-10-20 11:15:56.462 : Saving the model.
22-10-20 11:15:58.551 : ---1--> 01_0dB_x2_lr.png | 20.78dB
22-10-20 11:15:58.789 : ---2--> 02_0dB_x2_lr.png | 22.03dB
22-10-20 11:15:59.011 : ---3--> 03_0dB_x2_lr.png | 21.39dB
22-10-20 11:15:59.238 : ---4--> 04_0dB_x2_lr.png | 21.08dB
22-10-20 11:15:59.465 : ---5--> 05_0dB_x2_lr.png | 19.65dB
22-10-20 11:15:59.701 : ---6--> 06_0dB_x2_lr.png | 20.13dB
22-10-20 11:15:59.937 : ---7--> 07_0dB_x2_lr.png | 20.31dB
22-10-20 11:16:00.922 : ---8--> 08_0dB_x2_lr.png | 23.07dB
22-10-20 11:16:01.914 : ---9--> 09_0dB_x2_lr.png | 21.28dB
22-10-20 11:16:02.898 : --10--> 10_0dB_x2_lr.png | 22.49dB
22-10-20 11:16:03.903 : --11--> 11_0dB_x2_lr.png | 21.09dB
22-10-20 11:16:04.917 : --12--> 12_0dB_x2_lr.png | 22.68dB
22-10-20 11:16:05.130 : <epoch: 18, iter:  15,000, Average PSNR : 21.33dB

22-10-20 11:16:46.515 : <epoch: 18, iter:  15,200, lr:2.000e-04> G_loss: 9.841e-02 
22-10-20 11:17:44.364 : <epoch: 19, iter:  15,400, lr:2.000e-04> G_loss: 7.335e-02 
22-10-20 11:18:25.685 : <epoch: 19, iter:  15,600, lr:2.000e-04> G_loss: 7.365e-02 
22-10-20 11:19:06.994 : <epoch: 19, iter:  15,800, lr:2.000e-04> G_loss: 5.958e-02 
22-10-20 11:19:48.102 : <epoch: 19, iter:  16,000, lr:2.000e-04> G_loss: 5.017e-02 
22-10-20 11:20:45.687 : <epoch: 20, iter:  16,200, lr:2.000e-04> G_loss: 7.222e-02 
22-10-20 11:21:26.396 : <epoch: 20, iter:  16,400, lr:2.000e-04> G_loss: 9.536e-02 
22-10-20 11:22:07.351 : <epoch: 20, iter:  16,600, lr:2.000e-04> G_loss: 6.289e-02 
22-10-20 11:22:47.780 : <epoch: 20, iter:  16,800, lr:2.000e-04> G_loss: 9.665e-02 
22-10-20 11:23:44.767 : <epoch: 21, iter:  17,000, lr:2.000e-04> G_loss: 5.622e-02 
22-10-20 11:24:25.067 : <epoch: 21, iter:  17,200, lr:2.000e-04> G_loss: 7.305e-02 
22-10-20 11:25:05.406 : <epoch: 21, iter:  17,400, lr:2.000e-04> G_loss: 8.671e-02 
22-10-20 11:25:45.802 : <epoch: 21, iter:  17,600, lr:2.000e-04> G_loss: 7.309e-02 
22-10-20 11:26:42.351 : <epoch: 22, iter:  17,800, lr:2.000e-04> G_loss: 1.088e-01 
22-10-20 11:27:22.812 : <epoch: 22, iter:  18,000, lr:2.000e-04> G_loss: 8.449e-02 
22-10-20 11:28:03.314 : <epoch: 22, iter:  18,200, lr:2.000e-04> G_loss: 5.404e-02 
22-10-20 11:28:43.792 : <epoch: 22, iter:  18,400, lr:2.000e-04> G_loss: 1.033e-01 
22-10-20 11:29:41.622 : <epoch: 23, iter:  18,600, lr:2.000e-04> G_loss: 4.686e-02 
22-10-20 11:30:22.833 : <epoch: 23, iter:  18,800, lr:2.000e-04> G_loss: 6.420e-02 
22-10-20 11:31:05.142 : <epoch: 23, iter:  19,000, lr:2.000e-04> G_loss: 9.636e-02 
22-10-20 11:31:47.139 : <epoch: 23, iter:  19,200, lr:2.000e-04> G_loss: 7.817e-02 
22-10-20 11:32:44.024 : <epoch: 24, iter:  19,400, lr:2.000e-04> G_loss: 6.572e-02 
22-10-20 11:33:24.407 : <epoch: 24, iter:  19,600, lr:2.000e-04> G_loss: 5.213e-02 
22-10-20 11:34:04.931 : <epoch: 24, iter:  19,800, lr:2.000e-04> G_loss: 1.019e-01 
22-10-20 11:34:45.323 : <epoch: 24, iter:  20,000, lr:2.000e-04> G_loss: 5.088e-02 
22-10-20 11:34:45.324 : Saving the model.
22-10-20 11:34:47.190 : ---1--> 01_0dB_x2_lr.png | 20.90dB
22-10-20 11:34:47.406 : ---2--> 02_0dB_x2_lr.png | 21.45dB
22-10-20 11:34:47.613 : ---3--> 03_0dB_x2_lr.png | 21.63dB
22-10-20 11:34:47.833 : ---4--> 04_0dB_x2_lr.png | 21.34dB
22-10-20 11:34:48.051 : ---5--> 05_0dB_x2_lr.png | 20.16dB
22-10-20 11:34:48.269 : ---6--> 06_0dB_x2_lr.png | 21.47dB
22-10-20 11:34:48.489 : ---7--> 07_0dB_x2_lr.png | 19.61dB
22-10-20 11:34:49.465 : ---8--> 08_0dB_x2_lr.png | 24.21dB
22-10-20 11:34:50.440 : ---9--> 09_0dB_x2_lr.png | 21.87dB
22-10-20 11:34:51.429 : --10--> 10_0dB_x2_lr.png | 23.43dB
22-10-20 11:34:52.402 : --11--> 11_0dB_x2_lr.png | 23.55dB
22-10-20 11:34:53.389 : --12--> 12_0dB_x2_lr.png | 22.79dB
22-10-20 11:34:53.562 : <epoch: 24, iter:  20,000, Average PSNR : 21.87dB

22-10-20 11:35:50.379 : <epoch: 25, iter:  20,200, lr:2.000e-04> G_loss: 1.048e-01 
22-10-20 11:36:31.346 : <epoch: 25, iter:  20,400, lr:2.000e-04> G_loss: 9.643e-02 
22-10-20 11:37:11.766 : <epoch: 25, iter:  20,600, lr:2.000e-04> G_loss: 1.188e-01 
22-10-20 11:37:52.052 : <epoch: 25, iter:  20,800, lr:2.000e-04> G_loss: 9.138e-02 
22-10-20 11:38:49.160 : <epoch: 26, iter:  21,000, lr:2.000e-04> G_loss: 4.645e-02 
22-10-20 11:39:29.567 : <epoch: 26, iter:  21,200, lr:2.000e-04> G_loss: 7.642e-02 
22-10-20 11:40:10.083 : <epoch: 26, iter:  21,400, lr:2.000e-04> G_loss: 8.270e-02 
22-10-20 11:40:50.477 : <epoch: 26, iter:  21,600, lr:2.000e-04> G_loss: 6.243e-02 
22-10-20 11:41:49.602 : <epoch: 27, iter:  21,800, lr:2.000e-04> G_loss: 4.052e-02 
22-10-20 11:42:32.354 : <epoch: 27, iter:  22,000, lr:2.000e-04> G_loss: 5.104e-02 
22-10-20 11:43:14.893 : <epoch: 27, iter:  22,200, lr:2.000e-04> G_loss: 7.861e-02 
22-10-20 11:43:57.481 : <epoch: 27, iter:  22,400, lr:2.000e-04> G_loss: 4.935e-02 
22-10-20 11:44:54.588 : <epoch: 28, iter:  22,600, lr:2.000e-04> G_loss: 8.132e-02 
22-10-20 11:45:35.062 : <epoch: 28, iter:  22,800, lr:2.000e-04> G_loss: 6.532e-02 
22-10-20 11:46:15.520 : <epoch: 28, iter:  23,000, lr:2.000e-04> G_loss: 1.047e-01 
22-10-20 11:46:55.966 : <epoch: 28, iter:  23,200, lr:2.000e-04> G_loss: 8.539e-02 
22-10-20 11:47:53.370 : <epoch: 29, iter:  23,400, lr:2.000e-04> G_loss: 5.127e-02 
22-10-20 11:48:33.714 : <epoch: 29, iter:  23,600, lr:2.000e-04> G_loss: 9.411e-02 
22-10-20 11:49:14.063 : <epoch: 29, iter:  23,800, lr:2.000e-04> G_loss: 5.901e-02 
22-10-20 11:49:54.487 : <epoch: 29, iter:  24,000, lr:2.000e-04> G_loss: 8.060e-02 
22-10-20 11:50:51.541 : <epoch: 30, iter:  24,200, lr:2.000e-04> G_loss: 1.094e-01 
22-10-20 11:51:32.057 : <epoch: 30, iter:  24,400, lr:2.000e-04> G_loss: 6.888e-02 
22-10-20 11:52:12.468 : <epoch: 30, iter:  24,600, lr:2.000e-04> G_loss: 6.901e-02 
22-10-20 11:52:52.581 : <epoch: 30, iter:  24,800, lr:2.000e-04> G_loss: 7.299e-02 
22-10-20 11:53:49.533 : <epoch: 31, iter:  25,000, lr:2.000e-04> G_loss: 7.033e-02 
22-10-20 11:53:49.533 : Saving the model.
22-10-20 11:53:51.410 : ---1--> 01_0dB_x2_lr.png | 20.80dB
22-10-20 11:53:51.632 : ---2--> 02_0dB_x2_lr.png | 19.94dB
22-10-20 11:53:51.856 : ---3--> 03_0dB_x2_lr.png | 22.34dB
22-10-20 11:53:52.076 : ---4--> 04_0dB_x2_lr.png | 19.60dB
22-10-20 11:53:52.298 : ---5--> 05_0dB_x2_lr.png | 19.08dB
22-10-20 11:53:52.517 : ---6--> 06_0dB_x2_lr.png | 21.87dB
22-10-20 11:53:52.739 : ---7--> 07_0dB_x2_lr.png | 20.86dB
22-10-20 11:53:53.725 : ---8--> 08_0dB_x2_lr.png | 22.09dB
22-10-20 11:53:54.705 : ---9--> 09_0dB_x2_lr.png | 20.51dB
22-10-20 11:53:55.685 : --10--> 10_0dB_x2_lr.png | 23.13dB
22-10-20 11:53:56.664 : --11--> 11_0dB_x2_lr.png | 21.53dB
22-10-20 11:53:57.641 : --12--> 12_0dB_x2_lr.png | 22.34dB
22-10-20 11:53:57.809 : <epoch: 31, iter:  25,000, Average PSNR : 21.17dB

22-10-20 11:54:38.752 : <epoch: 31, iter:  25,200, lr:2.000e-04> G_loss: 8.747e-02 
22-10-20 11:55:19.495 : <epoch: 31, iter:  25,400, lr:2.000e-04> G_loss: 5.667e-02 
22-10-20 11:56:00.375 : <epoch: 31, iter:  25,600, lr:2.000e-04> G_loss: 7.479e-02 
22-10-20 11:56:57.269 : <epoch: 32, iter:  25,800, lr:2.000e-04> G_loss: 1.032e-01 
22-10-20 11:57:37.699 : <epoch: 32, iter:  26,000, lr:2.000e-04> G_loss: 5.793e-02 
22-10-20 11:58:18.010 : <epoch: 32, iter:  26,200, lr:2.000e-04> G_loss: 6.397e-02 
22-10-20 11:58:58.900 : <epoch: 32, iter:  26,400, lr:2.000e-04> G_loss: 9.980e-02 
22-10-20 11:59:55.658 : <epoch: 33, iter:  26,600, lr:2.000e-04> G_loss: 5.960e-02 
22-10-20 12:00:35.978 : <epoch: 33, iter:  26,800, lr:2.000e-04> G_loss: 4.765e-02 
22-10-20 12:01:16.420 : <epoch: 33, iter:  27,000, lr:2.000e-04> G_loss: 1.237e-01 
22-10-20 12:01:56.697 : <epoch: 33, iter:  27,200, lr:2.000e-04> G_loss: 6.668e-02 
22-10-20 12:02:53.586 : <epoch: 34, iter:  27,400, lr:2.000e-04> G_loss: 6.575e-02 
22-10-20 12:03:33.888 : <epoch: 34, iter:  27,600, lr:2.000e-04> G_loss: 5.361e-02 
22-10-20 12:04:14.135 : <epoch: 34, iter:  27,800, lr:2.000e-04> G_loss: 9.273e-02 
22-10-20 12:04:54.489 : <epoch: 34, iter:  28,000, lr:2.000e-04> G_loss: 4.395e-02 
22-10-20 12:05:51.310 : <epoch: 35, iter:  28,200, lr:2.000e-04> G_loss: 6.656e-02 
22-10-20 12:06:31.670 : <epoch: 35, iter:  28,400, lr:2.000e-04> G_loss: 1.041e-01 
22-10-20 12:07:12.046 : <epoch: 35, iter:  28,600, lr:2.000e-04> G_loss: 7.024e-02 
22-10-20 12:07:52.312 : <epoch: 35, iter:  28,800, lr:2.000e-04> G_loss: 8.724e-02 
22-10-20 12:08:49.515 : <epoch: 36, iter:  29,000, lr:2.000e-04> G_loss: 6.365e-02 
22-10-20 12:09:30.129 : <epoch: 36, iter:  29,200, lr:2.000e-04> G_loss: 3.880e-02 
22-10-20 12:10:10.751 : <epoch: 36, iter:  29,400, lr:2.000e-04> G_loss: 5.923e-02 
22-10-20 12:10:51.217 : <epoch: 36, iter:  29,600, lr:2.000e-04> G_loss: 9.637e-02 
22-10-20 12:11:48.361 : <epoch: 37, iter:  29,800, lr:2.000e-04> G_loss: 5.144e-02 
22-10-20 12:12:29.244 : <epoch: 37, iter:  30,000, lr:2.000e-04> G_loss: 7.802e-02 
22-10-20 12:12:29.244 : Saving the model.
22-10-20 12:12:31.132 : ---1--> 01_0dB_x2_lr.png | 21.33dB
22-10-20 12:12:31.355 : ---2--> 02_0dB_x2_lr.png | 22.11dB
22-10-20 12:12:31.575 : ---3--> 03_0dB_x2_lr.png | 21.96dB
22-10-20 12:12:31.790 : ---4--> 04_0dB_x2_lr.png | 21.16dB
22-10-20 12:12:32.021 : ---5--> 05_0dB_x2_lr.png | 20.28dB
22-10-20 12:12:32.247 : ---6--> 06_0dB_x2_lr.png | 20.92dB
22-10-20 12:12:32.463 : ---7--> 07_0dB_x2_lr.png | 20.70dB
22-10-20 12:12:33.442 : ---8--> 08_0dB_x2_lr.png | 23.76dB
22-10-20 12:12:34.413 : ---9--> 09_0dB_x2_lr.png | 21.61dB
22-10-20 12:12:35.393 : --10--> 10_0dB_x2_lr.png | 23.53dB
22-10-20 12:12:36.369 : --11--> 11_0dB_x2_lr.png | 21.68dB
22-10-20 12:12:37.368 : --12--> 12_0dB_x2_lr.png | 23.22dB
22-10-20 12:12:37.543 : <epoch: 37, iter:  30,000, Average PSNR : 21.86dB

22-10-20 12:13:18.343 : <epoch: 37, iter:  30,200, lr:2.000e-04> G_loss: 1.089e-01 
22-10-20 12:13:59.198 : <epoch: 37, iter:  30,400, lr:2.000e-04> G_loss: 5.759e-02 
22-10-20 12:14:56.154 : <epoch: 38, iter:  30,600, lr:2.000e-04> G_loss: 7.059e-02 
22-10-20 12:15:36.561 : <epoch: 38, iter:  30,800, lr:2.000e-04> G_loss: 1.078e-01 
22-10-20 12:16:16.995 : <epoch: 38, iter:  31,000, lr:2.000e-04> G_loss: 8.762e-02 
22-10-20 12:16:57.346 : <epoch: 38, iter:  31,200, lr:2.000e-04> G_loss: 7.568e-02 
22-10-20 12:17:53.833 : <epoch: 39, iter:  31,400, lr:2.000e-04> G_loss: 7.673e-02 
22-10-20 12:18:34.282 : <epoch: 39, iter:  31,600, lr:2.000e-04> G_loss: 9.645e-02 
22-10-20 12:19:14.695 : <epoch: 39, iter:  31,800, lr:2.000e-04> G_loss: 6.873e-02 
22-10-20 12:19:54.966 : <epoch: 39, iter:  32,000, lr:2.000e-04> G_loss: 7.924e-02 
22-10-20 12:20:52.975 : <epoch: 40, iter:  32,200, lr:2.000e-04> G_loss: 5.567e-02 
22-10-20 12:21:34.776 : <epoch: 40, iter:  32,400, lr:2.000e-04> G_loss: 7.416e-02 
22-10-20 12:22:15.525 : <epoch: 40, iter:  32,600, lr:2.000e-04> G_loss: 6.140e-02 
22-10-20 12:22:55.688 : <epoch: 40, iter:  32,800, lr:2.000e-04> G_loss: 5.073e-02 
22-10-20 12:23:52.940 : <epoch: 41, iter:  33,000, lr:2.000e-04> G_loss: 4.499e-02 
22-10-20 12:24:34.000 : <epoch: 41, iter:  33,200, lr:2.000e-04> G_loss: 7.948e-02 
22-10-20 12:25:17.146 : <epoch: 41, iter:  33,400, lr:2.000e-04> G_loss: 7.735e-02 
22-10-20 12:25:58.462 : <epoch: 41, iter:  33,600, lr:2.000e-04> G_loss: 7.217e-02 
22-10-20 12:26:57.494 : <epoch: 42, iter:  33,800, lr:2.000e-04> G_loss: 8.115e-02 
22-10-20 12:27:39.170 : <epoch: 42, iter:  34,000, lr:2.000e-04> G_loss: 4.576e-02 
22-10-20 12:28:21.565 : <epoch: 42, iter:  34,200, lr:2.000e-04> G_loss: 6.892e-02 
22-10-20 12:29:03.519 : <epoch: 42, iter:  34,400, lr:2.000e-04> G_loss: 1.786e-01 
22-10-20 12:30:03.582 : <epoch: 43, iter:  34,600, lr:2.000e-04> G_loss: 1.603e-01 
22-10-20 12:30:45.998 : <epoch: 43, iter:  34,800, lr:2.000e-04> G_loss: 9.727e-02 
22-10-20 12:31:28.991 : <epoch: 43, iter:  35,000, lr:2.000e-04> G_loss: 6.100e-02 
22-10-20 12:31:28.991 : Saving the model.
22-10-20 12:31:31.195 : ---1--> 01_0dB_x2_lr.png | 21.33dB
22-10-20 12:31:31.479 : ---2--> 02_0dB_x2_lr.png | 22.11dB
22-10-20 12:31:31.755 : ---3--> 03_0dB_x2_lr.png | 22.59dB
22-10-20 12:31:32.032 : ---4--> 04_0dB_x2_lr.png | 21.10dB
22-10-20 12:31:32.317 : ---5--> 05_0dB_x2_lr.png | 19.92dB
22-10-20 12:31:32.590 : ---6--> 06_0dB_x2_lr.png | 22.02dB
22-10-20 12:31:32.866 : ---7--> 07_0dB_x2_lr.png | 20.35dB
22-10-20 12:31:34.035 : ---8--> 08_0dB_x2_lr.png | 24.49dB
22-10-20 12:31:35.228 : ---9--> 09_0dB_x2_lr.png | 22.01dB
22-10-20 12:31:36.407 : --10--> 10_0dB_x2_lr.png | 23.76dB
22-10-20 12:31:37.585 : --11--> 11_0dB_x2_lr.png | 22.85dB
22-10-20 12:31:38.758 : --12--> 12_0dB_x2_lr.png | 23.33dB
22-10-20 12:31:38.982 : <epoch: 43, iter:  35,000, Average PSNR : 22.15dB

22-10-20 12:32:21.795 : <epoch: 43, iter:  35,200, lr:2.000e-04> G_loss: 7.279e-02 
22-10-20 12:33:23.405 : <epoch: 44, iter:  35,400, lr:2.000e-04> G_loss: 5.098e-02 
22-10-20 12:34:06.417 : <epoch: 44, iter:  35,600, lr:2.000e-04> G_loss: 4.831e-02 
22-10-20 12:34:49.401 : <epoch: 44, iter:  35,800, lr:2.000e-04> G_loss: 1.141e-01 
22-10-20 12:35:32.167 : <epoch: 44, iter:  36,000, lr:2.000e-04> G_loss: 8.741e-02 
22-10-20 12:36:32.888 : <epoch: 45, iter:  36,200, lr:2.000e-04> G_loss: 7.075e-02 
22-10-20 12:37:15.311 : <epoch: 45, iter:  36,400, lr:2.000e-04> G_loss: 4.907e-02 
22-10-20 12:37:58.037 : <epoch: 45, iter:  36,600, lr:2.000e-04> G_loss: 4.861e-02 
22-10-20 12:38:41.289 : <epoch: 45, iter:  36,800, lr:2.000e-04> G_loss: 1.143e-01 
22-10-20 12:39:42.567 : <epoch: 46, iter:  37,000, lr:2.000e-04> G_loss: 8.411e-02 
22-10-20 12:40:25.293 : <epoch: 46, iter:  37,200, lr:2.000e-04> G_loss: 6.782e-02 
22-10-20 12:41:08.413 : <epoch: 46, iter:  37,400, lr:2.000e-04> G_loss: 9.095e-02 
22-10-20 12:41:50.947 : <epoch: 46, iter:  37,600, lr:2.000e-04> G_loss: 6.565e-02 
22-10-20 12:42:53.320 : <epoch: 47, iter:  37,800, lr:2.000e-04> G_loss: 5.262e-02 
22-10-20 12:43:37.082 : <epoch: 47, iter:  38,000, lr:2.000e-04> G_loss: 4.661e-02 
22-10-20 12:44:21.244 : <epoch: 47, iter:  38,200, lr:2.000e-04> G_loss: 5.953e-02 
22-10-20 12:45:06.125 : <epoch: 47, iter:  38,400, lr:2.000e-04> G_loss: 3.344e-02 
22-10-20 12:46:09.828 : <epoch: 48, iter:  38,600, lr:2.000e-04> G_loss: 1.028e-01 
22-10-20 12:46:53.690 : <epoch: 48, iter:  38,800, lr:2.000e-04> G_loss: 8.736e-02 
22-10-20 12:47:37.822 : <epoch: 48, iter:  39,000, lr:2.000e-04> G_loss: 6.620e-02 
22-10-20 12:48:22.184 : <epoch: 48, iter:  39,200, lr:2.000e-04> G_loss: 9.328e-02 
22-10-20 12:49:25.536 : <epoch: 49, iter:  39,400, lr:2.000e-04> G_loss: 1.289e-01 
22-10-20 12:50:09.736 : <epoch: 49, iter:  39,600, lr:2.000e-04> G_loss: 4.747e-02 
22-10-20 12:50:53.595 : <epoch: 49, iter:  39,800, lr:2.000e-04> G_loss: 4.108e-02 
22-10-20 12:51:37.992 : <epoch: 49, iter:  40,000, lr:2.000e-04> G_loss: 5.399e-02 
22-10-20 12:51:37.993 : Saving the model.
22-10-20 12:51:40.241 : ---1--> 01_0dB_x2_lr.png | 21.17dB
22-10-20 12:51:40.510 : ---2--> 02_0dB_x2_lr.png | 21.61dB
22-10-20 12:51:40.774 : ---3--> 03_0dB_x2_lr.png | 21.45dB
22-10-20 12:51:41.046 : ---4--> 04_0dB_x2_lr.png | 21.14dB
22-10-20 12:51:41.309 : ---5--> 05_0dB_x2_lr.png | 19.82dB
22-10-20 12:51:41.569 : ---6--> 06_0dB_x2_lr.png | 20.69dB
22-10-20 12:51:41.838 : ---7--> 07_0dB_x2_lr.png | 20.72dB
22-10-20 12:51:42.995 : ---8--> 08_0dB_x2_lr.png | 23.12dB
22-10-20 12:51:44.139 : ---9--> 09_0dB_x2_lr.png | 21.27dB
22-10-20 12:51:45.311 : --10--> 10_0dB_x2_lr.png | 23.32dB
22-10-20 12:51:46.543 : --11--> 11_0dB_x2_lr.png | 21.25dB
22-10-20 12:51:47.734 : --12--> 12_0dB_x2_lr.png | 23.05dB
22-10-20 12:51:47.962 : <epoch: 49, iter:  40,000, Average PSNR : 21.55dB

22-10-20 12:52:50.916 : <epoch: 50, iter:  40,200, lr:2.000e-04> G_loss: 8.596e-02 
22-10-20 12:53:34.586 : <epoch: 50, iter:  40,400, lr:2.000e-04> G_loss: 7.137e-02 
22-10-20 12:54:18.451 : <epoch: 50, iter:  40,600, lr:2.000e-04> G_loss: 6.733e-02 
22-10-20 12:55:02.993 : <epoch: 50, iter:  40,800, lr:2.000e-04> G_loss: 9.071e-02 
22-10-20 12:56:08.430 : <epoch: 51, iter:  41,000, lr:2.000e-04> G_loss: 7.629e-02 
22-10-20 12:56:53.464 : <epoch: 51, iter:  41,200, lr:2.000e-04> G_loss: 7.679e-02 
22-10-20 12:57:37.916 : <epoch: 51, iter:  41,400, lr:2.000e-04> G_loss: 1.229e-01 
22-10-20 12:58:22.505 : <epoch: 51, iter:  41,600, lr:2.000e-04> G_loss: 7.680e-02 
22-10-20 12:59:25.709 : <epoch: 52, iter:  41,800, lr:2.000e-04> G_loss: 5.666e-02 
22-10-20 13:00:08.916 : <epoch: 52, iter:  42,000, lr:2.000e-04> G_loss: 4.354e-02 
22-10-20 13:00:52.015 : <epoch: 52, iter:  42,200, lr:2.000e-04> G_loss: 7.609e-02 
22-10-20 13:01:35.379 : <epoch: 52, iter:  42,400, lr:2.000e-04> G_loss: 5.183e-02 
22-10-20 13:02:38.023 : <epoch: 53, iter:  42,600, lr:2.000e-04> G_loss: 6.590e-02 
22-10-20 13:03:21.366 : <epoch: 53, iter:  42,800, lr:2.000e-04> G_loss: 5.959e-02 
22-10-20 13:04:04.398 : <epoch: 53, iter:  43,000, lr:2.000e-04> G_loss: 7.583e-02 
22-10-20 13:04:48.286 : <epoch: 53, iter:  43,200, lr:2.000e-04> G_loss: 6.560e-02 
22-10-20 13:05:46.474 : <epoch: 54, iter:  43,400, lr:2.000e-04> G_loss: 4.118e-02 
22-10-20 13:06:27.784 : <epoch: 54, iter:  43,600, lr:2.000e-04> G_loss: 1.018e-01 
22-10-20 13:07:08.893 : <epoch: 54, iter:  43,800, lr:2.000e-04> G_loss: 8.138e-02 
22-10-20 13:07:49.859 : <epoch: 54, iter:  44,000, lr:2.000e-04> G_loss: 4.945e-02 
22-10-20 13:08:48.728 : <epoch: 55, iter:  44,200, lr:2.000e-04> G_loss: 3.850e-02 
22-10-20 13:09:30.859 : <epoch: 55, iter:  44,400, lr:2.000e-04> G_loss: 7.173e-02 
22-10-20 13:10:12.535 : <epoch: 55, iter:  44,600, lr:2.000e-04> G_loss: 9.249e-02 
22-10-20 13:10:54.010 : <epoch: 55, iter:  44,800, lr:2.000e-04> G_loss: 7.589e-02 
22-10-20 13:11:53.497 : <epoch: 56, iter:  45,000, lr:2.000e-04> G_loss: 6.623e-02 
22-10-20 13:11:53.497 : Saving the model.
22-10-20 13:11:55.473 : ---1--> 01_0dB_x2_lr.png | 21.62dB
22-10-20 13:11:55.716 : ---2--> 02_0dB_x2_lr.png | 22.53dB
22-10-20 13:11:55.959 : ---3--> 03_0dB_x2_lr.png | 22.22dB
22-10-20 13:11:56.192 : ---4--> 04_0dB_x2_lr.png | 21.42dB
22-10-20 13:11:56.429 : ---5--> 05_0dB_x2_lr.png | 20.76dB
22-10-20 13:11:56.668 : ---6--> 06_0dB_x2_lr.png | 21.96dB
22-10-20 13:11:56.908 : ---7--> 07_0dB_x2_lr.png | 20.35dB
22-10-20 13:11:57.914 : ---8--> 08_0dB_x2_lr.png | 24.85dB
22-10-20 13:11:58.933 : ---9--> 09_0dB_x2_lr.png | 22.09dB
22-10-20 13:11:59.947 : --10--> 10_0dB_x2_lr.png | 23.83dB
22-10-20 13:12:00.970 : --11--> 11_0dB_x2_lr.png | 22.52dB
22-10-20 13:12:01.984 : --12--> 12_0dB_x2_lr.png | 23.21dB
22-10-20 13:12:02.166 : <epoch: 56, iter:  45,000, Average PSNR : 22.28dB

22-10-20 13:12:43.567 : <epoch: 56, iter:  45,200, lr:2.000e-04> G_loss: 8.212e-02 
22-10-20 13:13:24.908 : <epoch: 56, iter:  45,400, lr:2.000e-04> G_loss: 2.771e-02 
22-10-20 13:14:06.288 : <epoch: 56, iter:  45,600, lr:2.000e-04> G_loss: 7.080e-02 
22-10-20 13:15:05.062 : <epoch: 57, iter:  45,800, lr:2.000e-04> G_loss: 2.746e-02 
22-10-20 13:15:46.504 : <epoch: 57, iter:  46,000, lr:2.000e-04> G_loss: 6.925e-02 
22-10-20 13:16:27.997 : <epoch: 57, iter:  46,200, lr:2.000e-04> G_loss: 6.521e-02 
22-10-20 13:17:09.631 : <epoch: 57, iter:  46,400, lr:2.000e-04> G_loss: 7.487e-02 
22-10-20 13:18:08.586 : <epoch: 58, iter:  46,600, lr:2.000e-04> G_loss: 6.082e-02 
22-10-20 13:18:50.426 : <epoch: 58, iter:  46,800, lr:2.000e-04> G_loss: 5.104e-02 
22-10-20 13:19:32.496 : <epoch: 58, iter:  47,000, lr:2.000e-04> G_loss: 7.436e-02 
22-10-20 13:20:14.895 : <epoch: 58, iter:  47,200, lr:2.000e-04> G_loss: 7.243e-02 
22-10-20 13:21:13.927 : <epoch: 59, iter:  47,400, lr:2.000e-04> G_loss: 6.569e-02 
22-10-20 13:21:55.060 : <epoch: 59, iter:  47,600, lr:2.000e-04> G_loss: 7.978e-02 
22-10-20 13:22:36.077 : <epoch: 59, iter:  47,800, lr:2.000e-04> G_loss: 5.982e-02 
22-10-20 13:23:17.075 : <epoch: 59, iter:  48,000, lr:2.000e-04> G_loss: 6.077e-02 
22-10-20 13:24:15.108 : <epoch: 60, iter:  48,200, lr:2.000e-04> G_loss: 6.105e-02 
22-10-20 13:24:56.059 : <epoch: 60, iter:  48,400, lr:2.000e-04> G_loss: 6.484e-02 
22-10-20 13:25:37.120 : <epoch: 60, iter:  48,600, lr:2.000e-04> G_loss: 6.074e-02 
22-10-20 13:26:18.001 : <epoch: 60, iter:  48,800, lr:2.000e-04> G_loss: 8.083e-02 
22-10-20 13:27:15.695 : <epoch: 61, iter:  49,000, lr:2.000e-04> G_loss: 7.008e-02 
22-10-20 13:27:56.552 : <epoch: 61, iter:  49,200, lr:2.000e-04> G_loss: 7.209e-02 
22-10-20 13:28:37.436 : <epoch: 61, iter:  49,400, lr:2.000e-04> G_loss: 5.026e-02 
22-10-20 13:29:18.248 : <epoch: 61, iter:  49,600, lr:2.000e-04> G_loss: 6.194e-02 
22-10-20 13:30:16.109 : <epoch: 62, iter:  49,800, lr:2.000e-04> G_loss: 5.336e-02 
22-10-20 13:30:56.924 : <epoch: 62, iter:  50,000, lr:2.000e-04> G_loss: 5.929e-02 
22-10-20 13:30:56.925 : Saving the model.
22-10-20 13:30:58.852 : ---1--> 01_0dB_x2_lr.png | 21.07dB
22-10-20 13:30:59.087 : ---2--> 02_0dB_x2_lr.png | 22.53dB
22-10-20 13:30:59.319 : ---3--> 03_0dB_x2_lr.png | 22.25dB
22-10-20 13:30:59.542 : ---4--> 04_0dB_x2_lr.png | 21.17dB
22-10-20 13:30:59.772 : ---5--> 05_0dB_x2_lr.png | 20.48dB
22-10-20 13:31:00.002 : ---6--> 06_0dB_x2_lr.png | 20.80dB
22-10-20 13:31:00.246 : ---7--> 07_0dB_x2_lr.png | 20.41dB
22-10-20 13:31:01.264 : ---8--> 08_0dB_x2_lr.png | 24.17dB
22-10-20 13:31:02.279 : ---9--> 09_0dB_x2_lr.png | 21.85dB
22-10-20 13:31:03.281 : --10--> 10_0dB_x2_lr.png | 23.73dB
22-10-20 13:31:04.303 : --11--> 11_0dB_x2_lr.png | 22.26dB
22-10-20 13:31:05.315 : --12--> 12_0dB_x2_lr.png | 23.35dB
22-10-20 13:31:05.484 : <epoch: 62, iter:  50,000, Average PSNR : 22.01dB

22-10-20 13:31:47.203 : <epoch: 62, iter:  50,200, lr:2.000e-04> G_loss: 4.165e-02 
22-10-20 13:32:27.899 : <epoch: 62, iter:  50,400, lr:2.000e-04> G_loss: 1.130e-01 
22-10-20 13:33:26.103 : <epoch: 63, iter:  50,600, lr:2.000e-04> G_loss: 9.437e-02 
22-10-20 13:34:06.892 : <epoch: 63, iter:  50,800, lr:2.000e-04> G_loss: 1.056e-01 
22-10-20 13:34:47.721 : <epoch: 63, iter:  51,000, lr:2.000e-04> G_loss: 1.408e-01 
22-10-20 13:35:28.446 : <epoch: 63, iter:  51,200, lr:2.000e-04> G_loss: 9.335e-02 
22-10-20 13:36:26.372 : <epoch: 64, iter:  51,400, lr:2.000e-04> G_loss: 8.105e-02 
22-10-20 13:37:08.196 : <epoch: 64, iter:  51,600, lr:2.000e-04> G_loss: 5.449e-02 
22-10-20 13:37:49.545 : <epoch: 64, iter:  51,800, lr:2.000e-04> G_loss: 9.435e-02 
22-10-20 13:38:32.129 : <epoch: 64, iter:  52,000, lr:2.000e-04> G_loss: 4.942e-02 
22-10-20 13:39:32.472 : <epoch: 65, iter:  52,200, lr:2.000e-04> G_loss: 8.094e-02 
22-10-20 13:40:13.708 : <epoch: 65, iter:  52,400, lr:2.000e-04> G_loss: 9.027e-02 
22-10-20 13:40:55.551 : <epoch: 65, iter:  52,600, lr:2.000e-04> G_loss: 5.576e-02 
22-10-20 14:42:24.621 :   task: swinir_denoising_sr_x2_charbonnier
  model: plain
  gpu_ids: [0]
  dist: False
  scale: 2
  n_channels: 1
  path:[
    root: superresolution
    pretrained_netG: superresolution\swinir_denoising_sr_x2_charbonnier\models\50000_G.pth
    pretrained_netE: superresolution\swinir_denoising_sr_x2_charbonnier\models\50000_E.pth
    task: superresolution\swinir_denoising_sr_x2_charbonnier
    log: superresolution\swinir_denoising_sr_x2_charbonnier
    options: superresolution\swinir_denoising_sr_x2_charbonnier\options
    models: superresolution\swinir_denoising_sr_x2_charbonnier\models
    images: superresolution\swinir_denoising_sr_x2_charbonnier\images
    pretrained_optimizerG: superresolution\swinir_denoising_sr_x2_charbonnier\models\50000_optimizerG.pth
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr
      dataroot_H: trainsets/trainH
      dataroot_L: trainsets/X4_lq_x2_lr
      H_size: 96
      dataloader_shuffle: True
      dataloader_num_workers: 12
      dataloader_batch_size: 1
      phase: train
      scale: 2
      n_channels: 1
    ]
    test:[
      name: test_dataset
      dataset_type: sr
      dataroot_H: testsets/set12
      dataroot_L: testsets/set12_valid_lq_x2_lr
      phase: test
      scale: 2
      n_channels: 1
    ]
  ]
  netG:[
    net_type: swinir
    upscale: 2
    in_chans: 1
    img_size: 48
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    upsampler: pixelshuffle
    resi_connection: 1conv
    init_type: default
    scale: 2
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    E_decay: 0.999
    G_optimizer_type: adam
    G_optimizer_lr: 0.0002
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_optimizer_reuse: True
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [250000, 400000, 450000, 475000, 500000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    G_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
  ]
  opt_path: options\swinir\train_swinir_sr_classical.json
  is_train: True
  merge_bn: False
  merge_bn_startpoint: -1
  find_unused_parameters: True
  num_gpu: 1
  rank: 0
  world_size: 1

22-10-20 14:42:24.658 : Number of train images: 800, iters: 800
22-10-20 14:42:31.153 : 
Networks name: SwinIR
Params number: 11748093
Net structure:
SwinIR(
  (conv_first): Conv2d(1, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (patch_embed): PatchEmbed(
    (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  )
  (patch_unembed): PatchUnEmbed()
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (1): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (2): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (3): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (4): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (5): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
  )
  (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  (conv_after_body): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_before_upsample): Sequential(
    (0): Conv2d(180, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
  )
  (upsample): Upsample(
    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): PixelShuffle(upscale_factor=2)
  )
  (conv_last): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

22-10-20 14:42:31.338 : 
 |  mean  |  min   |  max   |  std   || shape               
 |  0.003 | -0.505 |  0.488 |  0.221 | torch.Size([180, 1, 3, 3]) || conv_first.weight
 | -0.009 | -0.371 |  0.342 |  0.203 | torch.Size([180]) || conv_first.bias
 |  1.021 |  0.961 |  1.071 |  0.022 | torch.Size([180]) || patch_embed.norm.weight
 |  0.001 | -0.041 |  0.073 |  0.024 | torch.Size([180]) || patch_embed.norm.bias
 |  0.996 |  0.906 |  1.053 |  0.025 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.weight
 |  0.001 | -0.084 |  0.121 |  0.030 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.bias
 | -0.007 | -0.100 |  0.117 |  0.033 | torch.Size([225, 6]) || layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.152 |  0.195 |  0.032 | torch.Size([540, 180]) || layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 | -0.091 |  0.041 |  0.020 | torch.Size([540]) || layers.0.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.233 |  0.259 |  0.028 | torch.Size([180, 180]) || layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.001 | -0.041 |  0.046 |  0.026 | torch.Size([180]) || layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.017 |  0.931 |  1.093 |  0.031 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 | -0.048 |  0.044 |  0.030 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.261 |  0.261 |  0.033 | torch.Size([360, 180]) || layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.012 | -0.032 |  0.043 |  0.020 | torch.Size([360]) || layers.0.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.143 |  0.157 |  0.031 | torch.Size([180, 360]) || layers.0.residual_group.blocks.0.mlp.fc2.weight
 | -0.000 | -0.043 |  0.040 |  0.023 | torch.Size([180]) || layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.1.attn_mask
 |  1.006 |  0.945 |  1.035 |  0.018 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.weight
 | -0.001 | -0.046 |  0.039 |  0.020 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.bias
 | -0.003 | -0.074 |  0.074 |  0.024 | torch.Size([225, 6]) || layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.112 |  0.119 |  0.029 | torch.Size([540, 180]) || layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 | -0.044 |  0.043 |  0.022 | torch.Size([540]) || layers.0.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.096 |  0.115 |  0.026 | torch.Size([180, 180]) || layers.0.residual_group.blocks.1.attn.proj.weight
 | -0.000 | -0.042 |  0.040 |  0.022 | torch.Size([180]) || layers.0.residual_group.blocks.1.attn.proj.bias
 |  0.989 |  0.924 |  1.061 |  0.023 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.weight
 |  0.001 | -0.045 |  0.080 |  0.021 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.102 |  0.107 |  0.025 | torch.Size([360, 180]) || layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.004 | -0.024 |  0.044 |  0.016 | torch.Size([360]) || layers.0.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.121 |  0.107 |  0.025 | torch.Size([180, 360]) || layers.0.residual_group.blocks.1.mlp.fc2.weight
 | -0.000 | -0.042 |  0.041 |  0.023 | torch.Size([180]) || layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  0.999 |  0.959 |  1.035 |  0.017 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.weight
 | -0.001 | -0.034 |  0.040 |  0.018 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.bias
 | -0.002 | -0.065 |  0.073 |  0.022 | torch.Size([225, 6]) || layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.121 |  0.109 |  0.027 | torch.Size([540, 180]) || layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 | -0.050 |  0.048 |  0.019 | torch.Size([540]) || layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.112 |  0.104 |  0.025 | torch.Size([180, 180]) || layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 | -0.041 |  0.040 |  0.023 | torch.Size([180]) || layers.0.residual_group.blocks.2.attn.proj.bias
 |  0.981 |  0.954 |  1.021 |  0.013 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.weight
 |  0.002 | -0.046 |  0.056 |  0.019 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.098 |  0.105 |  0.024 | torch.Size([360, 180]) || layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.001 | -0.025 |  0.041 |  0.012 | torch.Size([360]) || layers.0.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.108 |  0.100 |  0.024 | torch.Size([180, 360]) || layers.0.residual_group.blocks.2.mlp.fc2.weight
 | -0.000 | -0.041 |  0.040 |  0.023 | torch.Size([180]) || layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.3.attn_mask
 |  0.999 |  0.959 |  1.038 |  0.016 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.weight
 | -0.001 | -0.039 |  0.039 |  0.020 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.bias
 | -0.002 | -0.081 |  0.081 |  0.023 | torch.Size([225, 6]) || layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.112 |  0.116 |  0.027 | torch.Size([540, 180]) || layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 | -0.040 |  0.043 |  0.020 | torch.Size([540]) || layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.091 |  0.099 |  0.025 | torch.Size([180, 180]) || layers.0.residual_group.blocks.3.attn.proj.weight
 | -0.000 | -0.044 |  0.040 |  0.025 | torch.Size([180]) || layers.0.residual_group.blocks.3.attn.proj.bias
 |  0.983 |  0.952 |  1.035 |  0.014 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.weight
 |  0.001 | -0.046 |  0.042 |  0.018 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.111 |  0.127 |  0.024 | torch.Size([360, 180]) || layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 | -0.028 |  0.032 |  0.011 | torch.Size([360]) || layers.0.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.100 |  0.103 |  0.024 | torch.Size([180, 360]) || layers.0.residual_group.blocks.3.mlp.fc2.weight
 | -0.000 | -0.042 |  0.040 |  0.025 | torch.Size([180]) || layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  0.990 |  0.961 |  1.026 |  0.013 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.weight
 |  0.000 | -0.033 |  0.034 |  0.016 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.065 |  0.093 |  0.022 | torch.Size([225, 6]) || layers.0.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.108 |  0.111 |  0.025 | torch.Size([540, 180]) || layers.0.residual_group.blocks.4.attn.qkv.weight
 | -0.000 | -0.040 |  0.043 |  0.017 | torch.Size([540]) || layers.0.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.106 |  0.094 |  0.024 | torch.Size([180, 180]) || layers.0.residual_group.blocks.4.attn.proj.weight
 | -0.001 | -0.041 |  0.040 |  0.026 | torch.Size([180]) || layers.0.residual_group.blocks.4.attn.proj.bias
 |  0.984 |  0.952 |  1.028 |  0.014 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.weight
 | -0.001 | -0.041 |  0.045 |  0.017 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.101 |  0.131 |  0.024 | torch.Size([360, 180]) || layers.0.residual_group.blocks.4.mlp.fc1.weight
 |  0.001 | -0.040 |  0.041 |  0.014 | torch.Size([360]) || layers.0.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.101 |  0.121 |  0.024 | torch.Size([180, 360]) || layers.0.residual_group.blocks.4.mlp.fc2.weight
 | -0.001 | -0.040 |  0.040 |  0.026 | torch.Size([180]) || layers.0.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.5.attn_mask
 |  0.994 |  0.956 |  1.029 |  0.015 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.weight
 | -0.001 | -0.045 |  0.044 |  0.018 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.092 |  0.087 |  0.025 | torch.Size([225, 6]) || layers.0.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.103 |  0.104 |  0.025 | torch.Size([540, 180]) || layers.0.residual_group.blocks.5.attn.qkv.weight
 |  0.000 | -0.040 |  0.042 |  0.017 | torch.Size([540]) || layers.0.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.108 |  0.100 |  0.024 | torch.Size([180, 180]) || layers.0.residual_group.blocks.5.attn.proj.weight
 | -0.001 | -0.041 |  0.042 |  0.027 | torch.Size([180]) || layers.0.residual_group.blocks.5.attn.proj.bias
 |  0.989 |  0.960 |  1.033 |  0.014 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.weight
 | -0.000 | -0.043 |  0.045 |  0.021 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.114 |  0.111 |  0.025 | torch.Size([360, 180]) || layers.0.residual_group.blocks.5.mlp.fc1.weight
 |  0.001 | -0.039 |  0.037 |  0.014 | torch.Size([360]) || layers.0.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.117 |  0.094 |  0.024 | torch.Size([180, 360]) || layers.0.residual_group.blocks.5.mlp.fc2.weight
 | -0.000 | -0.041 |  0.043 |  0.028 | torch.Size([180]) || layers.0.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.103 |  0.092 |  0.022 | torch.Size([180, 180, 3, 3]) || layers.0.conv.weight
 |  0.003 | -0.060 |  0.061 |  0.030 | torch.Size([180]) || layers.0.conv.bias
 |  0.997 |  0.968 |  1.023 |  0.013 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.weight
 | -0.000 | -0.038 |  0.037 |  0.022 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.064 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.103 |  0.111 |  0.023 | torch.Size([540, 180]) || layers.1.residual_group.blocks.0.attn.qkv.weight
 | -0.000 | -0.040 |  0.038 |  0.016 | torch.Size([540]) || layers.1.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.092 |  0.103 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 | -0.036 |  0.036 |  0.022 | torch.Size([180]) || layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.001 |  0.966 |  1.042 |  0.014 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.weight
 |  0.002 | -0.041 |  0.040 |  0.019 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.096 |  0.106 |  0.025 | torch.Size([360, 180]) || layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.004 | -0.034 |  0.042 |  0.018 | torch.Size([360]) || layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.096 |  0.106 |  0.024 | torch.Size([180, 360]) || layers.1.residual_group.blocks.0.mlp.fc2.weight
 | -0.000 | -0.043 |  0.043 |  0.026 | torch.Size([180]) || layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.1.attn_mask
 |  1.003 |  0.958 |  1.032 |  0.016 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 | -0.047 |  0.044 |  0.027 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.bias
 |  0.001 | -0.071 |  0.078 |  0.022 | torch.Size([225, 6]) || layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.110 |  0.108 |  0.025 | torch.Size([540, 180]) || layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 | -0.044 |  0.048 |  0.021 | torch.Size([540]) || layers.1.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.097 |  0.097 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 | -0.044 |  0.043 |  0.027 | torch.Size([180]) || layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.002 |  0.965 |  1.039 |  0.015 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.weight
 |  0.001 | -0.045 |  0.045 |  0.019 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.109 |  0.107 |  0.025 | torch.Size([360, 180]) || layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.004 | -0.033 |  0.042 |  0.018 | torch.Size([360]) || layers.1.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.095 |  0.099 |  0.024 | torch.Size([180, 360]) || layers.1.residual_group.blocks.1.mlp.fc2.weight
 | -0.000 | -0.044 |  0.043 |  0.027 | torch.Size([180]) || layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  0.996 |  0.958 |  1.026 |  0.013 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.weight
 |  0.001 | -0.038 |  0.043 |  0.022 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.bias
 |  0.001 | -0.071 |  0.063 |  0.021 | torch.Size([225, 6]) || layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.100 |  0.097 |  0.023 | torch.Size([540, 180]) || layers.1.residual_group.blocks.2.attn.qkv.weight
 | -0.001 | -0.045 |  0.050 |  0.016 | torch.Size([540]) || layers.1.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.095 |  0.087 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.2.attn.proj.weight
 | -0.000 | -0.044 |  0.044 |  0.028 | torch.Size([180]) || layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.006 |  0.961 |  1.039 |  0.016 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.weight
 |  0.001 | -0.045 |  0.048 |  0.024 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.108 |  0.104 |  0.025 | torch.Size([360, 180]) || layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.009 | -0.042 |  0.044 |  0.019 | torch.Size([360]) || layers.1.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.097 |  0.102 |  0.025 | torch.Size([180, 360]) || layers.1.residual_group.blocks.2.mlp.fc2.weight
 | -0.000 | -0.044 |  0.044 |  0.028 | torch.Size([180]) || layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.3.attn_mask
 |  1.001 |  0.968 |  1.030 |  0.012 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.weight
 |  0.001 | -0.039 |  0.038 |  0.020 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.065 |  0.077 |  0.021 | torch.Size([225, 6]) || layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.104 |  0.101 |  0.025 | torch.Size([540, 180]) || layers.1.residual_group.blocks.3.attn.qkv.weight
 | -0.001 | -0.041 |  0.040 |  0.017 | torch.Size([540]) || layers.1.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.119 |  0.095 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.3.attn.proj.weight
 | -0.001 | -0.039 |  0.038 |  0.024 | torch.Size([180]) || layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.008 |  0.966 |  1.055 |  0.015 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.weight
 |  0.001 | -0.040 |  0.043 |  0.021 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.108 |  0.104 |  0.025 | torch.Size([360, 180]) || layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.008 | -0.033 |  0.043 |  0.018 | torch.Size([360]) || layers.1.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.103 |  0.108 |  0.024 | torch.Size([180, 360]) || layers.1.residual_group.blocks.3.mlp.fc2.weight
 | -0.000 | -0.044 |  0.044 |  0.028 | torch.Size([180]) || layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  0.999 |  0.966 |  1.030 |  0.015 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.weight
 | -0.000 | -0.043 |  0.045 |  0.027 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.067 |  0.077 |  0.022 | torch.Size([225, 6]) || layers.1.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.099 |  0.097 |  0.024 | torch.Size([540, 180]) || layers.1.residual_group.blocks.4.attn.qkv.weight
 | -0.001 | -0.043 |  0.044 |  0.018 | torch.Size([540]) || layers.1.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.099 |  0.097 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.4.attn.proj.weight
 | -0.000 | -0.044 |  0.041 |  0.028 | torch.Size([180]) || layers.1.residual_group.blocks.4.attn.proj.bias
 |  1.008 |  0.968 |  1.052 |  0.015 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.weight
 | -0.000 | -0.040 |  0.042 |  0.022 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.099 |  0.101 |  0.025 | torch.Size([360, 180]) || layers.1.residual_group.blocks.4.mlp.fc1.weight
 |  0.009 | -0.035 |  0.043 |  0.018 | torch.Size([360]) || layers.1.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.110 |  0.103 |  0.024 | torch.Size([180, 360]) || layers.1.residual_group.blocks.4.mlp.fc2.weight
 | -0.000 | -0.046 |  0.044 |  0.030 | torch.Size([180]) || layers.1.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.5.attn_mask
 |  1.003 |  0.961 |  1.043 |  0.015 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.weight
 |  0.001 | -0.049 |  0.048 |  0.025 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.bias
 |  0.001 | -0.078 |  0.073 |  0.022 | torch.Size([225, 6]) || layers.1.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.103 |  0.099 |  0.024 | torch.Size([540, 180]) || layers.1.residual_group.blocks.5.attn.qkv.weight
 | -0.001 | -0.045 |  0.045 |  0.019 | torch.Size([540]) || layers.1.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.097 |  0.091 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.5.attn.proj.weight
 | -0.000 | -0.045 |  0.045 |  0.030 | torch.Size([180]) || layers.1.residual_group.blocks.5.attn.proj.bias
 |  1.011 |  0.967 |  1.049 |  0.016 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.weight
 |  0.002 | -0.042 |  0.044 |  0.022 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.107 |  0.108 |  0.026 | torch.Size([360, 180]) || layers.1.residual_group.blocks.5.mlp.fc1.weight
 |  0.010 | -0.040 |  0.044 |  0.018 | torch.Size([360]) || layers.1.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.099 |  0.097 |  0.024 | torch.Size([180, 360]) || layers.1.residual_group.blocks.5.mlp.fc2.weight
 | -0.000 | -0.045 |  0.047 |  0.030 | torch.Size([180]) || layers.1.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.110 |  0.132 |  0.022 | torch.Size([180, 180, 3, 3]) || layers.1.conv.weight
 | -0.000 | -0.055 |  0.058 |  0.028 | torch.Size([180]) || layers.1.conv.bias
 |  0.996 |  0.977 |  1.012 |  0.007 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.weight
 | -0.000 | -0.032 |  0.029 |  0.013 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.bias
 |  0.001 | -0.070 |  0.072 |  0.021 | torch.Size([225, 6]) || layers.2.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.094 |  0.097 |  0.021 | torch.Size([540, 180]) || layers.2.residual_group.blocks.0.attn.qkv.weight
 |  0.001 | -0.028 |  0.030 |  0.011 | torch.Size([540]) || layers.2.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.094 |  0.092 |  0.022 | torch.Size([180, 180]) || layers.2.residual_group.blocks.0.attn.proj.weight
 | -0.001 | -0.032 |  0.032 |  0.021 | torch.Size([180]) || layers.2.residual_group.blocks.0.attn.proj.bias
 |  1.011 |  0.972 |  1.042 |  0.015 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.weight
 | -0.000 | -0.042 |  0.043 |  0.019 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.122 |  0.109 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.0.mlp.fc1.weight
 |  0.009 | -0.034 |  0.050 |  0.019 | torch.Size([360]) || layers.2.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.087 |  0.106 |  0.022 | torch.Size([180, 360]) || layers.2.residual_group.blocks.0.mlp.fc2.weight
 | -0.001 | -0.040 |  0.040 |  0.025 | torch.Size([180]) || layers.2.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.1.attn_mask
 |  0.997 |  0.968 |  1.025 |  0.010 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.weight
 |  0.001 | -0.040 |  0.039 |  0.020 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.bias
 | -0.001 | -0.063 |  0.071 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.099 |  0.088 |  0.022 | torch.Size([540, 180]) || layers.2.residual_group.blocks.1.attn.qkv.weight
 |  0.000 | -0.040 |  0.042 |  0.016 | torch.Size([540]) || layers.2.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.085 |  0.093 |  0.022 | torch.Size([180, 180]) || layers.2.residual_group.blocks.1.attn.proj.weight
 | -0.001 | -0.038 |  0.039 |  0.025 | torch.Size([180]) || layers.2.residual_group.blocks.1.attn.proj.bias
 |  1.011 |  0.977 |  1.045 |  0.014 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.weight
 |  0.001 | -0.045 |  0.042 |  0.022 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.103 |  0.103 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.1.mlp.fc1.weight
 |  0.007 | -0.039 |  0.045 |  0.021 | torch.Size([360]) || layers.2.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.102 |  0.093 |  0.022 | torch.Size([180, 360]) || layers.2.residual_group.blocks.1.mlp.fc2.weight
 | -0.001 | -0.038 |  0.039 |  0.025 | torch.Size([180]) || layers.2.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.978 |  1.026 |  0.010 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.weight
 |  0.000 | -0.036 |  0.036 |  0.018 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.bias
 | -0.001 | -0.063 |  0.072 |  0.021 | torch.Size([225, 6]) || layers.2.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.096 |  0.091 |  0.022 | torch.Size([540, 180]) || layers.2.residual_group.blocks.2.attn.qkv.weight
 | -0.001 | -0.037 |  0.037 |  0.015 | torch.Size([540]) || layers.2.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.089 |  0.093 |  0.023 | torch.Size([180, 180]) || layers.2.residual_group.blocks.2.attn.proj.weight
 | -0.001 | -0.036 |  0.037 |  0.024 | torch.Size([180]) || layers.2.residual_group.blocks.2.attn.proj.bias
 |  1.010 |  0.975 |  1.041 |  0.013 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.weight
 | -0.000 | -0.041 |  0.041 |  0.018 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.098 |  0.125 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.2.mlp.fc1.weight
 |  0.009 | -0.040 |  0.042 |  0.021 | torch.Size([360]) || layers.2.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.089 |  0.106 |  0.023 | torch.Size([180, 360]) || layers.2.residual_group.blocks.2.mlp.fc2.weight
 | -0.001 | -0.038 |  0.038 |  0.026 | torch.Size([180]) || layers.2.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.3.attn_mask
 |  0.997 |  0.971 |  1.030 |  0.010 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.weight
 |  0.000 | -0.045 |  0.044 |  0.021 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.075 |  0.066 |  0.021 | torch.Size([225, 6]) || layers.2.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.098 |  0.100 |  0.022 | torch.Size([540, 180]) || layers.2.residual_group.blocks.3.attn.qkv.weight
 |  0.000 | -0.041 |  0.039 |  0.016 | torch.Size([540]) || layers.2.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.095 |  0.089 |  0.022 | torch.Size([180, 180]) || layers.2.residual_group.blocks.3.attn.proj.weight
 | -0.001 | -0.039 |  0.038 |  0.026 | torch.Size([180]) || layers.2.residual_group.blocks.3.attn.proj.bias
 |  1.010 |  0.971 |  1.038 |  0.014 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.weight
 | -0.000 | -0.047 |  0.044 |  0.019 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.104 |  0.097 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.3.mlp.fc1.weight
 |  0.007 | -0.037 |  0.042 |  0.022 | torch.Size([360]) || layers.2.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.104 |  0.096 |  0.023 | torch.Size([180, 360]) || layers.2.residual_group.blocks.3.mlp.fc2.weight
 | -0.001 | -0.038 |  0.037 |  0.026 | torch.Size([180]) || layers.2.residual_group.blocks.3.mlp.fc2.bias
 |  0.998 |  0.973 |  1.025 |  0.010 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.weight
 |  0.001 | -0.048 |  0.045 |  0.023 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.073 |  0.080 |  0.022 | torch.Size([225, 6]) || layers.2.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.098 |  0.098 |  0.022 | torch.Size([540, 180]) || layers.2.residual_group.blocks.4.attn.qkv.weight
 |  0.000 | -0.045 |  0.041 |  0.017 | torch.Size([540]) || layers.2.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.088 |  0.088 |  0.022 | torch.Size([180, 180]) || layers.2.residual_group.blocks.4.attn.proj.weight
 | -0.001 | -0.039 |  0.038 |  0.026 | torch.Size([180]) || layers.2.residual_group.blocks.4.attn.proj.bias
 |  1.009 |  0.978 |  1.040 |  0.012 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.weight
 | -0.001 | -0.038 |  0.038 |  0.019 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.107 |  0.102 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.4.mlp.fc1.weight
 |  0.008 | -0.040 |  0.041 |  0.021 | torch.Size([360]) || layers.2.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.105 |  0.098 |  0.023 | torch.Size([180, 360]) || layers.2.residual_group.blocks.4.mlp.fc2.weight
 | -0.001 | -0.039 |  0.038 |  0.026 | torch.Size([180]) || layers.2.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.5.attn_mask
 |  0.996 |  0.973 |  1.016 |  0.009 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.weight
 |  0.000 | -0.031 |  0.037 |  0.014 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.bias
 |  0.001 | -0.067 |  0.075 |  0.022 | torch.Size([225, 6]) || layers.2.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.095 |  0.088 |  0.022 | torch.Size([540, 180]) || layers.2.residual_group.blocks.5.attn.qkv.weight
 |  0.000 | -0.036 |  0.033 |  0.013 | torch.Size([540]) || layers.2.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.078 |  0.080 |  0.021 | torch.Size([180, 180]) || layers.2.residual_group.blocks.5.attn.proj.weight
 | -0.001 | -0.032 |  0.032 |  0.022 | torch.Size([180]) || layers.2.residual_group.blocks.5.attn.proj.bias
 |  1.010 |  0.978 |  1.039 |  0.014 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.weight
 | -0.000 | -0.045 |  0.041 |  0.021 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.100 |  0.113 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.5.mlp.fc1.weight
 |  0.009 | -0.040 |  0.046 |  0.023 | torch.Size([360]) || layers.2.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.102 |  0.090 |  0.023 | torch.Size([180, 360]) || layers.2.residual_group.blocks.5.mlp.fc2.weight
 | -0.001 | -0.038 |  0.038 |  0.027 | torch.Size([180]) || layers.2.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.200 |  0.207 |  0.024 | torch.Size([180, 180, 3, 3]) || layers.2.conv.weight
 | -0.000 | -0.048 |  0.049 |  0.020 | torch.Size([180]) || layers.2.conv.bias
 |  1.000 |  0.984 |  1.020 |  0.006 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.weight
 | -0.001 | -0.025 |  0.029 |  0.015 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.062 |  0.068 |  0.021 | torch.Size([225, 6]) || layers.3.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.089 |  0.086 |  0.021 | torch.Size([540, 180]) || layers.3.residual_group.blocks.0.attn.qkv.weight
 | -0.000 | -0.028 |  0.027 |  0.011 | torch.Size([540]) || layers.3.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.091 |  0.093 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.0.attn.proj.weight
 |  0.000 | -0.025 |  0.027 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.0.attn.proj.bias
 |  1.005 |  0.989 |  1.030 |  0.008 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.weight
 |  0.001 | -0.029 |  0.031 |  0.016 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.093 |  0.100 |  0.021 | torch.Size([360, 180]) || layers.3.residual_group.blocks.0.mlp.fc1.weight
 |  0.004 | -0.022 |  0.030 |  0.013 | torch.Size([360]) || layers.3.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.090 |  0.098 |  0.021 | torch.Size([180, 360]) || layers.3.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 | -0.027 |  0.027 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.1.attn_mask
 |  1.001 |  0.992 |  1.019 |  0.005 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.weight
 | -0.000 | -0.028 |  0.030 |  0.015 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.069 |  0.063 |  0.021 | torch.Size([225, 6]) || layers.3.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.101 |  0.095 |  0.021 | torch.Size([540, 180]) || layers.3.residual_group.blocks.1.attn.qkv.weight
 | -0.000 | -0.029 |  0.028 |  0.010 | torch.Size([540]) || layers.3.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.087 |  0.086 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.1.attn.proj.weight
 |  0.000 | -0.026 |  0.027 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.1.attn.proj.bias
 |  1.004 |  0.992 |  1.026 |  0.007 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.weight
 |  0.000 | -0.024 |  0.027 |  0.012 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.086 |  0.089 |  0.021 | torch.Size([360, 180]) || layers.3.residual_group.blocks.1.mlp.fc1.weight
 |  0.004 | -0.017 |  0.024 |  0.009 | torch.Size([360]) || layers.3.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.093 |  0.090 |  0.021 | torch.Size([180, 360]) || layers.3.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 | -0.021 |  0.022 |  0.013 | torch.Size([180]) || layers.3.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.990 |  1.017 |  0.005 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.weight
 |  0.000 | -0.030 |  0.029 |  0.015 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.060 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.090 |  0.100 |  0.021 | torch.Size([540, 180]) || layers.3.residual_group.blocks.2.attn.qkv.weight
 |  0.001 | -0.027 |  0.028 |  0.010 | torch.Size([540]) || layers.3.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.094 |  0.077 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.2.attn.proj.weight
 |  0.001 | -0.027 |  0.028 |  0.018 | torch.Size([180]) || layers.3.residual_group.blocks.2.attn.proj.bias
 |  1.005 |  0.987 |  1.025 |  0.008 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.weight
 | -0.000 | -0.031 |  0.034 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.100 |  0.087 |  0.021 | torch.Size([360, 180]) || layers.3.residual_group.blocks.2.mlp.fc1.weight
 |  0.004 | -0.022 |  0.029 |  0.013 | torch.Size([360]) || layers.3.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.096 |  0.095 |  0.021 | torch.Size([180, 360]) || layers.3.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 | -0.026 |  0.026 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.3.attn_mask
 |  1.001 |  0.986 |  1.018 |  0.005 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.weight
 | -0.000 | -0.028 |  0.024 |  0.014 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.072 |  0.071 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.099 |  0.092 |  0.022 | torch.Size([540, 180]) || layers.3.residual_group.blocks.3.attn.qkv.weight
 | -0.000 | -0.026 |  0.024 |  0.010 | torch.Size([540]) || layers.3.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.082 |  0.086 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.3.attn.proj.weight
 |  0.000 | -0.021 |  0.024 |  0.014 | torch.Size([180]) || layers.3.residual_group.blocks.3.attn.proj.bias
 |  1.003 |  0.990 |  1.016 |  0.006 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.weight
 |  0.001 | -0.023 |  0.021 |  0.012 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.084 |  0.089 |  0.021 | torch.Size([360, 180]) || layers.3.residual_group.blocks.3.mlp.fc1.weight
 |  0.002 | -0.020 |  0.020 |  0.010 | torch.Size([360]) || layers.3.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.102 |  0.080 |  0.021 | torch.Size([180, 360]) || layers.3.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 | -0.020 |  0.019 |  0.013 | torch.Size([180]) || layers.3.residual_group.blocks.3.mlp.fc2.bias
 |  1.001 |  0.985 |  1.018 |  0.007 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.weight
 |  0.000 | -0.030 |  0.026 |  0.016 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.062 |  0.071 |  0.021 | torch.Size([225, 6]) || layers.3.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.087 |  0.090 |  0.021 | torch.Size([540, 180]) || layers.3.residual_group.blocks.4.attn.qkv.weight
 | -0.000 | -0.028 |  0.027 |  0.011 | torch.Size([540]) || layers.3.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.097 |  0.081 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.4.attn.proj.weight
 |  0.001 | -0.026 |  0.026 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.4.attn.proj.bias
 |  1.005 |  0.986 |  1.022 |  0.007 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.weight
 |  0.001 | -0.026 |  0.025 |  0.014 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.089 |  0.096 |  0.021 | torch.Size([360, 180]) || layers.3.residual_group.blocks.4.mlp.fc1.weight
 |  0.003 | -0.020 |  0.027 |  0.012 | torch.Size([360]) || layers.3.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.101 |  0.099 |  0.021 | torch.Size([180, 360]) || layers.3.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 | -0.022 |  0.023 |  0.015 | torch.Size([180]) || layers.3.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.5.attn_mask
 |  1.000 |  0.987 |  1.017 |  0.006 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.weight
 |  0.000 | -0.026 |  0.028 |  0.014 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.070 |  0.074 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.084 |  0.094 |  0.021 | torch.Size([540, 180]) || layers.3.residual_group.blocks.5.attn.qkv.weight
 | -0.000 | -0.027 |  0.031 |  0.011 | torch.Size([540]) || layers.3.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.092 |  0.080 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.5.attn.proj.weight
 |  0.000 | -0.025 |  0.026 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.5.attn.proj.bias
 |  1.006 |  0.987 |  1.029 |  0.009 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.weight
 | -0.001 | -0.028 |  0.030 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.104 |  0.091 |  0.022 | torch.Size([360, 180]) || layers.3.residual_group.blocks.5.mlp.fc1.weight
 |  0.004 | -0.024 |  0.034 |  0.014 | torch.Size([360]) || layers.3.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.091 |  0.089 |  0.022 | torch.Size([180, 360]) || layers.3.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 | -0.027 |  0.027 |  0.018 | torch.Size([180]) || layers.3.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.432 |  0.456 |  0.031 | torch.Size([180, 180, 3, 3]) || layers.3.conv.weight
 | -0.000 | -0.028 |  0.031 |  0.015 | torch.Size([180]) || layers.3.conv.bias
 |  1.000 |  0.992 |  1.013 |  0.003 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.weight
 | -0.000 | -0.012 |  0.014 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.062 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.089 |  0.090 |  0.021 | torch.Size([540, 180]) || layers.4.residual_group.blocks.0.attn.qkv.weight
 |  0.000 | -0.013 |  0.013 |  0.004 | torch.Size([540]) || layers.4.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.078 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.0.attn.proj.weight
 | -0.000 | -0.013 |  0.012 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.0.attn.proj.bias
 |  1.001 |  0.992 |  1.014 |  0.004 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.weight
 | -0.001 | -0.017 |  0.012 |  0.007 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.093 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.0.mlp.fc1.weight
 |  0.001 | -0.009 |  0.015 |  0.005 | torch.Size([360]) || layers.4.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.095 |  0.090 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.0.mlp.fc2.weight
 | -0.000 | -0.013 |  0.012 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.1.attn_mask
 |  1.000 |  0.991 |  1.011 |  0.004 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.weight
 | -0.000 | -0.014 |  0.013 |  0.007 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.080 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.092 |  0.097 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.1.attn.qkv.weight
 | -0.000 | -0.017 |  0.016 |  0.005 | torch.Size([540]) || layers.4.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.091 |  0.084 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.1.attn.proj.weight
 |  0.000 | -0.011 |  0.011 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.1.attn.proj.bias
 |  1.002 |  0.990 |  1.021 |  0.004 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.weight
 | -0.000 | -0.014 |  0.015 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.095 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.1.mlp.fc1.weight
 |  0.001 | -0.009 |  0.016 |  0.005 | torch.Size([360]) || layers.4.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.088 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.1.mlp.fc2.weight
 | -0.000 | -0.012 |  0.011 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.992 |  1.008 |  0.003 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.weight
 | -0.000 | -0.013 |  0.011 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.075 |  0.068 |  0.021 | torch.Size([225, 6]) || layers.4.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.095 |  0.097 |  0.021 | torch.Size([540, 180]) || layers.4.residual_group.blocks.2.attn.qkv.weight
 |  0.000 | -0.012 |  0.015 |  0.004 | torch.Size([540]) || layers.4.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.079 |  0.090 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.2.attn.proj.weight
 | -0.000 | -0.013 |  0.011 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.2.attn.proj.bias
 |  1.003 |  0.989 |  1.019 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.weight
 |  0.000 | -0.016 |  0.017 |  0.008 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.087 |  0.088 |  0.021 | torch.Size([360, 180]) || layers.4.residual_group.blocks.2.mlp.fc1.weight
 |  0.001 | -0.010 |  0.021 |  0.006 | torch.Size([360]) || layers.4.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.089 |  0.099 |  0.021 | torch.Size([180, 360]) || layers.4.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 | -0.012 |  0.012 |  0.007 | torch.Size([180]) || layers.4.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.3.attn_mask
 |  1.001 |  0.992 |  1.011 |  0.003 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.weight
 | -0.000 | -0.013 |  0.014 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.088 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.102 |  0.091 |  0.021 | torch.Size([540, 180]) || layers.4.residual_group.blocks.3.attn.qkv.weight
 | -0.000 | -0.013 |  0.015 |  0.005 | torch.Size([540]) || layers.4.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.084 |  0.086 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.3.attn.proj.weight
 | -0.000 | -0.012 |  0.011 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.3.attn.proj.bias
 |  1.003 |  0.994 |  1.026 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.weight
 | -0.000 | -0.016 |  0.015 |  0.007 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.089 |  0.082 |  0.021 | torch.Size([360, 180]) || layers.4.residual_group.blocks.3.mlp.fc1.weight
 |  0.001 | -0.010 |  0.019 |  0.005 | torch.Size([360]) || layers.4.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.091 |  0.096 |  0.021 | torch.Size([180, 360]) || layers.4.residual_group.blocks.3.mlp.fc2.weight
 | -0.000 | -0.012 |  0.010 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.3.mlp.fc2.bias
 |  1.001 |  0.994 |  1.009 |  0.003 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.weight
 |  0.000 | -0.011 |  0.014 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.bias
 |  0.000 | -0.072 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.088 |  0.091 |  0.021 | torch.Size([540, 180]) || layers.4.residual_group.blocks.4.attn.qkv.weight
 | -0.001 | -0.012 |  0.009 |  0.004 | torch.Size([540]) || layers.4.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.083 |  0.087 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.4.attn.proj.weight
 | -0.000 | -0.013 |  0.012 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.4.attn.proj.bias
 |  1.003 |  0.993 |  1.018 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.weight
 | -0.000 | -0.018 |  0.016 |  0.008 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.086 |  0.078 |  0.021 | torch.Size([360, 180]) || layers.4.residual_group.blocks.4.mlp.fc1.weight
 |  0.002 | -0.009 |  0.016 |  0.005 | torch.Size([360]) || layers.4.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.088 |  0.087 |  0.021 | torch.Size([180, 360]) || layers.4.residual_group.blocks.4.mlp.fc2.weight
 | -0.000 | -0.012 |  0.011 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.5.attn_mask
 |  1.000 |  0.991 |  1.011 |  0.003 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.weight
 | -0.000 | -0.010 |  0.008 |  0.004 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.062 |  0.053 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.082 |  0.091 |  0.021 | torch.Size([540, 180]) || layers.4.residual_group.blocks.5.attn.qkv.weight
 | -0.000 | -0.013 |  0.014 |  0.005 | torch.Size([540]) || layers.4.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.085 |  0.097 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.5.attn.proj.weight
 | -0.000 | -0.010 |  0.011 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.5.attn.proj.bias
 |  1.002 |  0.990 |  1.021 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.weight
 | -0.001 | -0.016 |  0.013 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.092 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.5.mlp.fc1.weight
 |  0.001 | -0.016 |  0.016 |  0.005 | torch.Size([360]) || layers.4.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.088 |  0.086 |  0.021 | torch.Size([180, 360]) || layers.4.residual_group.blocks.5.mlp.fc2.weight
 | -0.000 | -0.014 |  0.012 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.749 |  0.758 |  0.043 | torch.Size([180, 180, 3, 3]) || layers.4.conv.weight
 |  0.001 | -0.028 |  0.031 |  0.016 | torch.Size([180]) || layers.4.conv.bias
 |  1.001 |  0.995 |  1.009 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.weight
 | -0.000 | -0.012 |  0.014 |  0.004 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.065 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.094 |  0.090 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.0.attn.qkv.weight
 |  0.000 | -0.014 |  0.013 |  0.004 | torch.Size([540]) || layers.5.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.083 |  0.089 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.0.attn.proj.weight
 |  0.000 | -0.007 |  0.008 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.0.attn.proj.bias
 |  1.001 |  0.994 |  1.011 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.weight
 |  0.000 | -0.006 |  0.004 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.098 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 | -0.007 |  0.012 |  0.002 | torch.Size([360]) || layers.5.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.084 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 | -0.005 |  0.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.1.attn_mask
 |  1.001 |  0.996 |  1.015 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.weight
 |  0.000 | -0.010 |  0.010 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.060 |  0.066 |  0.021 | torch.Size([225, 6]) || layers.5.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.088 |  0.083 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.1.attn.qkv.weight
 | -0.000 | -0.013 |  0.010 |  0.003 | torch.Size([540]) || layers.5.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.076 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.1.attn.proj.weight
 |  0.000 | -0.007 |  0.007 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.1.attn.proj.bias
 |  1.003 |  0.995 |  1.023 |  0.004 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.weight
 | -0.000 | -0.011 |  0.012 |  0.004 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.087 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.1.mlp.fc1.weight
 |  0.001 | -0.007 |  0.019 |  0.003 | torch.Size([360]) || layers.5.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.081 |  0.091 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 | -0.006 |  0.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.994 |  1.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.weight
 | -0.000 | -0.008 |  0.005 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.bias
 | -0.001 | -0.082 |  0.060 |  0.021 | torch.Size([225, 6]) || layers.5.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.097 |  0.089 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.2.attn.qkv.weight
 | -0.000 | -0.009 |  0.009 |  0.002 | torch.Size([540]) || layers.5.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.080 |  0.087 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.2.attn.proj.weight
 |  0.000 | -0.007 |  0.008 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.2.attn.proj.bias
 |  1.002 |  0.994 |  1.013 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.weight
 | -0.000 | -0.008 |  0.011 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.092 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.2.mlp.fc1.weight
 |  0.001 | -0.008 |  0.013 |  0.003 | torch.Size([360]) || layers.5.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.087 |  0.094 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 | -0.006 |  0.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.3.attn_mask
 |  1.000 |  0.995 |  1.009 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.weight
 | -0.000 | -0.005 |  0.008 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.bias
 |  0.001 | -0.059 |  0.074 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.095 |  0.092 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.3.attn.qkv.weight
 |  0.000 | -0.018 |  0.019 |  0.004 | torch.Size([540]) || layers.5.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.076 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.3.attn.proj.weight
 |  0.000 | -0.006 |  0.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.3.attn.proj.bias
 |  1.002 |  0.992 |  1.019 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.weight
 | -0.000 | -0.009 |  0.009 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.086 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.3.mlp.fc1.weight
 |  0.001 | -0.006 |  0.015 |  0.003 | torch.Size([360]) || layers.5.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.105 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  0.997 |  1.009 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.weight
 | -0.000 | -0.007 |  0.010 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.065 |  0.065 |  0.021 | torch.Size([225, 6]) || layers.5.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.085 |  0.093 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.4.attn.qkv.weight
 | -0.000 | -0.010 |  0.010 |  0.003 | torch.Size([540]) || layers.5.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.085 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.4.attn.proj.weight
 |  0.000 | -0.005 |  0.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.4.attn.proj.bias
 |  1.004 |  0.993 |  1.021 |  0.005 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.weight
 | -0.000 | -0.016 |  0.016 |  0.006 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.095 |  0.094 |  0.021 | torch.Size([360, 180]) || layers.5.residual_group.blocks.4.mlp.fc1.weight
 |  0.002 | -0.010 |  0.025 |  0.005 | torch.Size([360]) || layers.5.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.083 |  0.090 |  0.021 | torch.Size([180, 360]) || layers.5.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 | -0.008 |  0.009 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.5.attn_mask
 |  1.000 |  0.995 |  1.008 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.weight
 |  0.000 | -0.007 |  0.007 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.065 |  0.068 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.085 |  0.104 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.5.attn.qkv.weight
 |  0.000 | -0.014 |  0.014 |  0.004 | torch.Size([540]) || layers.5.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.087 |  0.091 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.5.attn.proj.weight
 |  0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.5.attn.proj.bias
 |  1.002 |  0.992 |  1.018 |  0.004 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.weight
 | -0.000 | -0.009 |  0.010 |  0.004 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.087 |  0.085 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.5.mlp.fc1.weight
 |  0.001 | -0.012 |  0.019 |  0.003 | torch.Size([360]) || layers.5.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.088 |  0.096 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 | -0.006 |  0.005 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -1.255 |  1.261 |  0.047 | torch.Size([180, 180, 3, 3]) || layers.5.conv.weight
 |  0.001 | -0.030 |  0.026 |  0.015 | torch.Size([180]) || layers.5.conv.bias
 |  0.426 |  0.276 |  0.702 |  0.052 | torch.Size([180]) || norm.weight
 |  0.027 | -0.484 |  0.640 |  0.060 | torch.Size([180]) || norm.bias
 |  0.000 | -0.103 |  0.083 |  0.017 | torch.Size([180, 180, 3, 3]) || conv_after_body.weight
 | -0.002 | -0.042 |  0.040 |  0.016 | torch.Size([180]) || conv_after_body.bias
 |  0.000 | -0.265 |  0.214 |  0.031 | torch.Size([64, 180, 3, 3]) || conv_before_upsample.0.weight
 | -0.034 | -0.108 |  0.025 |  0.022 | torch.Size([64]) || conv_before_upsample.0.bias
 | -0.000 | -0.325 |  0.231 |  0.033 | torch.Size([256, 64, 3, 3]) || upsample.0.weight
 | -0.002 | -0.120 |  0.111 |  0.035 | torch.Size([256]) || upsample.0.bias
 |  0.000 | -0.044 |  0.051 |  0.009 | torch.Size([1, 64, 3, 3]) || conv_last.weight
 |  0.112 |  0.112 |  0.112 |    nan | torch.Size([1]) || conv_last.bias

22-10-20 20:29:25.365 :   task: swinir_denoising_sr_x2_charbonnier
  model: plain
  gpu_ids: [0]
  dist: False
  scale: 2
  n_channels: 1
  path:[
    root: superresolution
    pretrained_netG: superresolution\swinir_denoising_sr_x2_charbonnier\models\50000_G.pth
    pretrained_netE: superresolution\swinir_denoising_sr_x2_charbonnier\models\50000_E.pth
    task: superresolution\swinir_denoising_sr_x2_charbonnier
    log: superresolution\swinir_denoising_sr_x2_charbonnier
    options: superresolution\swinir_denoising_sr_x2_charbonnier\options
    models: superresolution\swinir_denoising_sr_x2_charbonnier\models
    images: superresolution\swinir_denoising_sr_x2_charbonnier\images
    pretrained_optimizerG: superresolution\swinir_denoising_sr_x2_charbonnier\models\50000_optimizerG.pth
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr
      dataroot_H: trainsets/trainH
      dataroot_L: trainsets/X4_lq_x2_lr
      H_size: 96
      dataloader_shuffle: True
      dataloader_num_workers: 12
      dataloader_batch_size: 1
      phase: train
      scale: 2
      n_channels: 1
    ]
    test:[
      name: test_dataset
      dataset_type: sr
      dataroot_H: testsets/set12
      dataroot_L: testsets/set12_valid_lq_x2_lr
      phase: test
      scale: 2
      n_channels: 1
    ]
  ]
  netG:[
    net_type: swinir
    upscale: 2
    in_chans: 1
    img_size: 48
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    upsampler: pixelshuffle
    resi_connection: 1conv
    init_type: default
    scale: 2
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    E_decay: 0.999
    G_optimizer_type: adam
    G_optimizer_lr: 0.0002
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_optimizer_reuse: True
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [250000, 400000, 450000, 475000, 500000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    G_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
  ]
  opt_path: options\swinir\train_swinir_sr_classical.json
  is_train: True
  merge_bn: False
  merge_bn_startpoint: -1
  find_unused_parameters: True
  num_gpu: 1
  rank: 0
  world_size: 1

22-10-20 20:29:25.380 : Number of train images: 800, iters: 800
22-10-20 20:29:27.618 : 
Networks name: SwinIR
Params number: 11748093
Net structure:
SwinIR(
  (conv_first): Conv2d(1, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (patch_embed): PatchEmbed(
    (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  )
  (patch_unembed): PatchUnEmbed()
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (1): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (2): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (3): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (4): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (5): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
  )
  (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  (conv_after_body): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_before_upsample): Sequential(
    (0): Conv2d(180, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
  )
  (upsample): Upsample(
    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): PixelShuffle(upscale_factor=2)
  )
  (conv_last): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

22-10-20 20:29:27.791 : 
 |  mean  |  min   |  max   |  std   || shape               
 |  0.003 | -0.505 |  0.488 |  0.221 | torch.Size([180, 1, 3, 3]) || conv_first.weight
 | -0.009 | -0.371 |  0.342 |  0.203 | torch.Size([180]) || conv_first.bias
 |  1.021 |  0.961 |  1.071 |  0.022 | torch.Size([180]) || patch_embed.norm.weight
 |  0.001 | -0.041 |  0.073 |  0.024 | torch.Size([180]) || patch_embed.norm.bias
 |  0.996 |  0.906 |  1.053 |  0.025 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.weight
 |  0.001 | -0.084 |  0.121 |  0.030 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.bias
 | -0.007 | -0.100 |  0.117 |  0.033 | torch.Size([225, 6]) || layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.152 |  0.195 |  0.032 | torch.Size([540, 180]) || layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 | -0.091 |  0.041 |  0.020 | torch.Size([540]) || layers.0.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.233 |  0.259 |  0.028 | torch.Size([180, 180]) || layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.001 | -0.041 |  0.046 |  0.026 | torch.Size([180]) || layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.017 |  0.931 |  1.093 |  0.031 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 | -0.048 |  0.044 |  0.030 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.261 |  0.261 |  0.033 | torch.Size([360, 180]) || layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.012 | -0.032 |  0.043 |  0.020 | torch.Size([360]) || layers.0.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.143 |  0.157 |  0.031 | torch.Size([180, 360]) || layers.0.residual_group.blocks.0.mlp.fc2.weight
 | -0.000 | -0.043 |  0.040 |  0.023 | torch.Size([180]) || layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.1.attn_mask
 |  1.006 |  0.945 |  1.035 |  0.018 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.weight
 | -0.001 | -0.046 |  0.039 |  0.020 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.bias
 | -0.003 | -0.074 |  0.074 |  0.024 | torch.Size([225, 6]) || layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.112 |  0.119 |  0.029 | torch.Size([540, 180]) || layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 | -0.044 |  0.043 |  0.022 | torch.Size([540]) || layers.0.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.096 |  0.115 |  0.026 | torch.Size([180, 180]) || layers.0.residual_group.blocks.1.attn.proj.weight
 | -0.000 | -0.042 |  0.040 |  0.022 | torch.Size([180]) || layers.0.residual_group.blocks.1.attn.proj.bias
 |  0.989 |  0.924 |  1.061 |  0.023 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.weight
 |  0.001 | -0.045 |  0.080 |  0.021 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.102 |  0.107 |  0.025 | torch.Size([360, 180]) || layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.004 | -0.024 |  0.044 |  0.016 | torch.Size([360]) || layers.0.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.121 |  0.107 |  0.025 | torch.Size([180, 360]) || layers.0.residual_group.blocks.1.mlp.fc2.weight
 | -0.000 | -0.042 |  0.041 |  0.023 | torch.Size([180]) || layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  0.999 |  0.959 |  1.035 |  0.017 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.weight
 | -0.001 | -0.034 |  0.040 |  0.018 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.bias
 | -0.002 | -0.065 |  0.073 |  0.022 | torch.Size([225, 6]) || layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.121 |  0.109 |  0.027 | torch.Size([540, 180]) || layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 | -0.050 |  0.048 |  0.019 | torch.Size([540]) || layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.112 |  0.104 |  0.025 | torch.Size([180, 180]) || layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 | -0.041 |  0.040 |  0.023 | torch.Size([180]) || layers.0.residual_group.blocks.2.attn.proj.bias
 |  0.981 |  0.954 |  1.021 |  0.013 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.weight
 |  0.002 | -0.046 |  0.056 |  0.019 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.098 |  0.105 |  0.024 | torch.Size([360, 180]) || layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.001 | -0.025 |  0.041 |  0.012 | torch.Size([360]) || layers.0.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.108 |  0.100 |  0.024 | torch.Size([180, 360]) || layers.0.residual_group.blocks.2.mlp.fc2.weight
 | -0.000 | -0.041 |  0.040 |  0.023 | torch.Size([180]) || layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.3.attn_mask
 |  0.999 |  0.959 |  1.038 |  0.016 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.weight
 | -0.001 | -0.039 |  0.039 |  0.020 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.bias
 | -0.002 | -0.081 |  0.081 |  0.023 | torch.Size([225, 6]) || layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.112 |  0.116 |  0.027 | torch.Size([540, 180]) || layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 | -0.040 |  0.043 |  0.020 | torch.Size([540]) || layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.091 |  0.099 |  0.025 | torch.Size([180, 180]) || layers.0.residual_group.blocks.3.attn.proj.weight
 | -0.000 | -0.044 |  0.040 |  0.025 | torch.Size([180]) || layers.0.residual_group.blocks.3.attn.proj.bias
 |  0.983 |  0.952 |  1.035 |  0.014 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.weight
 |  0.001 | -0.046 |  0.042 |  0.018 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.111 |  0.127 |  0.024 | torch.Size([360, 180]) || layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 | -0.028 |  0.032 |  0.011 | torch.Size([360]) || layers.0.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.100 |  0.103 |  0.024 | torch.Size([180, 360]) || layers.0.residual_group.blocks.3.mlp.fc2.weight
 | -0.000 | -0.042 |  0.040 |  0.025 | torch.Size([180]) || layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  0.990 |  0.961 |  1.026 |  0.013 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.weight
 |  0.000 | -0.033 |  0.034 |  0.016 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.065 |  0.093 |  0.022 | torch.Size([225, 6]) || layers.0.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.108 |  0.111 |  0.025 | torch.Size([540, 180]) || layers.0.residual_group.blocks.4.attn.qkv.weight
 | -0.000 | -0.040 |  0.043 |  0.017 | torch.Size([540]) || layers.0.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.106 |  0.094 |  0.024 | torch.Size([180, 180]) || layers.0.residual_group.blocks.4.attn.proj.weight
 | -0.001 | -0.041 |  0.040 |  0.026 | torch.Size([180]) || layers.0.residual_group.blocks.4.attn.proj.bias
 |  0.984 |  0.952 |  1.028 |  0.014 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.weight
 | -0.001 | -0.041 |  0.045 |  0.017 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.101 |  0.131 |  0.024 | torch.Size([360, 180]) || layers.0.residual_group.blocks.4.mlp.fc1.weight
 |  0.001 | -0.040 |  0.041 |  0.014 | torch.Size([360]) || layers.0.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.101 |  0.121 |  0.024 | torch.Size([180, 360]) || layers.0.residual_group.blocks.4.mlp.fc2.weight
 | -0.001 | -0.040 |  0.040 |  0.026 | torch.Size([180]) || layers.0.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.5.attn_mask
 |  0.994 |  0.956 |  1.029 |  0.015 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.weight
 | -0.001 | -0.045 |  0.044 |  0.018 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.092 |  0.087 |  0.025 | torch.Size([225, 6]) || layers.0.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.103 |  0.104 |  0.025 | torch.Size([540, 180]) || layers.0.residual_group.blocks.5.attn.qkv.weight
 |  0.000 | -0.040 |  0.042 |  0.017 | torch.Size([540]) || layers.0.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.108 |  0.100 |  0.024 | torch.Size([180, 180]) || layers.0.residual_group.blocks.5.attn.proj.weight
 | -0.001 | -0.041 |  0.042 |  0.027 | torch.Size([180]) || layers.0.residual_group.blocks.5.attn.proj.bias
 |  0.989 |  0.960 |  1.033 |  0.014 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.weight
 | -0.000 | -0.043 |  0.045 |  0.021 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.114 |  0.111 |  0.025 | torch.Size([360, 180]) || layers.0.residual_group.blocks.5.mlp.fc1.weight
 |  0.001 | -0.039 |  0.037 |  0.014 | torch.Size([360]) || layers.0.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.117 |  0.094 |  0.024 | torch.Size([180, 360]) || layers.0.residual_group.blocks.5.mlp.fc2.weight
 | -0.000 | -0.041 |  0.043 |  0.028 | torch.Size([180]) || layers.0.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.103 |  0.092 |  0.022 | torch.Size([180, 180, 3, 3]) || layers.0.conv.weight
 |  0.003 | -0.060 |  0.061 |  0.030 | torch.Size([180]) || layers.0.conv.bias
 |  0.997 |  0.968 |  1.023 |  0.013 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.weight
 | -0.000 | -0.038 |  0.037 |  0.022 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.064 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.103 |  0.111 |  0.023 | torch.Size([540, 180]) || layers.1.residual_group.blocks.0.attn.qkv.weight
 | -0.000 | -0.040 |  0.038 |  0.016 | torch.Size([540]) || layers.1.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.092 |  0.103 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 | -0.036 |  0.036 |  0.022 | torch.Size([180]) || layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.001 |  0.966 |  1.042 |  0.014 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.weight
 |  0.002 | -0.041 |  0.040 |  0.019 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.096 |  0.106 |  0.025 | torch.Size([360, 180]) || layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.004 | -0.034 |  0.042 |  0.018 | torch.Size([360]) || layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.096 |  0.106 |  0.024 | torch.Size([180, 360]) || layers.1.residual_group.blocks.0.mlp.fc2.weight
 | -0.000 | -0.043 |  0.043 |  0.026 | torch.Size([180]) || layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.1.attn_mask
 |  1.003 |  0.958 |  1.032 |  0.016 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 | -0.047 |  0.044 |  0.027 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.bias
 |  0.001 | -0.071 |  0.078 |  0.022 | torch.Size([225, 6]) || layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.110 |  0.108 |  0.025 | torch.Size([540, 180]) || layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 | -0.044 |  0.048 |  0.021 | torch.Size([540]) || layers.1.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.097 |  0.097 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 | -0.044 |  0.043 |  0.027 | torch.Size([180]) || layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.002 |  0.965 |  1.039 |  0.015 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.weight
 |  0.001 | -0.045 |  0.045 |  0.019 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.109 |  0.107 |  0.025 | torch.Size([360, 180]) || layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.004 | -0.033 |  0.042 |  0.018 | torch.Size([360]) || layers.1.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.095 |  0.099 |  0.024 | torch.Size([180, 360]) || layers.1.residual_group.blocks.1.mlp.fc2.weight
 | -0.000 | -0.044 |  0.043 |  0.027 | torch.Size([180]) || layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  0.996 |  0.958 |  1.026 |  0.013 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.weight
 |  0.001 | -0.038 |  0.043 |  0.022 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.bias
 |  0.001 | -0.071 |  0.063 |  0.021 | torch.Size([225, 6]) || layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.100 |  0.097 |  0.023 | torch.Size([540, 180]) || layers.1.residual_group.blocks.2.attn.qkv.weight
 | -0.001 | -0.045 |  0.050 |  0.016 | torch.Size([540]) || layers.1.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.095 |  0.087 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.2.attn.proj.weight
 | -0.000 | -0.044 |  0.044 |  0.028 | torch.Size([180]) || layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.006 |  0.961 |  1.039 |  0.016 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.weight
 |  0.001 | -0.045 |  0.048 |  0.024 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.108 |  0.104 |  0.025 | torch.Size([360, 180]) || layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.009 | -0.042 |  0.044 |  0.019 | torch.Size([360]) || layers.1.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.097 |  0.102 |  0.025 | torch.Size([180, 360]) || layers.1.residual_group.blocks.2.mlp.fc2.weight
 | -0.000 | -0.044 |  0.044 |  0.028 | torch.Size([180]) || layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.3.attn_mask
 |  1.001 |  0.968 |  1.030 |  0.012 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.weight
 |  0.001 | -0.039 |  0.038 |  0.020 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.065 |  0.077 |  0.021 | torch.Size([225, 6]) || layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.104 |  0.101 |  0.025 | torch.Size([540, 180]) || layers.1.residual_group.blocks.3.attn.qkv.weight
 | -0.001 | -0.041 |  0.040 |  0.017 | torch.Size([540]) || layers.1.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.119 |  0.095 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.3.attn.proj.weight
 | -0.001 | -0.039 |  0.038 |  0.024 | torch.Size([180]) || layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.008 |  0.966 |  1.055 |  0.015 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.weight
 |  0.001 | -0.040 |  0.043 |  0.021 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.108 |  0.104 |  0.025 | torch.Size([360, 180]) || layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.008 | -0.033 |  0.043 |  0.018 | torch.Size([360]) || layers.1.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.103 |  0.108 |  0.024 | torch.Size([180, 360]) || layers.1.residual_group.blocks.3.mlp.fc2.weight
 | -0.000 | -0.044 |  0.044 |  0.028 | torch.Size([180]) || layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  0.999 |  0.966 |  1.030 |  0.015 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.weight
 | -0.000 | -0.043 |  0.045 |  0.027 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.067 |  0.077 |  0.022 | torch.Size([225, 6]) || layers.1.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.099 |  0.097 |  0.024 | torch.Size([540, 180]) || layers.1.residual_group.blocks.4.attn.qkv.weight
 | -0.001 | -0.043 |  0.044 |  0.018 | torch.Size([540]) || layers.1.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.099 |  0.097 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.4.attn.proj.weight
 | -0.000 | -0.044 |  0.041 |  0.028 | torch.Size([180]) || layers.1.residual_group.blocks.4.attn.proj.bias
 |  1.008 |  0.968 |  1.052 |  0.015 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.weight
 | -0.000 | -0.040 |  0.042 |  0.022 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.099 |  0.101 |  0.025 | torch.Size([360, 180]) || layers.1.residual_group.blocks.4.mlp.fc1.weight
 |  0.009 | -0.035 |  0.043 |  0.018 | torch.Size([360]) || layers.1.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.110 |  0.103 |  0.024 | torch.Size([180, 360]) || layers.1.residual_group.blocks.4.mlp.fc2.weight
 | -0.000 | -0.046 |  0.044 |  0.030 | torch.Size([180]) || layers.1.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.5.attn_mask
 |  1.003 |  0.961 |  1.043 |  0.015 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.weight
 |  0.001 | -0.049 |  0.048 |  0.025 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.bias
 |  0.001 | -0.078 |  0.073 |  0.022 | torch.Size([225, 6]) || layers.1.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.103 |  0.099 |  0.024 | torch.Size([540, 180]) || layers.1.residual_group.blocks.5.attn.qkv.weight
 | -0.001 | -0.045 |  0.045 |  0.019 | torch.Size([540]) || layers.1.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.097 |  0.091 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.5.attn.proj.weight
 | -0.000 | -0.045 |  0.045 |  0.030 | torch.Size([180]) || layers.1.residual_group.blocks.5.attn.proj.bias
 |  1.011 |  0.967 |  1.049 |  0.016 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.weight
 |  0.002 | -0.042 |  0.044 |  0.022 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.107 |  0.108 |  0.026 | torch.Size([360, 180]) || layers.1.residual_group.blocks.5.mlp.fc1.weight
 |  0.010 | -0.040 |  0.044 |  0.018 | torch.Size([360]) || layers.1.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.099 |  0.097 |  0.024 | torch.Size([180, 360]) || layers.1.residual_group.blocks.5.mlp.fc2.weight
 | -0.000 | -0.045 |  0.047 |  0.030 | torch.Size([180]) || layers.1.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.110 |  0.132 |  0.022 | torch.Size([180, 180, 3, 3]) || layers.1.conv.weight
 | -0.000 | -0.055 |  0.058 |  0.028 | torch.Size([180]) || layers.1.conv.bias
 |  0.996 |  0.977 |  1.012 |  0.007 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.weight
 | -0.000 | -0.032 |  0.029 |  0.013 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.bias
 |  0.001 | -0.070 |  0.072 |  0.021 | torch.Size([225, 6]) || layers.2.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.094 |  0.097 |  0.021 | torch.Size([540, 180]) || layers.2.residual_group.blocks.0.attn.qkv.weight
 |  0.001 | -0.028 |  0.030 |  0.011 | torch.Size([540]) || layers.2.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.094 |  0.092 |  0.022 | torch.Size([180, 180]) || layers.2.residual_group.blocks.0.attn.proj.weight
 | -0.001 | -0.032 |  0.032 |  0.021 | torch.Size([180]) || layers.2.residual_group.blocks.0.attn.proj.bias
 |  1.011 |  0.972 |  1.042 |  0.015 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.weight
 | -0.000 | -0.042 |  0.043 |  0.019 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.122 |  0.109 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.0.mlp.fc1.weight
 |  0.009 | -0.034 |  0.050 |  0.019 | torch.Size([360]) || layers.2.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.087 |  0.106 |  0.022 | torch.Size([180, 360]) || layers.2.residual_group.blocks.0.mlp.fc2.weight
 | -0.001 | -0.040 |  0.040 |  0.025 | torch.Size([180]) || layers.2.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.1.attn_mask
 |  0.997 |  0.968 |  1.025 |  0.010 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.weight
 |  0.001 | -0.040 |  0.039 |  0.020 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.bias
 | -0.001 | -0.063 |  0.071 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.099 |  0.088 |  0.022 | torch.Size([540, 180]) || layers.2.residual_group.blocks.1.attn.qkv.weight
 |  0.000 | -0.040 |  0.042 |  0.016 | torch.Size([540]) || layers.2.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.085 |  0.093 |  0.022 | torch.Size([180, 180]) || layers.2.residual_group.blocks.1.attn.proj.weight
 | -0.001 | -0.038 |  0.039 |  0.025 | torch.Size([180]) || layers.2.residual_group.blocks.1.attn.proj.bias
 |  1.011 |  0.977 |  1.045 |  0.014 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.weight
 |  0.001 | -0.045 |  0.042 |  0.022 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.103 |  0.103 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.1.mlp.fc1.weight
 |  0.007 | -0.039 |  0.045 |  0.021 | torch.Size([360]) || layers.2.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.102 |  0.093 |  0.022 | torch.Size([180, 360]) || layers.2.residual_group.blocks.1.mlp.fc2.weight
 | -0.001 | -0.038 |  0.039 |  0.025 | torch.Size([180]) || layers.2.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.978 |  1.026 |  0.010 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.weight
 |  0.000 | -0.036 |  0.036 |  0.018 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.bias
 | -0.001 | -0.063 |  0.072 |  0.021 | torch.Size([225, 6]) || layers.2.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.096 |  0.091 |  0.022 | torch.Size([540, 180]) || layers.2.residual_group.blocks.2.attn.qkv.weight
 | -0.001 | -0.037 |  0.037 |  0.015 | torch.Size([540]) || layers.2.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.089 |  0.093 |  0.023 | torch.Size([180, 180]) || layers.2.residual_group.blocks.2.attn.proj.weight
 | -0.001 | -0.036 |  0.037 |  0.024 | torch.Size([180]) || layers.2.residual_group.blocks.2.attn.proj.bias
 |  1.010 |  0.975 |  1.041 |  0.013 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.weight
 | -0.000 | -0.041 |  0.041 |  0.018 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.098 |  0.125 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.2.mlp.fc1.weight
 |  0.009 | -0.040 |  0.042 |  0.021 | torch.Size([360]) || layers.2.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.089 |  0.106 |  0.023 | torch.Size([180, 360]) || layers.2.residual_group.blocks.2.mlp.fc2.weight
 | -0.001 | -0.038 |  0.038 |  0.026 | torch.Size([180]) || layers.2.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.3.attn_mask
 |  0.997 |  0.971 |  1.030 |  0.010 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.weight
 |  0.000 | -0.045 |  0.044 |  0.021 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.075 |  0.066 |  0.021 | torch.Size([225, 6]) || layers.2.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.098 |  0.100 |  0.022 | torch.Size([540, 180]) || layers.2.residual_group.blocks.3.attn.qkv.weight
 |  0.000 | -0.041 |  0.039 |  0.016 | torch.Size([540]) || layers.2.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.095 |  0.089 |  0.022 | torch.Size([180, 180]) || layers.2.residual_group.blocks.3.attn.proj.weight
 | -0.001 | -0.039 |  0.038 |  0.026 | torch.Size([180]) || layers.2.residual_group.blocks.3.attn.proj.bias
 |  1.010 |  0.971 |  1.038 |  0.014 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.weight
 | -0.000 | -0.047 |  0.044 |  0.019 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.104 |  0.097 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.3.mlp.fc1.weight
 |  0.007 | -0.037 |  0.042 |  0.022 | torch.Size([360]) || layers.2.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.104 |  0.096 |  0.023 | torch.Size([180, 360]) || layers.2.residual_group.blocks.3.mlp.fc2.weight
 | -0.001 | -0.038 |  0.037 |  0.026 | torch.Size([180]) || layers.2.residual_group.blocks.3.mlp.fc2.bias
 |  0.998 |  0.973 |  1.025 |  0.010 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.weight
 |  0.001 | -0.048 |  0.045 |  0.023 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.073 |  0.080 |  0.022 | torch.Size([225, 6]) || layers.2.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.098 |  0.098 |  0.022 | torch.Size([540, 180]) || layers.2.residual_group.blocks.4.attn.qkv.weight
 |  0.000 | -0.045 |  0.041 |  0.017 | torch.Size([540]) || layers.2.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.088 |  0.088 |  0.022 | torch.Size([180, 180]) || layers.2.residual_group.blocks.4.attn.proj.weight
 | -0.001 | -0.039 |  0.038 |  0.026 | torch.Size([180]) || layers.2.residual_group.blocks.4.attn.proj.bias
 |  1.009 |  0.978 |  1.040 |  0.012 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.weight
 | -0.001 | -0.038 |  0.038 |  0.019 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.107 |  0.102 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.4.mlp.fc1.weight
 |  0.008 | -0.040 |  0.041 |  0.021 | torch.Size([360]) || layers.2.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.105 |  0.098 |  0.023 | torch.Size([180, 360]) || layers.2.residual_group.blocks.4.mlp.fc2.weight
 | -0.001 | -0.039 |  0.038 |  0.026 | torch.Size([180]) || layers.2.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.5.attn_mask
 |  0.996 |  0.973 |  1.016 |  0.009 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.weight
 |  0.000 | -0.031 |  0.037 |  0.014 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.bias
 |  0.001 | -0.067 |  0.075 |  0.022 | torch.Size([225, 6]) || layers.2.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.095 |  0.088 |  0.022 | torch.Size([540, 180]) || layers.2.residual_group.blocks.5.attn.qkv.weight
 |  0.000 | -0.036 |  0.033 |  0.013 | torch.Size([540]) || layers.2.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.078 |  0.080 |  0.021 | torch.Size([180, 180]) || layers.2.residual_group.blocks.5.attn.proj.weight
 | -0.001 | -0.032 |  0.032 |  0.022 | torch.Size([180]) || layers.2.residual_group.blocks.5.attn.proj.bias
 |  1.010 |  0.978 |  1.039 |  0.014 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.weight
 | -0.000 | -0.045 |  0.041 |  0.021 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.100 |  0.113 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.5.mlp.fc1.weight
 |  0.009 | -0.040 |  0.046 |  0.023 | torch.Size([360]) || layers.2.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.102 |  0.090 |  0.023 | torch.Size([180, 360]) || layers.2.residual_group.blocks.5.mlp.fc2.weight
 | -0.001 | -0.038 |  0.038 |  0.027 | torch.Size([180]) || layers.2.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.200 |  0.207 |  0.024 | torch.Size([180, 180, 3, 3]) || layers.2.conv.weight
 | -0.000 | -0.048 |  0.049 |  0.020 | torch.Size([180]) || layers.2.conv.bias
 |  1.000 |  0.984 |  1.020 |  0.006 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.weight
 | -0.001 | -0.025 |  0.029 |  0.015 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.062 |  0.068 |  0.021 | torch.Size([225, 6]) || layers.3.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.089 |  0.086 |  0.021 | torch.Size([540, 180]) || layers.3.residual_group.blocks.0.attn.qkv.weight
 | -0.000 | -0.028 |  0.027 |  0.011 | torch.Size([540]) || layers.3.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.091 |  0.093 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.0.attn.proj.weight
 |  0.000 | -0.025 |  0.027 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.0.attn.proj.bias
 |  1.005 |  0.989 |  1.030 |  0.008 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.weight
 |  0.001 | -0.029 |  0.031 |  0.016 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.093 |  0.100 |  0.021 | torch.Size([360, 180]) || layers.3.residual_group.blocks.0.mlp.fc1.weight
 |  0.004 | -0.022 |  0.030 |  0.013 | torch.Size([360]) || layers.3.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.090 |  0.098 |  0.021 | torch.Size([180, 360]) || layers.3.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 | -0.027 |  0.027 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.1.attn_mask
 |  1.001 |  0.992 |  1.019 |  0.005 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.weight
 | -0.000 | -0.028 |  0.030 |  0.015 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.069 |  0.063 |  0.021 | torch.Size([225, 6]) || layers.3.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.101 |  0.095 |  0.021 | torch.Size([540, 180]) || layers.3.residual_group.blocks.1.attn.qkv.weight
 | -0.000 | -0.029 |  0.028 |  0.010 | torch.Size([540]) || layers.3.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.087 |  0.086 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.1.attn.proj.weight
 |  0.000 | -0.026 |  0.027 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.1.attn.proj.bias
 |  1.004 |  0.992 |  1.026 |  0.007 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.weight
 |  0.000 | -0.024 |  0.027 |  0.012 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.086 |  0.089 |  0.021 | torch.Size([360, 180]) || layers.3.residual_group.blocks.1.mlp.fc1.weight
 |  0.004 | -0.017 |  0.024 |  0.009 | torch.Size([360]) || layers.3.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.093 |  0.090 |  0.021 | torch.Size([180, 360]) || layers.3.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 | -0.021 |  0.022 |  0.013 | torch.Size([180]) || layers.3.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.990 |  1.017 |  0.005 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.weight
 |  0.000 | -0.030 |  0.029 |  0.015 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.060 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.090 |  0.100 |  0.021 | torch.Size([540, 180]) || layers.3.residual_group.blocks.2.attn.qkv.weight
 |  0.001 | -0.027 |  0.028 |  0.010 | torch.Size([540]) || layers.3.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.094 |  0.077 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.2.attn.proj.weight
 |  0.001 | -0.027 |  0.028 |  0.018 | torch.Size([180]) || layers.3.residual_group.blocks.2.attn.proj.bias
 |  1.005 |  0.987 |  1.025 |  0.008 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.weight
 | -0.000 | -0.031 |  0.034 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.100 |  0.087 |  0.021 | torch.Size([360, 180]) || layers.3.residual_group.blocks.2.mlp.fc1.weight
 |  0.004 | -0.022 |  0.029 |  0.013 | torch.Size([360]) || layers.3.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.096 |  0.095 |  0.021 | torch.Size([180, 360]) || layers.3.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 | -0.026 |  0.026 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.3.attn_mask
 |  1.001 |  0.986 |  1.018 |  0.005 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.weight
 | -0.000 | -0.028 |  0.024 |  0.014 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.072 |  0.071 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.099 |  0.092 |  0.022 | torch.Size([540, 180]) || layers.3.residual_group.blocks.3.attn.qkv.weight
 | -0.000 | -0.026 |  0.024 |  0.010 | torch.Size([540]) || layers.3.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.082 |  0.086 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.3.attn.proj.weight
 |  0.000 | -0.021 |  0.024 |  0.014 | torch.Size([180]) || layers.3.residual_group.blocks.3.attn.proj.bias
 |  1.003 |  0.990 |  1.016 |  0.006 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.weight
 |  0.001 | -0.023 |  0.021 |  0.012 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.084 |  0.089 |  0.021 | torch.Size([360, 180]) || layers.3.residual_group.blocks.3.mlp.fc1.weight
 |  0.002 | -0.020 |  0.020 |  0.010 | torch.Size([360]) || layers.3.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.102 |  0.080 |  0.021 | torch.Size([180, 360]) || layers.3.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 | -0.020 |  0.019 |  0.013 | torch.Size([180]) || layers.3.residual_group.blocks.3.mlp.fc2.bias
 |  1.001 |  0.985 |  1.018 |  0.007 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.weight
 |  0.000 | -0.030 |  0.026 |  0.016 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.062 |  0.071 |  0.021 | torch.Size([225, 6]) || layers.3.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.087 |  0.090 |  0.021 | torch.Size([540, 180]) || layers.3.residual_group.blocks.4.attn.qkv.weight
 | -0.000 | -0.028 |  0.027 |  0.011 | torch.Size([540]) || layers.3.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.097 |  0.081 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.4.attn.proj.weight
 |  0.001 | -0.026 |  0.026 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.4.attn.proj.bias
 |  1.005 |  0.986 |  1.022 |  0.007 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.weight
 |  0.001 | -0.026 |  0.025 |  0.014 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.089 |  0.096 |  0.021 | torch.Size([360, 180]) || layers.3.residual_group.blocks.4.mlp.fc1.weight
 |  0.003 | -0.020 |  0.027 |  0.012 | torch.Size([360]) || layers.3.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.101 |  0.099 |  0.021 | torch.Size([180, 360]) || layers.3.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 | -0.022 |  0.023 |  0.015 | torch.Size([180]) || layers.3.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.5.attn_mask
 |  1.000 |  0.987 |  1.017 |  0.006 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.weight
 |  0.000 | -0.026 |  0.028 |  0.014 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.070 |  0.074 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.084 |  0.094 |  0.021 | torch.Size([540, 180]) || layers.3.residual_group.blocks.5.attn.qkv.weight
 | -0.000 | -0.027 |  0.031 |  0.011 | torch.Size([540]) || layers.3.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.092 |  0.080 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.5.attn.proj.weight
 |  0.000 | -0.025 |  0.026 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.5.attn.proj.bias
 |  1.006 |  0.987 |  1.029 |  0.009 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.weight
 | -0.001 | -0.028 |  0.030 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.104 |  0.091 |  0.022 | torch.Size([360, 180]) || layers.3.residual_group.blocks.5.mlp.fc1.weight
 |  0.004 | -0.024 |  0.034 |  0.014 | torch.Size([360]) || layers.3.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.091 |  0.089 |  0.022 | torch.Size([180, 360]) || layers.3.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 | -0.027 |  0.027 |  0.018 | torch.Size([180]) || layers.3.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.432 |  0.456 |  0.031 | torch.Size([180, 180, 3, 3]) || layers.3.conv.weight
 | -0.000 | -0.028 |  0.031 |  0.015 | torch.Size([180]) || layers.3.conv.bias
 |  1.000 |  0.992 |  1.013 |  0.003 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.weight
 | -0.000 | -0.012 |  0.014 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.062 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.089 |  0.090 |  0.021 | torch.Size([540, 180]) || layers.4.residual_group.blocks.0.attn.qkv.weight
 |  0.000 | -0.013 |  0.013 |  0.004 | torch.Size([540]) || layers.4.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.078 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.0.attn.proj.weight
 | -0.000 | -0.013 |  0.012 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.0.attn.proj.bias
 |  1.001 |  0.992 |  1.014 |  0.004 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.weight
 | -0.001 | -0.017 |  0.012 |  0.007 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.093 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.0.mlp.fc1.weight
 |  0.001 | -0.009 |  0.015 |  0.005 | torch.Size([360]) || layers.4.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.095 |  0.090 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.0.mlp.fc2.weight
 | -0.000 | -0.013 |  0.012 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.1.attn_mask
 |  1.000 |  0.991 |  1.011 |  0.004 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.weight
 | -0.000 | -0.014 |  0.013 |  0.007 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.080 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.092 |  0.097 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.1.attn.qkv.weight
 | -0.000 | -0.017 |  0.016 |  0.005 | torch.Size([540]) || layers.4.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.091 |  0.084 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.1.attn.proj.weight
 |  0.000 | -0.011 |  0.011 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.1.attn.proj.bias
 |  1.002 |  0.990 |  1.021 |  0.004 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.weight
 | -0.000 | -0.014 |  0.015 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.095 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.1.mlp.fc1.weight
 |  0.001 | -0.009 |  0.016 |  0.005 | torch.Size([360]) || layers.4.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.088 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.1.mlp.fc2.weight
 | -0.000 | -0.012 |  0.011 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.992 |  1.008 |  0.003 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.weight
 | -0.000 | -0.013 |  0.011 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.075 |  0.068 |  0.021 | torch.Size([225, 6]) || layers.4.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.095 |  0.097 |  0.021 | torch.Size([540, 180]) || layers.4.residual_group.blocks.2.attn.qkv.weight
 |  0.000 | -0.012 |  0.015 |  0.004 | torch.Size([540]) || layers.4.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.079 |  0.090 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.2.attn.proj.weight
 | -0.000 | -0.013 |  0.011 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.2.attn.proj.bias
 |  1.003 |  0.989 |  1.019 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.weight
 |  0.000 | -0.016 |  0.017 |  0.008 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.087 |  0.088 |  0.021 | torch.Size([360, 180]) || layers.4.residual_group.blocks.2.mlp.fc1.weight
 |  0.001 | -0.010 |  0.021 |  0.006 | torch.Size([360]) || layers.4.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.089 |  0.099 |  0.021 | torch.Size([180, 360]) || layers.4.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 | -0.012 |  0.012 |  0.007 | torch.Size([180]) || layers.4.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.3.attn_mask
 |  1.001 |  0.992 |  1.011 |  0.003 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.weight
 | -0.000 | -0.013 |  0.014 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.088 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.102 |  0.091 |  0.021 | torch.Size([540, 180]) || layers.4.residual_group.blocks.3.attn.qkv.weight
 | -0.000 | -0.013 |  0.015 |  0.005 | torch.Size([540]) || layers.4.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.084 |  0.086 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.3.attn.proj.weight
 | -0.000 | -0.012 |  0.011 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.3.attn.proj.bias
 |  1.003 |  0.994 |  1.026 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.weight
 | -0.000 | -0.016 |  0.015 |  0.007 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.089 |  0.082 |  0.021 | torch.Size([360, 180]) || layers.4.residual_group.blocks.3.mlp.fc1.weight
 |  0.001 | -0.010 |  0.019 |  0.005 | torch.Size([360]) || layers.4.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.091 |  0.096 |  0.021 | torch.Size([180, 360]) || layers.4.residual_group.blocks.3.mlp.fc2.weight
 | -0.000 | -0.012 |  0.010 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.3.mlp.fc2.bias
 |  1.001 |  0.994 |  1.009 |  0.003 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.weight
 |  0.000 | -0.011 |  0.014 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.bias
 |  0.000 | -0.072 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.088 |  0.091 |  0.021 | torch.Size([540, 180]) || layers.4.residual_group.blocks.4.attn.qkv.weight
 | -0.001 | -0.012 |  0.009 |  0.004 | torch.Size([540]) || layers.4.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.083 |  0.087 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.4.attn.proj.weight
 | -0.000 | -0.013 |  0.012 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.4.attn.proj.bias
 |  1.003 |  0.993 |  1.018 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.weight
 | -0.000 | -0.018 |  0.016 |  0.008 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.086 |  0.078 |  0.021 | torch.Size([360, 180]) || layers.4.residual_group.blocks.4.mlp.fc1.weight
 |  0.002 | -0.009 |  0.016 |  0.005 | torch.Size([360]) || layers.4.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.088 |  0.087 |  0.021 | torch.Size([180, 360]) || layers.4.residual_group.blocks.4.mlp.fc2.weight
 | -0.000 | -0.012 |  0.011 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.5.attn_mask
 |  1.000 |  0.991 |  1.011 |  0.003 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.weight
 | -0.000 | -0.010 |  0.008 |  0.004 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.062 |  0.053 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.082 |  0.091 |  0.021 | torch.Size([540, 180]) || layers.4.residual_group.blocks.5.attn.qkv.weight
 | -0.000 | -0.013 |  0.014 |  0.005 | torch.Size([540]) || layers.4.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.085 |  0.097 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.5.attn.proj.weight
 | -0.000 | -0.010 |  0.011 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.5.attn.proj.bias
 |  1.002 |  0.990 |  1.021 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.weight
 | -0.001 | -0.016 |  0.013 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.092 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.5.mlp.fc1.weight
 |  0.001 | -0.016 |  0.016 |  0.005 | torch.Size([360]) || layers.4.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.088 |  0.086 |  0.021 | torch.Size([180, 360]) || layers.4.residual_group.blocks.5.mlp.fc2.weight
 | -0.000 | -0.014 |  0.012 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.749 |  0.758 |  0.043 | torch.Size([180, 180, 3, 3]) || layers.4.conv.weight
 |  0.001 | -0.028 |  0.031 |  0.016 | torch.Size([180]) || layers.4.conv.bias
 |  1.001 |  0.995 |  1.009 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.weight
 | -0.000 | -0.012 |  0.014 |  0.004 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.065 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.094 |  0.090 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.0.attn.qkv.weight
 |  0.000 | -0.014 |  0.013 |  0.004 | torch.Size([540]) || layers.5.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.083 |  0.089 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.0.attn.proj.weight
 |  0.000 | -0.007 |  0.008 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.0.attn.proj.bias
 |  1.001 |  0.994 |  1.011 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.weight
 |  0.000 | -0.006 |  0.004 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.098 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 | -0.007 |  0.012 |  0.002 | torch.Size([360]) || layers.5.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.084 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 | -0.005 |  0.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.1.attn_mask
 |  1.001 |  0.996 |  1.015 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.weight
 |  0.000 | -0.010 |  0.010 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.060 |  0.066 |  0.021 | torch.Size([225, 6]) || layers.5.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.088 |  0.083 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.1.attn.qkv.weight
 | -0.000 | -0.013 |  0.010 |  0.003 | torch.Size([540]) || layers.5.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.076 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.1.attn.proj.weight
 |  0.000 | -0.007 |  0.007 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.1.attn.proj.bias
 |  1.003 |  0.995 |  1.023 |  0.004 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.weight
 | -0.000 | -0.011 |  0.012 |  0.004 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.087 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.1.mlp.fc1.weight
 |  0.001 | -0.007 |  0.019 |  0.003 | torch.Size([360]) || layers.5.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.081 |  0.091 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 | -0.006 |  0.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.994 |  1.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.weight
 | -0.000 | -0.008 |  0.005 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.bias
 | -0.001 | -0.082 |  0.060 |  0.021 | torch.Size([225, 6]) || layers.5.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.097 |  0.089 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.2.attn.qkv.weight
 | -0.000 | -0.009 |  0.009 |  0.002 | torch.Size([540]) || layers.5.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.080 |  0.087 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.2.attn.proj.weight
 |  0.000 | -0.007 |  0.008 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.2.attn.proj.bias
 |  1.002 |  0.994 |  1.013 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.weight
 | -0.000 | -0.008 |  0.011 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.092 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.2.mlp.fc1.weight
 |  0.001 | -0.008 |  0.013 |  0.003 | torch.Size([360]) || layers.5.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.087 |  0.094 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 | -0.006 |  0.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.3.attn_mask
 |  1.000 |  0.995 |  1.009 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.weight
 | -0.000 | -0.005 |  0.008 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.bias
 |  0.001 | -0.059 |  0.074 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.095 |  0.092 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.3.attn.qkv.weight
 |  0.000 | -0.018 |  0.019 |  0.004 | torch.Size([540]) || layers.5.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.076 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.3.attn.proj.weight
 |  0.000 | -0.006 |  0.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.3.attn.proj.bias
 |  1.002 |  0.992 |  1.019 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.weight
 | -0.000 | -0.009 |  0.009 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.086 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.3.mlp.fc1.weight
 |  0.001 | -0.006 |  0.015 |  0.003 | torch.Size([360]) || layers.5.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.105 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  0.997 |  1.009 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.weight
 | -0.000 | -0.007 |  0.010 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.065 |  0.065 |  0.021 | torch.Size([225, 6]) || layers.5.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.085 |  0.093 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.4.attn.qkv.weight
 | -0.000 | -0.010 |  0.010 |  0.003 | torch.Size([540]) || layers.5.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.085 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.4.attn.proj.weight
 |  0.000 | -0.005 |  0.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.4.attn.proj.bias
 |  1.004 |  0.993 |  1.021 |  0.005 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.weight
 | -0.000 | -0.016 |  0.016 |  0.006 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.095 |  0.094 |  0.021 | torch.Size([360, 180]) || layers.5.residual_group.blocks.4.mlp.fc1.weight
 |  0.002 | -0.010 |  0.025 |  0.005 | torch.Size([360]) || layers.5.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.083 |  0.090 |  0.021 | torch.Size([180, 360]) || layers.5.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 | -0.008 |  0.009 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.5.attn_mask
 |  1.000 |  0.995 |  1.008 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.weight
 |  0.000 | -0.007 |  0.007 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.065 |  0.068 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.085 |  0.104 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.5.attn.qkv.weight
 |  0.000 | -0.014 |  0.014 |  0.004 | torch.Size([540]) || layers.5.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.087 |  0.091 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.5.attn.proj.weight
 |  0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.5.attn.proj.bias
 |  1.002 |  0.992 |  1.018 |  0.004 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.weight
 | -0.000 | -0.009 |  0.010 |  0.004 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.087 |  0.085 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.5.mlp.fc1.weight
 |  0.001 | -0.012 |  0.019 |  0.003 | torch.Size([360]) || layers.5.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.088 |  0.096 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 | -0.006 |  0.005 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -1.255 |  1.261 |  0.047 | torch.Size([180, 180, 3, 3]) || layers.5.conv.weight
 |  0.001 | -0.030 |  0.026 |  0.015 | torch.Size([180]) || layers.5.conv.bias
 |  0.426 |  0.276 |  0.702 |  0.052 | torch.Size([180]) || norm.weight
 |  0.027 | -0.484 |  0.640 |  0.060 | torch.Size([180]) || norm.bias
 |  0.000 | -0.103 |  0.083 |  0.017 | torch.Size([180, 180, 3, 3]) || conv_after_body.weight
 | -0.002 | -0.042 |  0.040 |  0.016 | torch.Size([180]) || conv_after_body.bias
 |  0.000 | -0.265 |  0.214 |  0.031 | torch.Size([64, 180, 3, 3]) || conv_before_upsample.0.weight
 | -0.034 | -0.108 |  0.025 |  0.022 | torch.Size([64]) || conv_before_upsample.0.bias
 | -0.000 | -0.325 |  0.231 |  0.033 | torch.Size([256, 64, 3, 3]) || upsample.0.weight
 | -0.002 | -0.120 |  0.111 |  0.035 | torch.Size([256]) || upsample.0.bias
 |  0.000 | -0.044 |  0.051 |  0.009 | torch.Size([1, 64, 3, 3]) || conv_last.weight
 |  0.112 |  0.112 |  0.112 |    nan | torch.Size([1]) || conv_last.bias

22-10-20 20:30:06.558 :   task: swinir_denoising_sr_x2_charbonnier
  model: plain
  gpu_ids: [0]
  dist: False
  scale: 2
  n_channels: 1
  path:[
    root: superresolution
    pretrained_netG: superresolution\swinir_denoising_sr_x2_charbonnier\models\50000_G.pth
    pretrained_netE: superresolution\swinir_denoising_sr_x2_charbonnier\models\50000_E.pth
    task: superresolution\swinir_denoising_sr_x2_charbonnier
    log: superresolution\swinir_denoising_sr_x2_charbonnier
    options: superresolution\swinir_denoising_sr_x2_charbonnier\options
    models: superresolution\swinir_denoising_sr_x2_charbonnier\models
    images: superresolution\swinir_denoising_sr_x2_charbonnier\images
    pretrained_optimizerG: superresolution\swinir_denoising_sr_x2_charbonnier\models\50000_optimizerG.pth
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr
      dataroot_H: trainsets/trainH
      dataroot_L: trainsets/X4_lq_x2_lr
      H_size: 96
      dataloader_shuffle: True
      dataloader_num_workers: 12
      dataloader_batch_size: 1
      phase: train
      scale: 2
      n_channels: 1
    ]
    test:[
      name: test_dataset
      dataset_type: sr
      dataroot_H: testsets/set12
      dataroot_L: testsets/set12_valid_lq_x2_lr
      phase: test
      scale: 2
      n_channels: 1
    ]
  ]
  netG:[
    net_type: swinir
    upscale: 2
    in_chans: 1
    img_size: 48
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    upsampler: pixelshuffle
    resi_connection: 1conv
    init_type: default
    scale: 2
  ]
  train:[
    G_lossfn_type: l2
    G_lossfn_weight: 1.0
    E_decay: 0.999
    G_optimizer_type: adam
    G_optimizer_lr: 0.0002
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_optimizer_reuse: True
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [250000, 400000, 450000, 475000, 500000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    G_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
  ]
  opt_path: options\swinir\train_swinir_sr_classical.json
  is_train: True
  merge_bn: False
  merge_bn_startpoint: -1
  find_unused_parameters: True
  num_gpu: 1
  rank: 0
  world_size: 1

22-10-20 20:30:06.573 : Number of train images: 800, iters: 800
22-10-20 20:30:08.706 : 
Networks name: SwinIR
Params number: 11748093
Net structure:
SwinIR(
  (conv_first): Conv2d(1, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (patch_embed): PatchEmbed(
    (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  )
  (patch_unembed): PatchUnEmbed()
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (1): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (2): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (3): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (4): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (5): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
  )
  (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  (conv_after_body): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_before_upsample): Sequential(
    (0): Conv2d(180, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
  )
  (upsample): Upsample(
    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): PixelShuffle(upscale_factor=2)
  )
  (conv_last): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

22-10-20 20:30:08.892 : 
 |  mean  |  min   |  max   |  std   || shape               
 |  0.003 | -0.505 |  0.488 |  0.221 | torch.Size([180, 1, 3, 3]) || conv_first.weight
 | -0.009 | -0.371 |  0.342 |  0.203 | torch.Size([180]) || conv_first.bias
 |  1.021 |  0.961 |  1.071 |  0.022 | torch.Size([180]) || patch_embed.norm.weight
 |  0.001 | -0.041 |  0.073 |  0.024 | torch.Size([180]) || patch_embed.norm.bias
 |  0.996 |  0.906 |  1.053 |  0.025 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.weight
 |  0.001 | -0.084 |  0.121 |  0.030 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.bias
 | -0.007 | -0.100 |  0.117 |  0.033 | torch.Size([225, 6]) || layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.152 |  0.195 |  0.032 | torch.Size([540, 180]) || layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 | -0.091 |  0.041 |  0.020 | torch.Size([540]) || layers.0.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.233 |  0.259 |  0.028 | torch.Size([180, 180]) || layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.001 | -0.041 |  0.046 |  0.026 | torch.Size([180]) || layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.017 |  0.931 |  1.093 |  0.031 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 | -0.048 |  0.044 |  0.030 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.261 |  0.261 |  0.033 | torch.Size([360, 180]) || layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.012 | -0.032 |  0.043 |  0.020 | torch.Size([360]) || layers.0.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.143 |  0.157 |  0.031 | torch.Size([180, 360]) || layers.0.residual_group.blocks.0.mlp.fc2.weight
 | -0.000 | -0.043 |  0.040 |  0.023 | torch.Size([180]) || layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.1.attn_mask
 |  1.006 |  0.945 |  1.035 |  0.018 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.weight
 | -0.001 | -0.046 |  0.039 |  0.020 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.bias
 | -0.003 | -0.074 |  0.074 |  0.024 | torch.Size([225, 6]) || layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.112 |  0.119 |  0.029 | torch.Size([540, 180]) || layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 | -0.044 |  0.043 |  0.022 | torch.Size([540]) || layers.0.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.096 |  0.115 |  0.026 | torch.Size([180, 180]) || layers.0.residual_group.blocks.1.attn.proj.weight
 | -0.000 | -0.042 |  0.040 |  0.022 | torch.Size([180]) || layers.0.residual_group.blocks.1.attn.proj.bias
 |  0.989 |  0.924 |  1.061 |  0.023 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.weight
 |  0.001 | -0.045 |  0.080 |  0.021 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.102 |  0.107 |  0.025 | torch.Size([360, 180]) || layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.004 | -0.024 |  0.044 |  0.016 | torch.Size([360]) || layers.0.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.121 |  0.107 |  0.025 | torch.Size([180, 360]) || layers.0.residual_group.blocks.1.mlp.fc2.weight
 | -0.000 | -0.042 |  0.041 |  0.023 | torch.Size([180]) || layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  0.999 |  0.959 |  1.035 |  0.017 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.weight
 | -0.001 | -0.034 |  0.040 |  0.018 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.bias
 | -0.002 | -0.065 |  0.073 |  0.022 | torch.Size([225, 6]) || layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.121 |  0.109 |  0.027 | torch.Size([540, 180]) || layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 | -0.050 |  0.048 |  0.019 | torch.Size([540]) || layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.112 |  0.104 |  0.025 | torch.Size([180, 180]) || layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 | -0.041 |  0.040 |  0.023 | torch.Size([180]) || layers.0.residual_group.blocks.2.attn.proj.bias
 |  0.981 |  0.954 |  1.021 |  0.013 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.weight
 |  0.002 | -0.046 |  0.056 |  0.019 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.098 |  0.105 |  0.024 | torch.Size([360, 180]) || layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.001 | -0.025 |  0.041 |  0.012 | torch.Size([360]) || layers.0.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.108 |  0.100 |  0.024 | torch.Size([180, 360]) || layers.0.residual_group.blocks.2.mlp.fc2.weight
 | -0.000 | -0.041 |  0.040 |  0.023 | torch.Size([180]) || layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.3.attn_mask
 |  0.999 |  0.959 |  1.038 |  0.016 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.weight
 | -0.001 | -0.039 |  0.039 |  0.020 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.bias
 | -0.002 | -0.081 |  0.081 |  0.023 | torch.Size([225, 6]) || layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.112 |  0.116 |  0.027 | torch.Size([540, 180]) || layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 | -0.040 |  0.043 |  0.020 | torch.Size([540]) || layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.091 |  0.099 |  0.025 | torch.Size([180, 180]) || layers.0.residual_group.blocks.3.attn.proj.weight
 | -0.000 | -0.044 |  0.040 |  0.025 | torch.Size([180]) || layers.0.residual_group.blocks.3.attn.proj.bias
 |  0.983 |  0.952 |  1.035 |  0.014 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.weight
 |  0.001 | -0.046 |  0.042 |  0.018 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.111 |  0.127 |  0.024 | torch.Size([360, 180]) || layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 | -0.028 |  0.032 |  0.011 | torch.Size([360]) || layers.0.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.100 |  0.103 |  0.024 | torch.Size([180, 360]) || layers.0.residual_group.blocks.3.mlp.fc2.weight
 | -0.000 | -0.042 |  0.040 |  0.025 | torch.Size([180]) || layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  0.990 |  0.961 |  1.026 |  0.013 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.weight
 |  0.000 | -0.033 |  0.034 |  0.016 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.065 |  0.093 |  0.022 | torch.Size([225, 6]) || layers.0.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.108 |  0.111 |  0.025 | torch.Size([540, 180]) || layers.0.residual_group.blocks.4.attn.qkv.weight
 | -0.000 | -0.040 |  0.043 |  0.017 | torch.Size([540]) || layers.0.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.106 |  0.094 |  0.024 | torch.Size([180, 180]) || layers.0.residual_group.blocks.4.attn.proj.weight
 | -0.001 | -0.041 |  0.040 |  0.026 | torch.Size([180]) || layers.0.residual_group.blocks.4.attn.proj.bias
 |  0.984 |  0.952 |  1.028 |  0.014 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.weight
 | -0.001 | -0.041 |  0.045 |  0.017 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.101 |  0.131 |  0.024 | torch.Size([360, 180]) || layers.0.residual_group.blocks.4.mlp.fc1.weight
 |  0.001 | -0.040 |  0.041 |  0.014 | torch.Size([360]) || layers.0.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.101 |  0.121 |  0.024 | torch.Size([180, 360]) || layers.0.residual_group.blocks.4.mlp.fc2.weight
 | -0.001 | -0.040 |  0.040 |  0.026 | torch.Size([180]) || layers.0.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.5.attn_mask
 |  0.994 |  0.956 |  1.029 |  0.015 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.weight
 | -0.001 | -0.045 |  0.044 |  0.018 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.092 |  0.087 |  0.025 | torch.Size([225, 6]) || layers.0.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.103 |  0.104 |  0.025 | torch.Size([540, 180]) || layers.0.residual_group.blocks.5.attn.qkv.weight
 |  0.000 | -0.040 |  0.042 |  0.017 | torch.Size([540]) || layers.0.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.108 |  0.100 |  0.024 | torch.Size([180, 180]) || layers.0.residual_group.blocks.5.attn.proj.weight
 | -0.001 | -0.041 |  0.042 |  0.027 | torch.Size([180]) || layers.0.residual_group.blocks.5.attn.proj.bias
 |  0.989 |  0.960 |  1.033 |  0.014 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.weight
 | -0.000 | -0.043 |  0.045 |  0.021 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.114 |  0.111 |  0.025 | torch.Size([360, 180]) || layers.0.residual_group.blocks.5.mlp.fc1.weight
 |  0.001 | -0.039 |  0.037 |  0.014 | torch.Size([360]) || layers.0.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.117 |  0.094 |  0.024 | torch.Size([180, 360]) || layers.0.residual_group.blocks.5.mlp.fc2.weight
 | -0.000 | -0.041 |  0.043 |  0.028 | torch.Size([180]) || layers.0.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.103 |  0.092 |  0.022 | torch.Size([180, 180, 3, 3]) || layers.0.conv.weight
 |  0.003 | -0.060 |  0.061 |  0.030 | torch.Size([180]) || layers.0.conv.bias
 |  0.997 |  0.968 |  1.023 |  0.013 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.weight
 | -0.000 | -0.038 |  0.037 |  0.022 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.064 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.103 |  0.111 |  0.023 | torch.Size([540, 180]) || layers.1.residual_group.blocks.0.attn.qkv.weight
 | -0.000 | -0.040 |  0.038 |  0.016 | torch.Size([540]) || layers.1.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.092 |  0.103 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 | -0.036 |  0.036 |  0.022 | torch.Size([180]) || layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.001 |  0.966 |  1.042 |  0.014 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.weight
 |  0.002 | -0.041 |  0.040 |  0.019 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.096 |  0.106 |  0.025 | torch.Size([360, 180]) || layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.004 | -0.034 |  0.042 |  0.018 | torch.Size([360]) || layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.096 |  0.106 |  0.024 | torch.Size([180, 360]) || layers.1.residual_group.blocks.0.mlp.fc2.weight
 | -0.000 | -0.043 |  0.043 |  0.026 | torch.Size([180]) || layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.1.attn_mask
 |  1.003 |  0.958 |  1.032 |  0.016 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 | -0.047 |  0.044 |  0.027 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.bias
 |  0.001 | -0.071 |  0.078 |  0.022 | torch.Size([225, 6]) || layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.110 |  0.108 |  0.025 | torch.Size([540, 180]) || layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 | -0.044 |  0.048 |  0.021 | torch.Size([540]) || layers.1.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.097 |  0.097 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 | -0.044 |  0.043 |  0.027 | torch.Size([180]) || layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.002 |  0.965 |  1.039 |  0.015 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.weight
 |  0.001 | -0.045 |  0.045 |  0.019 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.109 |  0.107 |  0.025 | torch.Size([360, 180]) || layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.004 | -0.033 |  0.042 |  0.018 | torch.Size([360]) || layers.1.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.095 |  0.099 |  0.024 | torch.Size([180, 360]) || layers.1.residual_group.blocks.1.mlp.fc2.weight
 | -0.000 | -0.044 |  0.043 |  0.027 | torch.Size([180]) || layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  0.996 |  0.958 |  1.026 |  0.013 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.weight
 |  0.001 | -0.038 |  0.043 |  0.022 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.bias
 |  0.001 | -0.071 |  0.063 |  0.021 | torch.Size([225, 6]) || layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.100 |  0.097 |  0.023 | torch.Size([540, 180]) || layers.1.residual_group.blocks.2.attn.qkv.weight
 | -0.001 | -0.045 |  0.050 |  0.016 | torch.Size([540]) || layers.1.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.095 |  0.087 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.2.attn.proj.weight
 | -0.000 | -0.044 |  0.044 |  0.028 | torch.Size([180]) || layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.006 |  0.961 |  1.039 |  0.016 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.weight
 |  0.001 | -0.045 |  0.048 |  0.024 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.108 |  0.104 |  0.025 | torch.Size([360, 180]) || layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.009 | -0.042 |  0.044 |  0.019 | torch.Size([360]) || layers.1.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.097 |  0.102 |  0.025 | torch.Size([180, 360]) || layers.1.residual_group.blocks.2.mlp.fc2.weight
 | -0.000 | -0.044 |  0.044 |  0.028 | torch.Size([180]) || layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.3.attn_mask
 |  1.001 |  0.968 |  1.030 |  0.012 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.weight
 |  0.001 | -0.039 |  0.038 |  0.020 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.065 |  0.077 |  0.021 | torch.Size([225, 6]) || layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.104 |  0.101 |  0.025 | torch.Size([540, 180]) || layers.1.residual_group.blocks.3.attn.qkv.weight
 | -0.001 | -0.041 |  0.040 |  0.017 | torch.Size([540]) || layers.1.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.119 |  0.095 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.3.attn.proj.weight
 | -0.001 | -0.039 |  0.038 |  0.024 | torch.Size([180]) || layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.008 |  0.966 |  1.055 |  0.015 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.weight
 |  0.001 | -0.040 |  0.043 |  0.021 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.108 |  0.104 |  0.025 | torch.Size([360, 180]) || layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.008 | -0.033 |  0.043 |  0.018 | torch.Size([360]) || layers.1.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.103 |  0.108 |  0.024 | torch.Size([180, 360]) || layers.1.residual_group.blocks.3.mlp.fc2.weight
 | -0.000 | -0.044 |  0.044 |  0.028 | torch.Size([180]) || layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  0.999 |  0.966 |  1.030 |  0.015 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.weight
 | -0.000 | -0.043 |  0.045 |  0.027 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.067 |  0.077 |  0.022 | torch.Size([225, 6]) || layers.1.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.099 |  0.097 |  0.024 | torch.Size([540, 180]) || layers.1.residual_group.blocks.4.attn.qkv.weight
 | -0.001 | -0.043 |  0.044 |  0.018 | torch.Size([540]) || layers.1.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.099 |  0.097 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.4.attn.proj.weight
 | -0.000 | -0.044 |  0.041 |  0.028 | torch.Size([180]) || layers.1.residual_group.blocks.4.attn.proj.bias
 |  1.008 |  0.968 |  1.052 |  0.015 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.weight
 | -0.000 | -0.040 |  0.042 |  0.022 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.099 |  0.101 |  0.025 | torch.Size([360, 180]) || layers.1.residual_group.blocks.4.mlp.fc1.weight
 |  0.009 | -0.035 |  0.043 |  0.018 | torch.Size([360]) || layers.1.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.110 |  0.103 |  0.024 | torch.Size([180, 360]) || layers.1.residual_group.blocks.4.mlp.fc2.weight
 | -0.000 | -0.046 |  0.044 |  0.030 | torch.Size([180]) || layers.1.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.5.attn_mask
 |  1.003 |  0.961 |  1.043 |  0.015 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.weight
 |  0.001 | -0.049 |  0.048 |  0.025 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.bias
 |  0.001 | -0.078 |  0.073 |  0.022 | torch.Size([225, 6]) || layers.1.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.103 |  0.099 |  0.024 | torch.Size([540, 180]) || layers.1.residual_group.blocks.5.attn.qkv.weight
 | -0.001 | -0.045 |  0.045 |  0.019 | torch.Size([540]) || layers.1.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.097 |  0.091 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.5.attn.proj.weight
 | -0.000 | -0.045 |  0.045 |  0.030 | torch.Size([180]) || layers.1.residual_group.blocks.5.attn.proj.bias
 |  1.011 |  0.967 |  1.049 |  0.016 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.weight
 |  0.002 | -0.042 |  0.044 |  0.022 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.107 |  0.108 |  0.026 | torch.Size([360, 180]) || layers.1.residual_group.blocks.5.mlp.fc1.weight
 |  0.010 | -0.040 |  0.044 |  0.018 | torch.Size([360]) || layers.1.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.099 |  0.097 |  0.024 | torch.Size([180, 360]) || layers.1.residual_group.blocks.5.mlp.fc2.weight
 | -0.000 | -0.045 |  0.047 |  0.030 | torch.Size([180]) || layers.1.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.110 |  0.132 |  0.022 | torch.Size([180, 180, 3, 3]) || layers.1.conv.weight
 | -0.000 | -0.055 |  0.058 |  0.028 | torch.Size([180]) || layers.1.conv.bias
 |  0.996 |  0.977 |  1.012 |  0.007 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.weight
 | -0.000 | -0.032 |  0.029 |  0.013 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.bias
 |  0.001 | -0.070 |  0.072 |  0.021 | torch.Size([225, 6]) || layers.2.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.094 |  0.097 |  0.021 | torch.Size([540, 180]) || layers.2.residual_group.blocks.0.attn.qkv.weight
 |  0.001 | -0.028 |  0.030 |  0.011 | torch.Size([540]) || layers.2.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.094 |  0.092 |  0.022 | torch.Size([180, 180]) || layers.2.residual_group.blocks.0.attn.proj.weight
 | -0.001 | -0.032 |  0.032 |  0.021 | torch.Size([180]) || layers.2.residual_group.blocks.0.attn.proj.bias
 |  1.011 |  0.972 |  1.042 |  0.015 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.weight
 | -0.000 | -0.042 |  0.043 |  0.019 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.122 |  0.109 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.0.mlp.fc1.weight
 |  0.009 | -0.034 |  0.050 |  0.019 | torch.Size([360]) || layers.2.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.087 |  0.106 |  0.022 | torch.Size([180, 360]) || layers.2.residual_group.blocks.0.mlp.fc2.weight
 | -0.001 | -0.040 |  0.040 |  0.025 | torch.Size([180]) || layers.2.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.1.attn_mask
 |  0.997 |  0.968 |  1.025 |  0.010 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.weight
 |  0.001 | -0.040 |  0.039 |  0.020 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.bias
 | -0.001 | -0.063 |  0.071 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.099 |  0.088 |  0.022 | torch.Size([540, 180]) || layers.2.residual_group.blocks.1.attn.qkv.weight
 |  0.000 | -0.040 |  0.042 |  0.016 | torch.Size([540]) || layers.2.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.085 |  0.093 |  0.022 | torch.Size([180, 180]) || layers.2.residual_group.blocks.1.attn.proj.weight
 | -0.001 | -0.038 |  0.039 |  0.025 | torch.Size([180]) || layers.2.residual_group.blocks.1.attn.proj.bias
 |  1.011 |  0.977 |  1.045 |  0.014 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.weight
 |  0.001 | -0.045 |  0.042 |  0.022 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.103 |  0.103 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.1.mlp.fc1.weight
 |  0.007 | -0.039 |  0.045 |  0.021 | torch.Size([360]) || layers.2.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.102 |  0.093 |  0.022 | torch.Size([180, 360]) || layers.2.residual_group.blocks.1.mlp.fc2.weight
 | -0.001 | -0.038 |  0.039 |  0.025 | torch.Size([180]) || layers.2.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.978 |  1.026 |  0.010 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.weight
 |  0.000 | -0.036 |  0.036 |  0.018 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.bias
 | -0.001 | -0.063 |  0.072 |  0.021 | torch.Size([225, 6]) || layers.2.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.096 |  0.091 |  0.022 | torch.Size([540, 180]) || layers.2.residual_group.blocks.2.attn.qkv.weight
 | -0.001 | -0.037 |  0.037 |  0.015 | torch.Size([540]) || layers.2.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.089 |  0.093 |  0.023 | torch.Size([180, 180]) || layers.2.residual_group.blocks.2.attn.proj.weight
 | -0.001 | -0.036 |  0.037 |  0.024 | torch.Size([180]) || layers.2.residual_group.blocks.2.attn.proj.bias
 |  1.010 |  0.975 |  1.041 |  0.013 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.weight
 | -0.000 | -0.041 |  0.041 |  0.018 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.098 |  0.125 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.2.mlp.fc1.weight
 |  0.009 | -0.040 |  0.042 |  0.021 | torch.Size([360]) || layers.2.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.089 |  0.106 |  0.023 | torch.Size([180, 360]) || layers.2.residual_group.blocks.2.mlp.fc2.weight
 | -0.001 | -0.038 |  0.038 |  0.026 | torch.Size([180]) || layers.2.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.3.attn_mask
 |  0.997 |  0.971 |  1.030 |  0.010 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.weight
 |  0.000 | -0.045 |  0.044 |  0.021 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.075 |  0.066 |  0.021 | torch.Size([225, 6]) || layers.2.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.098 |  0.100 |  0.022 | torch.Size([540, 180]) || layers.2.residual_group.blocks.3.attn.qkv.weight
 |  0.000 | -0.041 |  0.039 |  0.016 | torch.Size([540]) || layers.2.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.095 |  0.089 |  0.022 | torch.Size([180, 180]) || layers.2.residual_group.blocks.3.attn.proj.weight
 | -0.001 | -0.039 |  0.038 |  0.026 | torch.Size([180]) || layers.2.residual_group.blocks.3.attn.proj.bias
 |  1.010 |  0.971 |  1.038 |  0.014 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.weight
 | -0.000 | -0.047 |  0.044 |  0.019 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.104 |  0.097 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.3.mlp.fc1.weight
 |  0.007 | -0.037 |  0.042 |  0.022 | torch.Size([360]) || layers.2.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.104 |  0.096 |  0.023 | torch.Size([180, 360]) || layers.2.residual_group.blocks.3.mlp.fc2.weight
 | -0.001 | -0.038 |  0.037 |  0.026 | torch.Size([180]) || layers.2.residual_group.blocks.3.mlp.fc2.bias
 |  0.998 |  0.973 |  1.025 |  0.010 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.weight
 |  0.001 | -0.048 |  0.045 |  0.023 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.073 |  0.080 |  0.022 | torch.Size([225, 6]) || layers.2.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.098 |  0.098 |  0.022 | torch.Size([540, 180]) || layers.2.residual_group.blocks.4.attn.qkv.weight
 |  0.000 | -0.045 |  0.041 |  0.017 | torch.Size([540]) || layers.2.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.088 |  0.088 |  0.022 | torch.Size([180, 180]) || layers.2.residual_group.blocks.4.attn.proj.weight
 | -0.001 | -0.039 |  0.038 |  0.026 | torch.Size([180]) || layers.2.residual_group.blocks.4.attn.proj.bias
 |  1.009 |  0.978 |  1.040 |  0.012 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.weight
 | -0.001 | -0.038 |  0.038 |  0.019 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.107 |  0.102 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.4.mlp.fc1.weight
 |  0.008 | -0.040 |  0.041 |  0.021 | torch.Size([360]) || layers.2.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.105 |  0.098 |  0.023 | torch.Size([180, 360]) || layers.2.residual_group.blocks.4.mlp.fc2.weight
 | -0.001 | -0.039 |  0.038 |  0.026 | torch.Size([180]) || layers.2.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.5.attn_mask
 |  0.996 |  0.973 |  1.016 |  0.009 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.weight
 |  0.000 | -0.031 |  0.037 |  0.014 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.bias
 |  0.001 | -0.067 |  0.075 |  0.022 | torch.Size([225, 6]) || layers.2.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.095 |  0.088 |  0.022 | torch.Size([540, 180]) || layers.2.residual_group.blocks.5.attn.qkv.weight
 |  0.000 | -0.036 |  0.033 |  0.013 | torch.Size([540]) || layers.2.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.078 |  0.080 |  0.021 | torch.Size([180, 180]) || layers.2.residual_group.blocks.5.attn.proj.weight
 | -0.001 | -0.032 |  0.032 |  0.022 | torch.Size([180]) || layers.2.residual_group.blocks.5.attn.proj.bias
 |  1.010 |  0.978 |  1.039 |  0.014 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.weight
 | -0.000 | -0.045 |  0.041 |  0.021 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.100 |  0.113 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.5.mlp.fc1.weight
 |  0.009 | -0.040 |  0.046 |  0.023 | torch.Size([360]) || layers.2.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.102 |  0.090 |  0.023 | torch.Size([180, 360]) || layers.2.residual_group.blocks.5.mlp.fc2.weight
 | -0.001 | -0.038 |  0.038 |  0.027 | torch.Size([180]) || layers.2.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.200 |  0.207 |  0.024 | torch.Size([180, 180, 3, 3]) || layers.2.conv.weight
 | -0.000 | -0.048 |  0.049 |  0.020 | torch.Size([180]) || layers.2.conv.bias
 |  1.000 |  0.984 |  1.020 |  0.006 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.weight
 | -0.001 | -0.025 |  0.029 |  0.015 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.062 |  0.068 |  0.021 | torch.Size([225, 6]) || layers.3.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.089 |  0.086 |  0.021 | torch.Size([540, 180]) || layers.3.residual_group.blocks.0.attn.qkv.weight
 | -0.000 | -0.028 |  0.027 |  0.011 | torch.Size([540]) || layers.3.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.091 |  0.093 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.0.attn.proj.weight
 |  0.000 | -0.025 |  0.027 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.0.attn.proj.bias
 |  1.005 |  0.989 |  1.030 |  0.008 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.weight
 |  0.001 | -0.029 |  0.031 |  0.016 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.093 |  0.100 |  0.021 | torch.Size([360, 180]) || layers.3.residual_group.blocks.0.mlp.fc1.weight
 |  0.004 | -0.022 |  0.030 |  0.013 | torch.Size([360]) || layers.3.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.090 |  0.098 |  0.021 | torch.Size([180, 360]) || layers.3.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 | -0.027 |  0.027 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.1.attn_mask
 |  1.001 |  0.992 |  1.019 |  0.005 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.weight
 | -0.000 | -0.028 |  0.030 |  0.015 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.069 |  0.063 |  0.021 | torch.Size([225, 6]) || layers.3.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.101 |  0.095 |  0.021 | torch.Size([540, 180]) || layers.3.residual_group.blocks.1.attn.qkv.weight
 | -0.000 | -0.029 |  0.028 |  0.010 | torch.Size([540]) || layers.3.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.087 |  0.086 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.1.attn.proj.weight
 |  0.000 | -0.026 |  0.027 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.1.attn.proj.bias
 |  1.004 |  0.992 |  1.026 |  0.007 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.weight
 |  0.000 | -0.024 |  0.027 |  0.012 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.086 |  0.089 |  0.021 | torch.Size([360, 180]) || layers.3.residual_group.blocks.1.mlp.fc1.weight
 |  0.004 | -0.017 |  0.024 |  0.009 | torch.Size([360]) || layers.3.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.093 |  0.090 |  0.021 | torch.Size([180, 360]) || layers.3.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 | -0.021 |  0.022 |  0.013 | torch.Size([180]) || layers.3.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.990 |  1.017 |  0.005 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.weight
 |  0.000 | -0.030 |  0.029 |  0.015 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.060 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.090 |  0.100 |  0.021 | torch.Size([540, 180]) || layers.3.residual_group.blocks.2.attn.qkv.weight
 |  0.001 | -0.027 |  0.028 |  0.010 | torch.Size([540]) || layers.3.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.094 |  0.077 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.2.attn.proj.weight
 |  0.001 | -0.027 |  0.028 |  0.018 | torch.Size([180]) || layers.3.residual_group.blocks.2.attn.proj.bias
 |  1.005 |  0.987 |  1.025 |  0.008 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.weight
 | -0.000 | -0.031 |  0.034 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.100 |  0.087 |  0.021 | torch.Size([360, 180]) || layers.3.residual_group.blocks.2.mlp.fc1.weight
 |  0.004 | -0.022 |  0.029 |  0.013 | torch.Size([360]) || layers.3.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.096 |  0.095 |  0.021 | torch.Size([180, 360]) || layers.3.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 | -0.026 |  0.026 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.3.attn_mask
 |  1.001 |  0.986 |  1.018 |  0.005 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.weight
 | -0.000 | -0.028 |  0.024 |  0.014 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.072 |  0.071 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.099 |  0.092 |  0.022 | torch.Size([540, 180]) || layers.3.residual_group.blocks.3.attn.qkv.weight
 | -0.000 | -0.026 |  0.024 |  0.010 | torch.Size([540]) || layers.3.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.082 |  0.086 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.3.attn.proj.weight
 |  0.000 | -0.021 |  0.024 |  0.014 | torch.Size([180]) || layers.3.residual_group.blocks.3.attn.proj.bias
 |  1.003 |  0.990 |  1.016 |  0.006 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.weight
 |  0.001 | -0.023 |  0.021 |  0.012 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.084 |  0.089 |  0.021 | torch.Size([360, 180]) || layers.3.residual_group.blocks.3.mlp.fc1.weight
 |  0.002 | -0.020 |  0.020 |  0.010 | torch.Size([360]) || layers.3.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.102 |  0.080 |  0.021 | torch.Size([180, 360]) || layers.3.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 | -0.020 |  0.019 |  0.013 | torch.Size([180]) || layers.3.residual_group.blocks.3.mlp.fc2.bias
 |  1.001 |  0.985 |  1.018 |  0.007 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.weight
 |  0.000 | -0.030 |  0.026 |  0.016 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.062 |  0.071 |  0.021 | torch.Size([225, 6]) || layers.3.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.087 |  0.090 |  0.021 | torch.Size([540, 180]) || layers.3.residual_group.blocks.4.attn.qkv.weight
 | -0.000 | -0.028 |  0.027 |  0.011 | torch.Size([540]) || layers.3.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.097 |  0.081 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.4.attn.proj.weight
 |  0.001 | -0.026 |  0.026 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.4.attn.proj.bias
 |  1.005 |  0.986 |  1.022 |  0.007 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.weight
 |  0.001 | -0.026 |  0.025 |  0.014 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.089 |  0.096 |  0.021 | torch.Size([360, 180]) || layers.3.residual_group.blocks.4.mlp.fc1.weight
 |  0.003 | -0.020 |  0.027 |  0.012 | torch.Size([360]) || layers.3.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.101 |  0.099 |  0.021 | torch.Size([180, 360]) || layers.3.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 | -0.022 |  0.023 |  0.015 | torch.Size([180]) || layers.3.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.5.attn_mask
 |  1.000 |  0.987 |  1.017 |  0.006 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.weight
 |  0.000 | -0.026 |  0.028 |  0.014 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.070 |  0.074 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.084 |  0.094 |  0.021 | torch.Size([540, 180]) || layers.3.residual_group.blocks.5.attn.qkv.weight
 | -0.000 | -0.027 |  0.031 |  0.011 | torch.Size([540]) || layers.3.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.092 |  0.080 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.5.attn.proj.weight
 |  0.000 | -0.025 |  0.026 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.5.attn.proj.bias
 |  1.006 |  0.987 |  1.029 |  0.009 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.weight
 | -0.001 | -0.028 |  0.030 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.104 |  0.091 |  0.022 | torch.Size([360, 180]) || layers.3.residual_group.blocks.5.mlp.fc1.weight
 |  0.004 | -0.024 |  0.034 |  0.014 | torch.Size([360]) || layers.3.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.091 |  0.089 |  0.022 | torch.Size([180, 360]) || layers.3.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 | -0.027 |  0.027 |  0.018 | torch.Size([180]) || layers.3.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.432 |  0.456 |  0.031 | torch.Size([180, 180, 3, 3]) || layers.3.conv.weight
 | -0.000 | -0.028 |  0.031 |  0.015 | torch.Size([180]) || layers.3.conv.bias
 |  1.000 |  0.992 |  1.013 |  0.003 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.weight
 | -0.000 | -0.012 |  0.014 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.062 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.089 |  0.090 |  0.021 | torch.Size([540, 180]) || layers.4.residual_group.blocks.0.attn.qkv.weight
 |  0.000 | -0.013 |  0.013 |  0.004 | torch.Size([540]) || layers.4.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.078 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.0.attn.proj.weight
 | -0.000 | -0.013 |  0.012 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.0.attn.proj.bias
 |  1.001 |  0.992 |  1.014 |  0.004 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.weight
 | -0.001 | -0.017 |  0.012 |  0.007 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.093 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.0.mlp.fc1.weight
 |  0.001 | -0.009 |  0.015 |  0.005 | torch.Size([360]) || layers.4.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.095 |  0.090 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.0.mlp.fc2.weight
 | -0.000 | -0.013 |  0.012 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.1.attn_mask
 |  1.000 |  0.991 |  1.011 |  0.004 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.weight
 | -0.000 | -0.014 |  0.013 |  0.007 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.080 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.092 |  0.097 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.1.attn.qkv.weight
 | -0.000 | -0.017 |  0.016 |  0.005 | torch.Size([540]) || layers.4.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.091 |  0.084 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.1.attn.proj.weight
 |  0.000 | -0.011 |  0.011 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.1.attn.proj.bias
 |  1.002 |  0.990 |  1.021 |  0.004 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.weight
 | -0.000 | -0.014 |  0.015 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.095 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.1.mlp.fc1.weight
 |  0.001 | -0.009 |  0.016 |  0.005 | torch.Size([360]) || layers.4.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.088 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.1.mlp.fc2.weight
 | -0.000 | -0.012 |  0.011 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.992 |  1.008 |  0.003 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.weight
 | -0.000 | -0.013 |  0.011 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.075 |  0.068 |  0.021 | torch.Size([225, 6]) || layers.4.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.095 |  0.097 |  0.021 | torch.Size([540, 180]) || layers.4.residual_group.blocks.2.attn.qkv.weight
 |  0.000 | -0.012 |  0.015 |  0.004 | torch.Size([540]) || layers.4.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.079 |  0.090 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.2.attn.proj.weight
 | -0.000 | -0.013 |  0.011 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.2.attn.proj.bias
 |  1.003 |  0.989 |  1.019 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.weight
 |  0.000 | -0.016 |  0.017 |  0.008 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.087 |  0.088 |  0.021 | torch.Size([360, 180]) || layers.4.residual_group.blocks.2.mlp.fc1.weight
 |  0.001 | -0.010 |  0.021 |  0.006 | torch.Size([360]) || layers.4.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.089 |  0.099 |  0.021 | torch.Size([180, 360]) || layers.4.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 | -0.012 |  0.012 |  0.007 | torch.Size([180]) || layers.4.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.3.attn_mask
 |  1.001 |  0.992 |  1.011 |  0.003 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.weight
 | -0.000 | -0.013 |  0.014 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.088 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.102 |  0.091 |  0.021 | torch.Size([540, 180]) || layers.4.residual_group.blocks.3.attn.qkv.weight
 | -0.000 | -0.013 |  0.015 |  0.005 | torch.Size([540]) || layers.4.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.084 |  0.086 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.3.attn.proj.weight
 | -0.000 | -0.012 |  0.011 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.3.attn.proj.bias
 |  1.003 |  0.994 |  1.026 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.weight
 | -0.000 | -0.016 |  0.015 |  0.007 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.089 |  0.082 |  0.021 | torch.Size([360, 180]) || layers.4.residual_group.blocks.3.mlp.fc1.weight
 |  0.001 | -0.010 |  0.019 |  0.005 | torch.Size([360]) || layers.4.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.091 |  0.096 |  0.021 | torch.Size([180, 360]) || layers.4.residual_group.blocks.3.mlp.fc2.weight
 | -0.000 | -0.012 |  0.010 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.3.mlp.fc2.bias
 |  1.001 |  0.994 |  1.009 |  0.003 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.weight
 |  0.000 | -0.011 |  0.014 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.bias
 |  0.000 | -0.072 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.088 |  0.091 |  0.021 | torch.Size([540, 180]) || layers.4.residual_group.blocks.4.attn.qkv.weight
 | -0.001 | -0.012 |  0.009 |  0.004 | torch.Size([540]) || layers.4.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.083 |  0.087 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.4.attn.proj.weight
 | -0.000 | -0.013 |  0.012 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.4.attn.proj.bias
 |  1.003 |  0.993 |  1.018 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.weight
 | -0.000 | -0.018 |  0.016 |  0.008 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.086 |  0.078 |  0.021 | torch.Size([360, 180]) || layers.4.residual_group.blocks.4.mlp.fc1.weight
 |  0.002 | -0.009 |  0.016 |  0.005 | torch.Size([360]) || layers.4.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.088 |  0.087 |  0.021 | torch.Size([180, 360]) || layers.4.residual_group.blocks.4.mlp.fc2.weight
 | -0.000 | -0.012 |  0.011 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.5.attn_mask
 |  1.000 |  0.991 |  1.011 |  0.003 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.weight
 | -0.000 | -0.010 |  0.008 |  0.004 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.062 |  0.053 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.082 |  0.091 |  0.021 | torch.Size([540, 180]) || layers.4.residual_group.blocks.5.attn.qkv.weight
 | -0.000 | -0.013 |  0.014 |  0.005 | torch.Size([540]) || layers.4.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.085 |  0.097 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.5.attn.proj.weight
 | -0.000 | -0.010 |  0.011 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.5.attn.proj.bias
 |  1.002 |  0.990 |  1.021 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.weight
 | -0.001 | -0.016 |  0.013 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.092 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.5.mlp.fc1.weight
 |  0.001 | -0.016 |  0.016 |  0.005 | torch.Size([360]) || layers.4.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.088 |  0.086 |  0.021 | torch.Size([180, 360]) || layers.4.residual_group.blocks.5.mlp.fc2.weight
 | -0.000 | -0.014 |  0.012 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.749 |  0.758 |  0.043 | torch.Size([180, 180, 3, 3]) || layers.4.conv.weight
 |  0.001 | -0.028 |  0.031 |  0.016 | torch.Size([180]) || layers.4.conv.bias
 |  1.001 |  0.995 |  1.009 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.weight
 | -0.000 | -0.012 |  0.014 |  0.004 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.065 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.094 |  0.090 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.0.attn.qkv.weight
 |  0.000 | -0.014 |  0.013 |  0.004 | torch.Size([540]) || layers.5.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.083 |  0.089 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.0.attn.proj.weight
 |  0.000 | -0.007 |  0.008 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.0.attn.proj.bias
 |  1.001 |  0.994 |  1.011 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.weight
 |  0.000 | -0.006 |  0.004 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.098 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 | -0.007 |  0.012 |  0.002 | torch.Size([360]) || layers.5.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.084 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 | -0.005 |  0.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.1.attn_mask
 |  1.001 |  0.996 |  1.015 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.weight
 |  0.000 | -0.010 |  0.010 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.060 |  0.066 |  0.021 | torch.Size([225, 6]) || layers.5.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.088 |  0.083 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.1.attn.qkv.weight
 | -0.000 | -0.013 |  0.010 |  0.003 | torch.Size([540]) || layers.5.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.076 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.1.attn.proj.weight
 |  0.000 | -0.007 |  0.007 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.1.attn.proj.bias
 |  1.003 |  0.995 |  1.023 |  0.004 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.weight
 | -0.000 | -0.011 |  0.012 |  0.004 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.087 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.1.mlp.fc1.weight
 |  0.001 | -0.007 |  0.019 |  0.003 | torch.Size([360]) || layers.5.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.081 |  0.091 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 | -0.006 |  0.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.994 |  1.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.weight
 | -0.000 | -0.008 |  0.005 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.bias
 | -0.001 | -0.082 |  0.060 |  0.021 | torch.Size([225, 6]) || layers.5.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.097 |  0.089 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.2.attn.qkv.weight
 | -0.000 | -0.009 |  0.009 |  0.002 | torch.Size([540]) || layers.5.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.080 |  0.087 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.2.attn.proj.weight
 |  0.000 | -0.007 |  0.008 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.2.attn.proj.bias
 |  1.002 |  0.994 |  1.013 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.weight
 | -0.000 | -0.008 |  0.011 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.092 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.2.mlp.fc1.weight
 |  0.001 | -0.008 |  0.013 |  0.003 | torch.Size([360]) || layers.5.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.087 |  0.094 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 | -0.006 |  0.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.3.attn_mask
 |  1.000 |  0.995 |  1.009 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.weight
 | -0.000 | -0.005 |  0.008 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.bias
 |  0.001 | -0.059 |  0.074 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.095 |  0.092 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.3.attn.qkv.weight
 |  0.000 | -0.018 |  0.019 |  0.004 | torch.Size([540]) || layers.5.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.076 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.3.attn.proj.weight
 |  0.000 | -0.006 |  0.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.3.attn.proj.bias
 |  1.002 |  0.992 |  1.019 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.weight
 | -0.000 | -0.009 |  0.009 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.086 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.3.mlp.fc1.weight
 |  0.001 | -0.006 |  0.015 |  0.003 | torch.Size([360]) || layers.5.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.105 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  0.997 |  1.009 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.weight
 | -0.000 | -0.007 |  0.010 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.065 |  0.065 |  0.021 | torch.Size([225, 6]) || layers.5.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.085 |  0.093 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.4.attn.qkv.weight
 | -0.000 | -0.010 |  0.010 |  0.003 | torch.Size([540]) || layers.5.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.085 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.4.attn.proj.weight
 |  0.000 | -0.005 |  0.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.4.attn.proj.bias
 |  1.004 |  0.993 |  1.021 |  0.005 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.weight
 | -0.000 | -0.016 |  0.016 |  0.006 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.095 |  0.094 |  0.021 | torch.Size([360, 180]) || layers.5.residual_group.blocks.4.mlp.fc1.weight
 |  0.002 | -0.010 |  0.025 |  0.005 | torch.Size([360]) || layers.5.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.083 |  0.090 |  0.021 | torch.Size([180, 360]) || layers.5.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 | -0.008 |  0.009 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.5.attn_mask
 |  1.000 |  0.995 |  1.008 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.weight
 |  0.000 | -0.007 |  0.007 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.065 |  0.068 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.085 |  0.104 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.5.attn.qkv.weight
 |  0.000 | -0.014 |  0.014 |  0.004 | torch.Size([540]) || layers.5.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.087 |  0.091 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.5.attn.proj.weight
 |  0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.5.attn.proj.bias
 |  1.002 |  0.992 |  1.018 |  0.004 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.weight
 | -0.000 | -0.009 |  0.010 |  0.004 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.087 |  0.085 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.5.mlp.fc1.weight
 |  0.001 | -0.012 |  0.019 |  0.003 | torch.Size([360]) || layers.5.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.088 |  0.096 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 | -0.006 |  0.005 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -1.255 |  1.261 |  0.047 | torch.Size([180, 180, 3, 3]) || layers.5.conv.weight
 |  0.001 | -0.030 |  0.026 |  0.015 | torch.Size([180]) || layers.5.conv.bias
 |  0.426 |  0.276 |  0.702 |  0.052 | torch.Size([180]) || norm.weight
 |  0.027 | -0.484 |  0.640 |  0.060 | torch.Size([180]) || norm.bias
 |  0.000 | -0.103 |  0.083 |  0.017 | torch.Size([180, 180, 3, 3]) || conv_after_body.weight
 | -0.002 | -0.042 |  0.040 |  0.016 | torch.Size([180]) || conv_after_body.bias
 |  0.000 | -0.265 |  0.214 |  0.031 | torch.Size([64, 180, 3, 3]) || conv_before_upsample.0.weight
 | -0.034 | -0.108 |  0.025 |  0.022 | torch.Size([64]) || conv_before_upsample.0.bias
 | -0.000 | -0.325 |  0.231 |  0.033 | torch.Size([256, 64, 3, 3]) || upsample.0.weight
 | -0.002 | -0.120 |  0.111 |  0.035 | torch.Size([256]) || upsample.0.bias
 |  0.000 | -0.044 |  0.051 |  0.009 | torch.Size([1, 64, 3, 3]) || conv_last.weight
 |  0.112 |  0.112 |  0.112 |    nan | torch.Size([1]) || conv_last.bias

22-10-24 11:31:52.580 :   task: swinir_denoising_sr_x2_charbonnier
  model: plain
  gpu_ids: [0]
  dist: False
  scale: 2
  n_channels: 1
  path:[
    root: superresolution
    pretrained_netG: superresolution\swinir_denoising_sr_x2_charbonnier\models\50000_G.pth
    pretrained_netE: superresolution\swinir_denoising_sr_x2_charbonnier\models\50000_E.pth
    task: superresolution\swinir_denoising_sr_x2_charbonnier
    log: superresolution\swinir_denoising_sr_x2_charbonnier
    options: superresolution\swinir_denoising_sr_x2_charbonnier\options
    models: superresolution\swinir_denoising_sr_x2_charbonnier\models
    images: superresolution\swinir_denoising_sr_x2_charbonnier\images
    pretrained_optimizerG: superresolution\swinir_denoising_sr_x2_charbonnier\models\50000_optimizerG.pth
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr
      dataroot_H: trainsets/trainH
      dataroot_L: trainsets/X4_lq_x2_lr
      H_size: 96
      dataloader_shuffle: True
      dataloader_num_workers: 12
      dataloader_batch_size: 1
      phase: train
      scale: 2
      n_channels: 1
    ]
    test:[
      name: test_dataset
      dataset_type: sr
      dataroot_H: testsets/set12
      dataroot_L: testsets/set12_valid_lq_x2_lr
      phase: test
      scale: 2
      n_channels: 1
    ]
  ]
  netG:[
    net_type: swinir
    upscale: 2
    in_chans: 1
    img_size: 48
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    upsampler: pixelshuffle
    resi_connection: 1conv
    init_type: default
    scale: 2
  ]
  train:[
    G_lossfn_type: l2
    G_lossfn_weight: 1.0
    E_decay: 0.999
    G_optimizer_type: adam
    G_optimizer_lr: 0.0002
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_optimizer_reuse: True
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [250000, 400000, 450000, 475000, 500000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    G_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
  ]
  opt_path: options\swinir\train_swinir_sr_classical.json
  is_train: True
  merge_bn: False
  merge_bn_startpoint: -1
  find_unused_parameters: True
  num_gpu: 1
  rank: 0
  world_size: 1

22-10-24 11:31:52.619 : Number of train images: 800, iters: 800
22-10-24 12:54:04.598 :   task: swinir_denoising_sr_x2_charbonnier
  model: plain
  gpu_ids: [0]
  dist: False
  scale: 2
  n_channels: 1
  path:[
    root: superresolution
    pretrained_netG: superresolution\swinir_denoising_sr_x2_charbonnier\models\50000_G.pth
    pretrained_netE: superresolution\swinir_denoising_sr_x2_charbonnier\models\50000_E.pth
    task: superresolution\swinir_denoising_sr_x2_charbonnier
    log: superresolution\swinir_denoising_sr_x2_charbonnier
    options: superresolution\swinir_denoising_sr_x2_charbonnier\options
    models: superresolution\swinir_denoising_sr_x2_charbonnier\models
    images: superresolution\swinir_denoising_sr_x2_charbonnier\images
    pretrained_optimizerG: superresolution\swinir_denoising_sr_x2_charbonnier\models\50000_optimizerG.pth
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr
      dataroot_H: trainsets/trainH
      dataroot_L: trainsets/X4_lq_x2_lr
      H_size: 96
      dataloader_shuffle: True
      dataloader_num_workers: 12
      dataloader_batch_size: 1
      phase: train
      scale: 2
      n_channels: 1
    ]
    test:[
      name: test_dataset
      dataset_type: sr
      dataroot_H: testsets/set12
      dataroot_L: testsets/set12_valid_lq_x2_lr
      phase: test
      scale: 2
      n_channels: 1
    ]
  ]
  netG:[
    net_type: swinir
    upscale: 2
    in_chans: 1
    img_size: 48
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    upsampler: pixelshuffle
    resi_connection: 1conv
    init_type: default
    scale: 2
  ]
  train:[
    G_lossfn_type: l2
    G_lossfn_weight: 1.0
    E_decay: 0.999
    G_optimizer_type: adam
    G_optimizer_lr: 0.0002
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_optimizer_reuse: True
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [250000, 400000, 450000, 475000, 500000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    G_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
  ]
  opt_path: options\swinir\train_swinir_sr_classical.json
  is_train: True
  merge_bn: False
  merge_bn_startpoint: -1
  find_unused_parameters: True
  num_gpu: 1
  rank: 0
  world_size: 1

22-10-24 12:54:04.613 : Number of train images: 800, iters: 800
22-10-24 12:54:59.290 :   task: swinir_denoising_sr_x2_charbonnier
  model: plain
  gpu_ids: [0]
  dist: False
  scale: 2
  n_channels: 1
  path:[
    root: superresolution
    pretrained_netG: superresolution\swinir_denoising_sr_x2_charbonnier\models\50000_G.pth
    pretrained_netE: superresolution\swinir_denoising_sr_x2_charbonnier\models\50000_E.pth
    task: superresolution\swinir_denoising_sr_x2_charbonnier
    log: superresolution\swinir_denoising_sr_x2_charbonnier
    options: superresolution\swinir_denoising_sr_x2_charbonnier\options
    models: superresolution\swinir_denoising_sr_x2_charbonnier\models
    images: superresolution\swinir_denoising_sr_x2_charbonnier\images
    pretrained_optimizerG: superresolution\swinir_denoising_sr_x2_charbonnier\models\50000_optimizerG.pth
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr
      dataroot_H: trainsets/trainH
      dataroot_L: trainsets/X4_lq_x2_lr
      H_size: 96
      dataloader_shuffle: True
      dataloader_num_workers: 12
      dataloader_batch_size: 1
      phase: train
      scale: 2
      n_channels: 1
    ]
    test:[
      name: test_dataset
      dataset_type: sr
      dataroot_H: testsets/set12
      dataroot_L: testsets/set12_valid_lq_x2_lr
      phase: test
      scale: 2
      n_channels: 1
    ]
  ]
  netG:[
    net_type: swinir
    upscale: 2
    in_chans: 1
    img_size: 48
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    upsampler: pixelshuffle
    resi_connection: 1conv
    init_type: default
    scale: 2
  ]
  train:[
    G_lossfn_type: l2
    G_lossfn_weight: 1.0
    E_decay: 0.999
    G_optimizer_type: adam
    G_optimizer_lr: 0.0002
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_optimizer_reuse: True
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [250000, 400000, 450000, 475000, 500000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    G_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
  ]
  opt_path: .\options\swinir\train_swinir_sr_classical.json
  is_train: True
  merge_bn: False
  merge_bn_startpoint: -1
  find_unused_parameters: True
  num_gpu: 1
  rank: 0
  world_size: 1

22-10-24 12:54:59.305 : Number of train images: 800, iters: 800
22-10-24 12:59:57.670 :   task: swinir_denoising_sr_x2_charbonnier
  model: plain
  gpu_ids: [0]
  dist: False
  scale: 2
  n_channels: 1
  path:[
    root: superresolution
    pretrained_netG: superresolution\swinir_denoising_sr_x2_charbonnier\models\50000_G.pth
    pretrained_netE: superresolution\swinir_denoising_sr_x2_charbonnier\models\50000_E.pth
    task: superresolution\swinir_denoising_sr_x2_charbonnier
    log: superresolution\swinir_denoising_sr_x2_charbonnier
    options: superresolution\swinir_denoising_sr_x2_charbonnier\options
    models: superresolution\swinir_denoising_sr_x2_charbonnier\models
    images: superresolution\swinir_denoising_sr_x2_charbonnier\images
    pretrained_optimizerG: superresolution\swinir_denoising_sr_x2_charbonnier\models\50000_optimizerG.pth
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr
      dataroot_H: trainsets/trainH
      dataroot_L: trainsets/X4_lq_x2_lr
      H_size: 96
      dataloader_shuffle: True
      dataloader_num_workers: 12
      dataloader_batch_size: 1
      phase: train
      scale: 2
      n_channels: 1
    ]
    test:[
      name: test_dataset
      dataset_type: sr
      dataroot_H: testsets/set12
      dataroot_L: testsets/set12_valid_lq_x2_lr
      phase: test
      scale: 2
      n_channels: 1
    ]
  ]
  netG:[
    net_type: swinir
    upscale: 2
    in_chans: 1
    img_size: 48
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    upsampler: pixelshuffle
    resi_connection: 1conv
    init_type: default
    scale: 2
  ]
  train:[
    G_lossfn_type: l2
    G_lossfn_weight: 1.0
    E_decay: 0.999
    G_optimizer_type: adam
    G_optimizer_lr: 0.0002
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_optimizer_reuse: True
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [250000, 400000, 450000, 475000, 500000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    G_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
  ]
  opt_path: .\options\swinir\train_swinir_sr_classical.json
  is_train: True
  merge_bn: False
  merge_bn_startpoint: -1
  find_unused_parameters: True
  num_gpu: 1
  rank: 0
  world_size: 1

22-10-24 12:59:57.685 : Number of train images: 800, iters: 800
22-10-24 13:00:02.813 : 
Networks name: SwinIR
Params number: 11748093
Net structure:
SwinIR(
  (conv_first): Conv2d(1, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (patch_embed): PatchEmbed(
    (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  )
  (patch_unembed): PatchUnEmbed()
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (1): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (2): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (3): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (4): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (5): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
  )
  (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  (conv_after_body): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_before_upsample): Sequential(
    (0): Conv2d(180, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
  )
  (upsample): Upsample(
    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): PixelShuffle(upscale_factor=2)
  )
  (conv_last): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

22-10-24 13:00:02.996 : 
 |  mean  |  min   |  max   |  std   || shape               
 |  0.003 | -0.505 |  0.488 |  0.221 | torch.Size([180, 1, 3, 3]) || conv_first.weight
 | -0.009 | -0.371 |  0.342 |  0.203 | torch.Size([180]) || conv_first.bias
 |  1.021 |  0.961 |  1.071 |  0.022 | torch.Size([180]) || patch_embed.norm.weight
 |  0.001 | -0.041 |  0.073 |  0.024 | torch.Size([180]) || patch_embed.norm.bias
 |  0.996 |  0.906 |  1.053 |  0.025 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.weight
 |  0.001 | -0.084 |  0.121 |  0.030 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.bias
 | -0.007 | -0.100 |  0.117 |  0.033 | torch.Size([225, 6]) || layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.152 |  0.195 |  0.032 | torch.Size([540, 180]) || layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 | -0.091 |  0.041 |  0.020 | torch.Size([540]) || layers.0.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.233 |  0.259 |  0.028 | torch.Size([180, 180]) || layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.001 | -0.041 |  0.046 |  0.026 | torch.Size([180]) || layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.017 |  0.931 |  1.093 |  0.031 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 | -0.048 |  0.044 |  0.030 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.261 |  0.261 |  0.033 | torch.Size([360, 180]) || layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.012 | -0.032 |  0.043 |  0.020 | torch.Size([360]) || layers.0.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.143 |  0.157 |  0.031 | torch.Size([180, 360]) || layers.0.residual_group.blocks.0.mlp.fc2.weight
 | -0.000 | -0.043 |  0.040 |  0.023 | torch.Size([180]) || layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.1.attn_mask
 |  1.006 |  0.945 |  1.035 |  0.018 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.weight
 | -0.001 | -0.046 |  0.039 |  0.020 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.bias
 | -0.003 | -0.074 |  0.074 |  0.024 | torch.Size([225, 6]) || layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.112 |  0.119 |  0.029 | torch.Size([540, 180]) || layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 | -0.044 |  0.043 |  0.022 | torch.Size([540]) || layers.0.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.096 |  0.115 |  0.026 | torch.Size([180, 180]) || layers.0.residual_group.blocks.1.attn.proj.weight
 | -0.000 | -0.042 |  0.040 |  0.022 | torch.Size([180]) || layers.0.residual_group.blocks.1.attn.proj.bias
 |  0.989 |  0.924 |  1.061 |  0.023 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.weight
 |  0.001 | -0.045 |  0.080 |  0.021 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.102 |  0.107 |  0.025 | torch.Size([360, 180]) || layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.004 | -0.024 |  0.044 |  0.016 | torch.Size([360]) || layers.0.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.121 |  0.107 |  0.025 | torch.Size([180, 360]) || layers.0.residual_group.blocks.1.mlp.fc2.weight
 | -0.000 | -0.042 |  0.041 |  0.023 | torch.Size([180]) || layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  0.999 |  0.959 |  1.035 |  0.017 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.weight
 | -0.001 | -0.034 |  0.040 |  0.018 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.bias
 | -0.002 | -0.065 |  0.073 |  0.022 | torch.Size([225, 6]) || layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.121 |  0.109 |  0.027 | torch.Size([540, 180]) || layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 | -0.050 |  0.048 |  0.019 | torch.Size([540]) || layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.112 |  0.104 |  0.025 | torch.Size([180, 180]) || layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 | -0.041 |  0.040 |  0.023 | torch.Size([180]) || layers.0.residual_group.blocks.2.attn.proj.bias
 |  0.981 |  0.954 |  1.021 |  0.013 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.weight
 |  0.002 | -0.046 |  0.056 |  0.019 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.098 |  0.105 |  0.024 | torch.Size([360, 180]) || layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.001 | -0.025 |  0.041 |  0.012 | torch.Size([360]) || layers.0.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.108 |  0.100 |  0.024 | torch.Size([180, 360]) || layers.0.residual_group.blocks.2.mlp.fc2.weight
 | -0.000 | -0.041 |  0.040 |  0.023 | torch.Size([180]) || layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.3.attn_mask
 |  0.999 |  0.959 |  1.038 |  0.016 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.weight
 | -0.001 | -0.039 |  0.039 |  0.020 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.bias
 | -0.002 | -0.081 |  0.081 |  0.023 | torch.Size([225, 6]) || layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.112 |  0.116 |  0.027 | torch.Size([540, 180]) || layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 | -0.040 |  0.043 |  0.020 | torch.Size([540]) || layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.091 |  0.099 |  0.025 | torch.Size([180, 180]) || layers.0.residual_group.blocks.3.attn.proj.weight
 | -0.000 | -0.044 |  0.040 |  0.025 | torch.Size([180]) || layers.0.residual_group.blocks.3.attn.proj.bias
 |  0.983 |  0.952 |  1.035 |  0.014 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.weight
 |  0.001 | -0.046 |  0.042 |  0.018 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.111 |  0.127 |  0.024 | torch.Size([360, 180]) || layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 | -0.028 |  0.032 |  0.011 | torch.Size([360]) || layers.0.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.100 |  0.103 |  0.024 | torch.Size([180, 360]) || layers.0.residual_group.blocks.3.mlp.fc2.weight
 | -0.000 | -0.042 |  0.040 |  0.025 | torch.Size([180]) || layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  0.990 |  0.961 |  1.026 |  0.013 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.weight
 |  0.000 | -0.033 |  0.034 |  0.016 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.065 |  0.093 |  0.022 | torch.Size([225, 6]) || layers.0.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.108 |  0.111 |  0.025 | torch.Size([540, 180]) || layers.0.residual_group.blocks.4.attn.qkv.weight
 | -0.000 | -0.040 |  0.043 |  0.017 | torch.Size([540]) || layers.0.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.106 |  0.094 |  0.024 | torch.Size([180, 180]) || layers.0.residual_group.blocks.4.attn.proj.weight
 | -0.001 | -0.041 |  0.040 |  0.026 | torch.Size([180]) || layers.0.residual_group.blocks.4.attn.proj.bias
 |  0.984 |  0.952 |  1.028 |  0.014 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.weight
 | -0.001 | -0.041 |  0.045 |  0.017 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.101 |  0.131 |  0.024 | torch.Size([360, 180]) || layers.0.residual_group.blocks.4.mlp.fc1.weight
 |  0.001 | -0.040 |  0.041 |  0.014 | torch.Size([360]) || layers.0.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.101 |  0.121 |  0.024 | torch.Size([180, 360]) || layers.0.residual_group.blocks.4.mlp.fc2.weight
 | -0.001 | -0.040 |  0.040 |  0.026 | torch.Size([180]) || layers.0.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.5.attn_mask
 |  0.994 |  0.956 |  1.029 |  0.015 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.weight
 | -0.001 | -0.045 |  0.044 |  0.018 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.092 |  0.087 |  0.025 | torch.Size([225, 6]) || layers.0.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.103 |  0.104 |  0.025 | torch.Size([540, 180]) || layers.0.residual_group.blocks.5.attn.qkv.weight
 |  0.000 | -0.040 |  0.042 |  0.017 | torch.Size([540]) || layers.0.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.108 |  0.100 |  0.024 | torch.Size([180, 180]) || layers.0.residual_group.blocks.5.attn.proj.weight
 | -0.001 | -0.041 |  0.042 |  0.027 | torch.Size([180]) || layers.0.residual_group.blocks.5.attn.proj.bias
 |  0.989 |  0.960 |  1.033 |  0.014 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.weight
 | -0.000 | -0.043 |  0.045 |  0.021 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.114 |  0.111 |  0.025 | torch.Size([360, 180]) || layers.0.residual_group.blocks.5.mlp.fc1.weight
 |  0.001 | -0.039 |  0.037 |  0.014 | torch.Size([360]) || layers.0.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.117 |  0.094 |  0.024 | torch.Size([180, 360]) || layers.0.residual_group.blocks.5.mlp.fc2.weight
 | -0.000 | -0.041 |  0.043 |  0.028 | torch.Size([180]) || layers.0.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.103 |  0.092 |  0.022 | torch.Size([180, 180, 3, 3]) || layers.0.conv.weight
 |  0.003 | -0.060 |  0.061 |  0.030 | torch.Size([180]) || layers.0.conv.bias
 |  0.997 |  0.968 |  1.023 |  0.013 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.weight
 | -0.000 | -0.038 |  0.037 |  0.022 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.064 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.103 |  0.111 |  0.023 | torch.Size([540, 180]) || layers.1.residual_group.blocks.0.attn.qkv.weight
 | -0.000 | -0.040 |  0.038 |  0.016 | torch.Size([540]) || layers.1.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.092 |  0.103 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 | -0.036 |  0.036 |  0.022 | torch.Size([180]) || layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.001 |  0.966 |  1.042 |  0.014 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.weight
 |  0.002 | -0.041 |  0.040 |  0.019 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.096 |  0.106 |  0.025 | torch.Size([360, 180]) || layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.004 | -0.034 |  0.042 |  0.018 | torch.Size([360]) || layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.096 |  0.106 |  0.024 | torch.Size([180, 360]) || layers.1.residual_group.blocks.0.mlp.fc2.weight
 | -0.000 | -0.043 |  0.043 |  0.026 | torch.Size([180]) || layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.1.attn_mask
 |  1.003 |  0.958 |  1.032 |  0.016 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 | -0.047 |  0.044 |  0.027 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.bias
 |  0.001 | -0.071 |  0.078 |  0.022 | torch.Size([225, 6]) || layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.110 |  0.108 |  0.025 | torch.Size([540, 180]) || layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 | -0.044 |  0.048 |  0.021 | torch.Size([540]) || layers.1.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.097 |  0.097 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 | -0.044 |  0.043 |  0.027 | torch.Size([180]) || layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.002 |  0.965 |  1.039 |  0.015 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.weight
 |  0.001 | -0.045 |  0.045 |  0.019 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.109 |  0.107 |  0.025 | torch.Size([360, 180]) || layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.004 | -0.033 |  0.042 |  0.018 | torch.Size([360]) || layers.1.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.095 |  0.099 |  0.024 | torch.Size([180, 360]) || layers.1.residual_group.blocks.1.mlp.fc2.weight
 | -0.000 | -0.044 |  0.043 |  0.027 | torch.Size([180]) || layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  0.996 |  0.958 |  1.026 |  0.013 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.weight
 |  0.001 | -0.038 |  0.043 |  0.022 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.bias
 |  0.001 | -0.071 |  0.063 |  0.021 | torch.Size([225, 6]) || layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.100 |  0.097 |  0.023 | torch.Size([540, 180]) || layers.1.residual_group.blocks.2.attn.qkv.weight
 | -0.001 | -0.045 |  0.050 |  0.016 | torch.Size([540]) || layers.1.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.095 |  0.087 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.2.attn.proj.weight
 | -0.000 | -0.044 |  0.044 |  0.028 | torch.Size([180]) || layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.006 |  0.961 |  1.039 |  0.016 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.weight
 |  0.001 | -0.045 |  0.048 |  0.024 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.108 |  0.104 |  0.025 | torch.Size([360, 180]) || layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.009 | -0.042 |  0.044 |  0.019 | torch.Size([360]) || layers.1.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.097 |  0.102 |  0.025 | torch.Size([180, 360]) || layers.1.residual_group.blocks.2.mlp.fc2.weight
 | -0.000 | -0.044 |  0.044 |  0.028 | torch.Size([180]) || layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.3.attn_mask
 |  1.001 |  0.968 |  1.030 |  0.012 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.weight
 |  0.001 | -0.039 |  0.038 |  0.020 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.065 |  0.077 |  0.021 | torch.Size([225, 6]) || layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.104 |  0.101 |  0.025 | torch.Size([540, 180]) || layers.1.residual_group.blocks.3.attn.qkv.weight
 | -0.001 | -0.041 |  0.040 |  0.017 | torch.Size([540]) || layers.1.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.119 |  0.095 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.3.attn.proj.weight
 | -0.001 | -0.039 |  0.038 |  0.024 | torch.Size([180]) || layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.008 |  0.966 |  1.055 |  0.015 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.weight
 |  0.001 | -0.040 |  0.043 |  0.021 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.108 |  0.104 |  0.025 | torch.Size([360, 180]) || layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.008 | -0.033 |  0.043 |  0.018 | torch.Size([360]) || layers.1.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.103 |  0.108 |  0.024 | torch.Size([180, 360]) || layers.1.residual_group.blocks.3.mlp.fc2.weight
 | -0.000 | -0.044 |  0.044 |  0.028 | torch.Size([180]) || layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  0.999 |  0.966 |  1.030 |  0.015 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.weight
 | -0.000 | -0.043 |  0.045 |  0.027 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.067 |  0.077 |  0.022 | torch.Size([225, 6]) || layers.1.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.099 |  0.097 |  0.024 | torch.Size([540, 180]) || layers.1.residual_group.blocks.4.attn.qkv.weight
 | -0.001 | -0.043 |  0.044 |  0.018 | torch.Size([540]) || layers.1.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.099 |  0.097 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.4.attn.proj.weight
 | -0.000 | -0.044 |  0.041 |  0.028 | torch.Size([180]) || layers.1.residual_group.blocks.4.attn.proj.bias
 |  1.008 |  0.968 |  1.052 |  0.015 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.weight
 | -0.000 | -0.040 |  0.042 |  0.022 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.099 |  0.101 |  0.025 | torch.Size([360, 180]) || layers.1.residual_group.blocks.4.mlp.fc1.weight
 |  0.009 | -0.035 |  0.043 |  0.018 | torch.Size([360]) || layers.1.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.110 |  0.103 |  0.024 | torch.Size([180, 360]) || layers.1.residual_group.blocks.4.mlp.fc2.weight
 | -0.000 | -0.046 |  0.044 |  0.030 | torch.Size([180]) || layers.1.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.5.attn_mask
 |  1.003 |  0.961 |  1.043 |  0.015 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.weight
 |  0.001 | -0.049 |  0.048 |  0.025 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.bias
 |  0.001 | -0.078 |  0.073 |  0.022 | torch.Size([225, 6]) || layers.1.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.103 |  0.099 |  0.024 | torch.Size([540, 180]) || layers.1.residual_group.blocks.5.attn.qkv.weight
 | -0.001 | -0.045 |  0.045 |  0.019 | torch.Size([540]) || layers.1.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.097 |  0.091 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.5.attn.proj.weight
 | -0.000 | -0.045 |  0.045 |  0.030 | torch.Size([180]) || layers.1.residual_group.blocks.5.attn.proj.bias
 |  1.011 |  0.967 |  1.049 |  0.016 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.weight
 |  0.002 | -0.042 |  0.044 |  0.022 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.107 |  0.108 |  0.026 | torch.Size([360, 180]) || layers.1.residual_group.blocks.5.mlp.fc1.weight
 |  0.010 | -0.040 |  0.044 |  0.018 | torch.Size([360]) || layers.1.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.099 |  0.097 |  0.024 | torch.Size([180, 360]) || layers.1.residual_group.blocks.5.mlp.fc2.weight
 | -0.000 | -0.045 |  0.047 |  0.030 | torch.Size([180]) || layers.1.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.110 |  0.132 |  0.022 | torch.Size([180, 180, 3, 3]) || layers.1.conv.weight
 | -0.000 | -0.055 |  0.058 |  0.028 | torch.Size([180]) || layers.1.conv.bias
 |  0.996 |  0.977 |  1.012 |  0.007 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.weight
 | -0.000 | -0.032 |  0.029 |  0.013 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.bias
 |  0.001 | -0.070 |  0.072 |  0.021 | torch.Size([225, 6]) || layers.2.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.094 |  0.097 |  0.021 | torch.Size([540, 180]) || layers.2.residual_group.blocks.0.attn.qkv.weight
 |  0.001 | -0.028 |  0.030 |  0.011 | torch.Size([540]) || layers.2.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.094 |  0.092 |  0.022 | torch.Size([180, 180]) || layers.2.residual_group.blocks.0.attn.proj.weight
 | -0.001 | -0.032 |  0.032 |  0.021 | torch.Size([180]) || layers.2.residual_group.blocks.0.attn.proj.bias
 |  1.011 |  0.972 |  1.042 |  0.015 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.weight
 | -0.000 | -0.042 |  0.043 |  0.019 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.122 |  0.109 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.0.mlp.fc1.weight
 |  0.009 | -0.034 |  0.050 |  0.019 | torch.Size([360]) || layers.2.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.087 |  0.106 |  0.022 | torch.Size([180, 360]) || layers.2.residual_group.blocks.0.mlp.fc2.weight
 | -0.001 | -0.040 |  0.040 |  0.025 | torch.Size([180]) || layers.2.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.1.attn_mask
 |  0.997 |  0.968 |  1.025 |  0.010 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.weight
 |  0.001 | -0.040 |  0.039 |  0.020 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.bias
 | -0.001 | -0.063 |  0.071 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.099 |  0.088 |  0.022 | torch.Size([540, 180]) || layers.2.residual_group.blocks.1.attn.qkv.weight
 |  0.000 | -0.040 |  0.042 |  0.016 | torch.Size([540]) || layers.2.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.085 |  0.093 |  0.022 | torch.Size([180, 180]) || layers.2.residual_group.blocks.1.attn.proj.weight
 | -0.001 | -0.038 |  0.039 |  0.025 | torch.Size([180]) || layers.2.residual_group.blocks.1.attn.proj.bias
 |  1.011 |  0.977 |  1.045 |  0.014 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.weight
 |  0.001 | -0.045 |  0.042 |  0.022 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.103 |  0.103 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.1.mlp.fc1.weight
 |  0.007 | -0.039 |  0.045 |  0.021 | torch.Size([360]) || layers.2.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.102 |  0.093 |  0.022 | torch.Size([180, 360]) || layers.2.residual_group.blocks.1.mlp.fc2.weight
 | -0.001 | -0.038 |  0.039 |  0.025 | torch.Size([180]) || layers.2.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.978 |  1.026 |  0.010 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.weight
 |  0.000 | -0.036 |  0.036 |  0.018 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.bias
 | -0.001 | -0.063 |  0.072 |  0.021 | torch.Size([225, 6]) || layers.2.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.096 |  0.091 |  0.022 | torch.Size([540, 180]) || layers.2.residual_group.blocks.2.attn.qkv.weight
 | -0.001 | -0.037 |  0.037 |  0.015 | torch.Size([540]) || layers.2.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.089 |  0.093 |  0.023 | torch.Size([180, 180]) || layers.2.residual_group.blocks.2.attn.proj.weight
 | -0.001 | -0.036 |  0.037 |  0.024 | torch.Size([180]) || layers.2.residual_group.blocks.2.attn.proj.bias
 |  1.010 |  0.975 |  1.041 |  0.013 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.weight
 | -0.000 | -0.041 |  0.041 |  0.018 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.098 |  0.125 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.2.mlp.fc1.weight
 |  0.009 | -0.040 |  0.042 |  0.021 | torch.Size([360]) || layers.2.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.089 |  0.106 |  0.023 | torch.Size([180, 360]) || layers.2.residual_group.blocks.2.mlp.fc2.weight
 | -0.001 | -0.038 |  0.038 |  0.026 | torch.Size([180]) || layers.2.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.3.attn_mask
 |  0.997 |  0.971 |  1.030 |  0.010 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.weight
 |  0.000 | -0.045 |  0.044 |  0.021 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.075 |  0.066 |  0.021 | torch.Size([225, 6]) || layers.2.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.098 |  0.100 |  0.022 | torch.Size([540, 180]) || layers.2.residual_group.blocks.3.attn.qkv.weight
 |  0.000 | -0.041 |  0.039 |  0.016 | torch.Size([540]) || layers.2.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.095 |  0.089 |  0.022 | torch.Size([180, 180]) || layers.2.residual_group.blocks.3.attn.proj.weight
 | -0.001 | -0.039 |  0.038 |  0.026 | torch.Size([180]) || layers.2.residual_group.blocks.3.attn.proj.bias
 |  1.010 |  0.971 |  1.038 |  0.014 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.weight
 | -0.000 | -0.047 |  0.044 |  0.019 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.104 |  0.097 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.3.mlp.fc1.weight
 |  0.007 | -0.037 |  0.042 |  0.022 | torch.Size([360]) || layers.2.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.104 |  0.096 |  0.023 | torch.Size([180, 360]) || layers.2.residual_group.blocks.3.mlp.fc2.weight
 | -0.001 | -0.038 |  0.037 |  0.026 | torch.Size([180]) || layers.2.residual_group.blocks.3.mlp.fc2.bias
 |  0.998 |  0.973 |  1.025 |  0.010 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.weight
 |  0.001 | -0.048 |  0.045 |  0.023 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.073 |  0.080 |  0.022 | torch.Size([225, 6]) || layers.2.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.098 |  0.098 |  0.022 | torch.Size([540, 180]) || layers.2.residual_group.blocks.4.attn.qkv.weight
 |  0.000 | -0.045 |  0.041 |  0.017 | torch.Size([540]) || layers.2.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.088 |  0.088 |  0.022 | torch.Size([180, 180]) || layers.2.residual_group.blocks.4.attn.proj.weight
 | -0.001 | -0.039 |  0.038 |  0.026 | torch.Size([180]) || layers.2.residual_group.blocks.4.attn.proj.bias
 |  1.009 |  0.978 |  1.040 |  0.012 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.weight
 | -0.001 | -0.038 |  0.038 |  0.019 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.107 |  0.102 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.4.mlp.fc1.weight
 |  0.008 | -0.040 |  0.041 |  0.021 | torch.Size([360]) || layers.2.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.105 |  0.098 |  0.023 | torch.Size([180, 360]) || layers.2.residual_group.blocks.4.mlp.fc2.weight
 | -0.001 | -0.039 |  0.038 |  0.026 | torch.Size([180]) || layers.2.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.5.attn_mask
 |  0.996 |  0.973 |  1.016 |  0.009 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.weight
 |  0.000 | -0.031 |  0.037 |  0.014 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.bias
 |  0.001 | -0.067 |  0.075 |  0.022 | torch.Size([225, 6]) || layers.2.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.095 |  0.088 |  0.022 | torch.Size([540, 180]) || layers.2.residual_group.blocks.5.attn.qkv.weight
 |  0.000 | -0.036 |  0.033 |  0.013 | torch.Size([540]) || layers.2.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.078 |  0.080 |  0.021 | torch.Size([180, 180]) || layers.2.residual_group.blocks.5.attn.proj.weight
 | -0.001 | -0.032 |  0.032 |  0.022 | torch.Size([180]) || layers.2.residual_group.blocks.5.attn.proj.bias
 |  1.010 |  0.978 |  1.039 |  0.014 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.weight
 | -0.000 | -0.045 |  0.041 |  0.021 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.100 |  0.113 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.5.mlp.fc1.weight
 |  0.009 | -0.040 |  0.046 |  0.023 | torch.Size([360]) || layers.2.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.102 |  0.090 |  0.023 | torch.Size([180, 360]) || layers.2.residual_group.blocks.5.mlp.fc2.weight
 | -0.001 | -0.038 |  0.038 |  0.027 | torch.Size([180]) || layers.2.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.200 |  0.207 |  0.024 | torch.Size([180, 180, 3, 3]) || layers.2.conv.weight
 | -0.000 | -0.048 |  0.049 |  0.020 | torch.Size([180]) || layers.2.conv.bias
 |  1.000 |  0.984 |  1.020 |  0.006 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.weight
 | -0.001 | -0.025 |  0.029 |  0.015 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.062 |  0.068 |  0.021 | torch.Size([225, 6]) || layers.3.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.089 |  0.086 |  0.021 | torch.Size([540, 180]) || layers.3.residual_group.blocks.0.attn.qkv.weight
 | -0.000 | -0.028 |  0.027 |  0.011 | torch.Size([540]) || layers.3.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.091 |  0.093 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.0.attn.proj.weight
 |  0.000 | -0.025 |  0.027 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.0.attn.proj.bias
 |  1.005 |  0.989 |  1.030 |  0.008 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.weight
 |  0.001 | -0.029 |  0.031 |  0.016 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.093 |  0.100 |  0.021 | torch.Size([360, 180]) || layers.3.residual_group.blocks.0.mlp.fc1.weight
 |  0.004 | -0.022 |  0.030 |  0.013 | torch.Size([360]) || layers.3.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.090 |  0.098 |  0.021 | torch.Size([180, 360]) || layers.3.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 | -0.027 |  0.027 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.1.attn_mask
 |  1.001 |  0.992 |  1.019 |  0.005 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.weight
 | -0.000 | -0.028 |  0.030 |  0.015 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.069 |  0.063 |  0.021 | torch.Size([225, 6]) || layers.3.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.101 |  0.095 |  0.021 | torch.Size([540, 180]) || layers.3.residual_group.blocks.1.attn.qkv.weight
 | -0.000 | -0.029 |  0.028 |  0.010 | torch.Size([540]) || layers.3.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.087 |  0.086 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.1.attn.proj.weight
 |  0.000 | -0.026 |  0.027 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.1.attn.proj.bias
 |  1.004 |  0.992 |  1.026 |  0.007 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.weight
 |  0.000 | -0.024 |  0.027 |  0.012 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.086 |  0.089 |  0.021 | torch.Size([360, 180]) || layers.3.residual_group.blocks.1.mlp.fc1.weight
 |  0.004 | -0.017 |  0.024 |  0.009 | torch.Size([360]) || layers.3.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.093 |  0.090 |  0.021 | torch.Size([180, 360]) || layers.3.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 | -0.021 |  0.022 |  0.013 | torch.Size([180]) || layers.3.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.990 |  1.017 |  0.005 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.weight
 |  0.000 | -0.030 |  0.029 |  0.015 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.060 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.090 |  0.100 |  0.021 | torch.Size([540, 180]) || layers.3.residual_group.blocks.2.attn.qkv.weight
 |  0.001 | -0.027 |  0.028 |  0.010 | torch.Size([540]) || layers.3.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.094 |  0.077 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.2.attn.proj.weight
 |  0.001 | -0.027 |  0.028 |  0.018 | torch.Size([180]) || layers.3.residual_group.blocks.2.attn.proj.bias
 |  1.005 |  0.987 |  1.025 |  0.008 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.weight
 | -0.000 | -0.031 |  0.034 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.100 |  0.087 |  0.021 | torch.Size([360, 180]) || layers.3.residual_group.blocks.2.mlp.fc1.weight
 |  0.004 | -0.022 |  0.029 |  0.013 | torch.Size([360]) || layers.3.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.096 |  0.095 |  0.021 | torch.Size([180, 360]) || layers.3.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 | -0.026 |  0.026 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.3.attn_mask
 |  1.001 |  0.986 |  1.018 |  0.005 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.weight
 | -0.000 | -0.028 |  0.024 |  0.014 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.072 |  0.071 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.099 |  0.092 |  0.022 | torch.Size([540, 180]) || layers.3.residual_group.blocks.3.attn.qkv.weight
 | -0.000 | -0.026 |  0.024 |  0.010 | torch.Size([540]) || layers.3.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.082 |  0.086 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.3.attn.proj.weight
 |  0.000 | -0.021 |  0.024 |  0.014 | torch.Size([180]) || layers.3.residual_group.blocks.3.attn.proj.bias
 |  1.003 |  0.990 |  1.016 |  0.006 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.weight
 |  0.001 | -0.023 |  0.021 |  0.012 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.084 |  0.089 |  0.021 | torch.Size([360, 180]) || layers.3.residual_group.blocks.3.mlp.fc1.weight
 |  0.002 | -0.020 |  0.020 |  0.010 | torch.Size([360]) || layers.3.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.102 |  0.080 |  0.021 | torch.Size([180, 360]) || layers.3.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 | -0.020 |  0.019 |  0.013 | torch.Size([180]) || layers.3.residual_group.blocks.3.mlp.fc2.bias
 |  1.001 |  0.985 |  1.018 |  0.007 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.weight
 |  0.000 | -0.030 |  0.026 |  0.016 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.062 |  0.071 |  0.021 | torch.Size([225, 6]) || layers.3.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.087 |  0.090 |  0.021 | torch.Size([540, 180]) || layers.3.residual_group.blocks.4.attn.qkv.weight
 | -0.000 | -0.028 |  0.027 |  0.011 | torch.Size([540]) || layers.3.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.097 |  0.081 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.4.attn.proj.weight
 |  0.001 | -0.026 |  0.026 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.4.attn.proj.bias
 |  1.005 |  0.986 |  1.022 |  0.007 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.weight
 |  0.001 | -0.026 |  0.025 |  0.014 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.089 |  0.096 |  0.021 | torch.Size([360, 180]) || layers.3.residual_group.blocks.4.mlp.fc1.weight
 |  0.003 | -0.020 |  0.027 |  0.012 | torch.Size([360]) || layers.3.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.101 |  0.099 |  0.021 | torch.Size([180, 360]) || layers.3.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 | -0.022 |  0.023 |  0.015 | torch.Size([180]) || layers.3.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.5.attn_mask
 |  1.000 |  0.987 |  1.017 |  0.006 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.weight
 |  0.000 | -0.026 |  0.028 |  0.014 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.070 |  0.074 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.084 |  0.094 |  0.021 | torch.Size([540, 180]) || layers.3.residual_group.blocks.5.attn.qkv.weight
 | -0.000 | -0.027 |  0.031 |  0.011 | torch.Size([540]) || layers.3.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.092 |  0.080 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.5.attn.proj.weight
 |  0.000 | -0.025 |  0.026 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.5.attn.proj.bias
 |  1.006 |  0.987 |  1.029 |  0.009 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.weight
 | -0.001 | -0.028 |  0.030 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.104 |  0.091 |  0.022 | torch.Size([360, 180]) || layers.3.residual_group.blocks.5.mlp.fc1.weight
 |  0.004 | -0.024 |  0.034 |  0.014 | torch.Size([360]) || layers.3.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.091 |  0.089 |  0.022 | torch.Size([180, 360]) || layers.3.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 | -0.027 |  0.027 |  0.018 | torch.Size([180]) || layers.3.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.432 |  0.456 |  0.031 | torch.Size([180, 180, 3, 3]) || layers.3.conv.weight
 | -0.000 | -0.028 |  0.031 |  0.015 | torch.Size([180]) || layers.3.conv.bias
 |  1.000 |  0.992 |  1.013 |  0.003 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.weight
 | -0.000 | -0.012 |  0.014 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.062 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.089 |  0.090 |  0.021 | torch.Size([540, 180]) || layers.4.residual_group.blocks.0.attn.qkv.weight
 |  0.000 | -0.013 |  0.013 |  0.004 | torch.Size([540]) || layers.4.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.078 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.0.attn.proj.weight
 | -0.000 | -0.013 |  0.012 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.0.attn.proj.bias
 |  1.001 |  0.992 |  1.014 |  0.004 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.weight
 | -0.001 | -0.017 |  0.012 |  0.007 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.093 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.0.mlp.fc1.weight
 |  0.001 | -0.009 |  0.015 |  0.005 | torch.Size([360]) || layers.4.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.095 |  0.090 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.0.mlp.fc2.weight
 | -0.000 | -0.013 |  0.012 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.1.attn_mask
 |  1.000 |  0.991 |  1.011 |  0.004 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.weight
 | -0.000 | -0.014 |  0.013 |  0.007 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.080 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.092 |  0.097 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.1.attn.qkv.weight
 | -0.000 | -0.017 |  0.016 |  0.005 | torch.Size([540]) || layers.4.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.091 |  0.084 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.1.attn.proj.weight
 |  0.000 | -0.011 |  0.011 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.1.attn.proj.bias
 |  1.002 |  0.990 |  1.021 |  0.004 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.weight
 | -0.000 | -0.014 |  0.015 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.095 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.1.mlp.fc1.weight
 |  0.001 | -0.009 |  0.016 |  0.005 | torch.Size([360]) || layers.4.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.088 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.1.mlp.fc2.weight
 | -0.000 | -0.012 |  0.011 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.992 |  1.008 |  0.003 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.weight
 | -0.000 | -0.013 |  0.011 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.075 |  0.068 |  0.021 | torch.Size([225, 6]) || layers.4.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.095 |  0.097 |  0.021 | torch.Size([540, 180]) || layers.4.residual_group.blocks.2.attn.qkv.weight
 |  0.000 | -0.012 |  0.015 |  0.004 | torch.Size([540]) || layers.4.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.079 |  0.090 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.2.attn.proj.weight
 | -0.000 | -0.013 |  0.011 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.2.attn.proj.bias
 |  1.003 |  0.989 |  1.019 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.weight
 |  0.000 | -0.016 |  0.017 |  0.008 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.087 |  0.088 |  0.021 | torch.Size([360, 180]) || layers.4.residual_group.blocks.2.mlp.fc1.weight
 |  0.001 | -0.010 |  0.021 |  0.006 | torch.Size([360]) || layers.4.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.089 |  0.099 |  0.021 | torch.Size([180, 360]) || layers.4.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 | -0.012 |  0.012 |  0.007 | torch.Size([180]) || layers.4.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.3.attn_mask
 |  1.001 |  0.992 |  1.011 |  0.003 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.weight
 | -0.000 | -0.013 |  0.014 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.088 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.102 |  0.091 |  0.021 | torch.Size([540, 180]) || layers.4.residual_group.blocks.3.attn.qkv.weight
 | -0.000 | -0.013 |  0.015 |  0.005 | torch.Size([540]) || layers.4.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.084 |  0.086 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.3.attn.proj.weight
 | -0.000 | -0.012 |  0.011 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.3.attn.proj.bias
 |  1.003 |  0.994 |  1.026 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.weight
 | -0.000 | -0.016 |  0.015 |  0.007 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.089 |  0.082 |  0.021 | torch.Size([360, 180]) || layers.4.residual_group.blocks.3.mlp.fc1.weight
 |  0.001 | -0.010 |  0.019 |  0.005 | torch.Size([360]) || layers.4.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.091 |  0.096 |  0.021 | torch.Size([180, 360]) || layers.4.residual_group.blocks.3.mlp.fc2.weight
 | -0.000 | -0.012 |  0.010 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.3.mlp.fc2.bias
 |  1.001 |  0.994 |  1.009 |  0.003 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.weight
 |  0.000 | -0.011 |  0.014 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.bias
 |  0.000 | -0.072 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.088 |  0.091 |  0.021 | torch.Size([540, 180]) || layers.4.residual_group.blocks.4.attn.qkv.weight
 | -0.001 | -0.012 |  0.009 |  0.004 | torch.Size([540]) || layers.4.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.083 |  0.087 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.4.attn.proj.weight
 | -0.000 | -0.013 |  0.012 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.4.attn.proj.bias
 |  1.003 |  0.993 |  1.018 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.weight
 | -0.000 | -0.018 |  0.016 |  0.008 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.086 |  0.078 |  0.021 | torch.Size([360, 180]) || layers.4.residual_group.blocks.4.mlp.fc1.weight
 |  0.002 | -0.009 |  0.016 |  0.005 | torch.Size([360]) || layers.4.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.088 |  0.087 |  0.021 | torch.Size([180, 360]) || layers.4.residual_group.blocks.4.mlp.fc2.weight
 | -0.000 | -0.012 |  0.011 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.5.attn_mask
 |  1.000 |  0.991 |  1.011 |  0.003 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.weight
 | -0.000 | -0.010 |  0.008 |  0.004 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.062 |  0.053 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.082 |  0.091 |  0.021 | torch.Size([540, 180]) || layers.4.residual_group.blocks.5.attn.qkv.weight
 | -0.000 | -0.013 |  0.014 |  0.005 | torch.Size([540]) || layers.4.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.085 |  0.097 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.5.attn.proj.weight
 | -0.000 | -0.010 |  0.011 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.5.attn.proj.bias
 |  1.002 |  0.990 |  1.021 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.weight
 | -0.001 | -0.016 |  0.013 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.092 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.5.mlp.fc1.weight
 |  0.001 | -0.016 |  0.016 |  0.005 | torch.Size([360]) || layers.4.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.088 |  0.086 |  0.021 | torch.Size([180, 360]) || layers.4.residual_group.blocks.5.mlp.fc2.weight
 | -0.000 | -0.014 |  0.012 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.749 |  0.758 |  0.043 | torch.Size([180, 180, 3, 3]) || layers.4.conv.weight
 |  0.001 | -0.028 |  0.031 |  0.016 | torch.Size([180]) || layers.4.conv.bias
 |  1.001 |  0.995 |  1.009 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.weight
 | -0.000 | -0.012 |  0.014 |  0.004 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.065 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.094 |  0.090 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.0.attn.qkv.weight
 |  0.000 | -0.014 |  0.013 |  0.004 | torch.Size([540]) || layers.5.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.083 |  0.089 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.0.attn.proj.weight
 |  0.000 | -0.007 |  0.008 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.0.attn.proj.bias
 |  1.001 |  0.994 |  1.011 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.weight
 |  0.000 | -0.006 |  0.004 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.098 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 | -0.007 |  0.012 |  0.002 | torch.Size([360]) || layers.5.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.084 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 | -0.005 |  0.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.1.attn_mask
 |  1.001 |  0.996 |  1.015 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.weight
 |  0.000 | -0.010 |  0.010 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.060 |  0.066 |  0.021 | torch.Size([225, 6]) || layers.5.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.088 |  0.083 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.1.attn.qkv.weight
 | -0.000 | -0.013 |  0.010 |  0.003 | torch.Size([540]) || layers.5.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.076 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.1.attn.proj.weight
 |  0.000 | -0.007 |  0.007 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.1.attn.proj.bias
 |  1.003 |  0.995 |  1.023 |  0.004 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.weight
 | -0.000 | -0.011 |  0.012 |  0.004 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.087 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.1.mlp.fc1.weight
 |  0.001 | -0.007 |  0.019 |  0.003 | torch.Size([360]) || layers.5.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.081 |  0.091 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 | -0.006 |  0.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.994 |  1.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.weight
 | -0.000 | -0.008 |  0.005 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.bias
 | -0.001 | -0.082 |  0.060 |  0.021 | torch.Size([225, 6]) || layers.5.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.097 |  0.089 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.2.attn.qkv.weight
 | -0.000 | -0.009 |  0.009 |  0.002 | torch.Size([540]) || layers.5.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.080 |  0.087 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.2.attn.proj.weight
 |  0.000 | -0.007 |  0.008 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.2.attn.proj.bias
 |  1.002 |  0.994 |  1.013 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.weight
 | -0.000 | -0.008 |  0.011 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.092 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.2.mlp.fc1.weight
 |  0.001 | -0.008 |  0.013 |  0.003 | torch.Size([360]) || layers.5.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.087 |  0.094 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 | -0.006 |  0.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.3.attn_mask
 |  1.000 |  0.995 |  1.009 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.weight
 | -0.000 | -0.005 |  0.008 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.bias
 |  0.001 | -0.059 |  0.074 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.095 |  0.092 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.3.attn.qkv.weight
 |  0.000 | -0.018 |  0.019 |  0.004 | torch.Size([540]) || layers.5.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.076 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.3.attn.proj.weight
 |  0.000 | -0.006 |  0.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.3.attn.proj.bias
 |  1.002 |  0.992 |  1.019 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.weight
 | -0.000 | -0.009 |  0.009 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.086 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.3.mlp.fc1.weight
 |  0.001 | -0.006 |  0.015 |  0.003 | torch.Size([360]) || layers.5.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.105 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  0.997 |  1.009 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.weight
 | -0.000 | -0.007 |  0.010 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.065 |  0.065 |  0.021 | torch.Size([225, 6]) || layers.5.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.085 |  0.093 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.4.attn.qkv.weight
 | -0.000 | -0.010 |  0.010 |  0.003 | torch.Size([540]) || layers.5.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.085 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.4.attn.proj.weight
 |  0.000 | -0.005 |  0.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.4.attn.proj.bias
 |  1.004 |  0.993 |  1.021 |  0.005 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.weight
 | -0.000 | -0.016 |  0.016 |  0.006 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.095 |  0.094 |  0.021 | torch.Size([360, 180]) || layers.5.residual_group.blocks.4.mlp.fc1.weight
 |  0.002 | -0.010 |  0.025 |  0.005 | torch.Size([360]) || layers.5.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.083 |  0.090 |  0.021 | torch.Size([180, 360]) || layers.5.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 | -0.008 |  0.009 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.5.attn_mask
 |  1.000 |  0.995 |  1.008 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.weight
 |  0.000 | -0.007 |  0.007 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.065 |  0.068 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.085 |  0.104 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.5.attn.qkv.weight
 |  0.000 | -0.014 |  0.014 |  0.004 | torch.Size([540]) || layers.5.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.087 |  0.091 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.5.attn.proj.weight
 |  0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.5.attn.proj.bias
 |  1.002 |  0.992 |  1.018 |  0.004 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.weight
 | -0.000 | -0.009 |  0.010 |  0.004 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.087 |  0.085 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.5.mlp.fc1.weight
 |  0.001 | -0.012 |  0.019 |  0.003 | torch.Size([360]) || layers.5.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.088 |  0.096 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 | -0.006 |  0.005 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -1.255 |  1.261 |  0.047 | torch.Size([180, 180, 3, 3]) || layers.5.conv.weight
 |  0.001 | -0.030 |  0.026 |  0.015 | torch.Size([180]) || layers.5.conv.bias
 |  0.426 |  0.276 |  0.702 |  0.052 | torch.Size([180]) || norm.weight
 |  0.027 | -0.484 |  0.640 |  0.060 | torch.Size([180]) || norm.bias
 |  0.000 | -0.103 |  0.083 |  0.017 | torch.Size([180, 180, 3, 3]) || conv_after_body.weight
 | -0.002 | -0.042 |  0.040 |  0.016 | torch.Size([180]) || conv_after_body.bias
 |  0.000 | -0.265 |  0.214 |  0.031 | torch.Size([64, 180, 3, 3]) || conv_before_upsample.0.weight
 | -0.034 | -0.108 |  0.025 |  0.022 | torch.Size([64]) || conv_before_upsample.0.bias
 | -0.000 | -0.325 |  0.231 |  0.033 | torch.Size([256, 64, 3, 3]) || upsample.0.weight
 | -0.002 | -0.120 |  0.111 |  0.035 | torch.Size([256]) || upsample.0.bias
 |  0.000 | -0.044 |  0.051 |  0.009 | torch.Size([1, 64, 3, 3]) || conv_last.weight
 |  0.112 |  0.112 |  0.112 |    nan | torch.Size([1]) || conv_last.bias

22-10-24 13:00:22.487 :   task: swinir_denoising_sr_x2_charbonnier
  model: plain
  gpu_ids: [0]
  dist: False
  scale: 2
  n_channels: 1
  path:[
    root: superresolution
    pretrained_netG: superresolution\swinir_denoising_sr_x2_charbonnier\models\50000_G.pth
    pretrained_netE: superresolution\swinir_denoising_sr_x2_charbonnier\models\50000_E.pth
    task: superresolution\swinir_denoising_sr_x2_charbonnier
    log: superresolution\swinir_denoising_sr_x2_charbonnier
    options: superresolution\swinir_denoising_sr_x2_charbonnier\options
    models: superresolution\swinir_denoising_sr_x2_charbonnier\models
    images: superresolution\swinir_denoising_sr_x2_charbonnier\images
    pretrained_optimizerG: superresolution\swinir_denoising_sr_x2_charbonnier\models\50000_optimizerG.pth
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr
      dataroot_H: trainsets/trainH
      dataroot_L: trainsets/X4_lq_x2_lr
      H_size: 96
      dataloader_shuffle: True
      dataloader_num_workers: 12
      dataloader_batch_size: 1
      phase: train
      scale: 2
      n_channels: 1
    ]
    test:[
      name: test_dataset
      dataset_type: sr
      dataroot_H: testsets/set12
      dataroot_L: testsets/set12_valid_lq_x2_lr
      phase: test
      scale: 2
      n_channels: 1
    ]
  ]
  netG:[
    net_type: swinir
    upscale: 2
    in_chans: 1
    img_size: 48
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    upsampler: pixelshuffle
    resi_connection: 1conv
    init_type: default
    scale: 2
  ]
  train:[
    G_lossfn_type: l2
    G_lossfn_weight: 1.0
    E_decay: 0.999
    G_optimizer_type: adam
    G_optimizer_lr: 0.0002
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_optimizer_reuse: True
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [250000, 400000, 450000, 475000, 500000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    G_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
  ]
  opt_path: .\options\swinir\train_swinir_sr_classical.json
  is_train: True
  merge_bn: False
  merge_bn_startpoint: -1
  find_unused_parameters: True
  num_gpu: 1
  rank: 0
  world_size: 1

22-10-24 13:00:22.502 : Number of train images: 800, iters: 800
22-10-24 13:00:24.738 : 
Networks name: SwinIR
Params number: 11748093
Net structure:
SwinIR(
  (conv_first): Conv2d(1, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (patch_embed): PatchEmbed(
    (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  )
  (patch_unembed): PatchUnEmbed()
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (1): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (2): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (3): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (4): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (5): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
  )
  (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  (conv_after_body): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_before_upsample): Sequential(
    (0): Conv2d(180, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
  )
  (upsample): Upsample(
    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): PixelShuffle(upscale_factor=2)
  )
  (conv_last): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

22-10-24 13:00:24.903 : 
 |  mean  |  min   |  max   |  std   || shape               
 |  0.003 | -0.505 |  0.488 |  0.221 | torch.Size([180, 1, 3, 3]) || conv_first.weight
 | -0.009 | -0.371 |  0.342 |  0.203 | torch.Size([180]) || conv_first.bias
 |  1.021 |  0.961 |  1.071 |  0.022 | torch.Size([180]) || patch_embed.norm.weight
 |  0.001 | -0.041 |  0.073 |  0.024 | torch.Size([180]) || patch_embed.norm.bias
 |  0.996 |  0.906 |  1.053 |  0.025 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.weight
 |  0.001 | -0.084 |  0.121 |  0.030 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.bias
 | -0.007 | -0.100 |  0.117 |  0.033 | torch.Size([225, 6]) || layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.152 |  0.195 |  0.032 | torch.Size([540, 180]) || layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 | -0.091 |  0.041 |  0.020 | torch.Size([540]) || layers.0.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.233 |  0.259 |  0.028 | torch.Size([180, 180]) || layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.001 | -0.041 |  0.046 |  0.026 | torch.Size([180]) || layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.017 |  0.931 |  1.093 |  0.031 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 | -0.048 |  0.044 |  0.030 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.261 |  0.261 |  0.033 | torch.Size([360, 180]) || layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.012 | -0.032 |  0.043 |  0.020 | torch.Size([360]) || layers.0.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.143 |  0.157 |  0.031 | torch.Size([180, 360]) || layers.0.residual_group.blocks.0.mlp.fc2.weight
 | -0.000 | -0.043 |  0.040 |  0.023 | torch.Size([180]) || layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.1.attn_mask
 |  1.006 |  0.945 |  1.035 |  0.018 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.weight
 | -0.001 | -0.046 |  0.039 |  0.020 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.bias
 | -0.003 | -0.074 |  0.074 |  0.024 | torch.Size([225, 6]) || layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.112 |  0.119 |  0.029 | torch.Size([540, 180]) || layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 | -0.044 |  0.043 |  0.022 | torch.Size([540]) || layers.0.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.096 |  0.115 |  0.026 | torch.Size([180, 180]) || layers.0.residual_group.blocks.1.attn.proj.weight
 | -0.000 | -0.042 |  0.040 |  0.022 | torch.Size([180]) || layers.0.residual_group.blocks.1.attn.proj.bias
 |  0.989 |  0.924 |  1.061 |  0.023 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.weight
 |  0.001 | -0.045 |  0.080 |  0.021 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.102 |  0.107 |  0.025 | torch.Size([360, 180]) || layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.004 | -0.024 |  0.044 |  0.016 | torch.Size([360]) || layers.0.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.121 |  0.107 |  0.025 | torch.Size([180, 360]) || layers.0.residual_group.blocks.1.mlp.fc2.weight
 | -0.000 | -0.042 |  0.041 |  0.023 | torch.Size([180]) || layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  0.999 |  0.959 |  1.035 |  0.017 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.weight
 | -0.001 | -0.034 |  0.040 |  0.018 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.bias
 | -0.002 | -0.065 |  0.073 |  0.022 | torch.Size([225, 6]) || layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.121 |  0.109 |  0.027 | torch.Size([540, 180]) || layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 | -0.050 |  0.048 |  0.019 | torch.Size([540]) || layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.112 |  0.104 |  0.025 | torch.Size([180, 180]) || layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 | -0.041 |  0.040 |  0.023 | torch.Size([180]) || layers.0.residual_group.blocks.2.attn.proj.bias
 |  0.981 |  0.954 |  1.021 |  0.013 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.weight
 |  0.002 | -0.046 |  0.056 |  0.019 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.098 |  0.105 |  0.024 | torch.Size([360, 180]) || layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.001 | -0.025 |  0.041 |  0.012 | torch.Size([360]) || layers.0.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.108 |  0.100 |  0.024 | torch.Size([180, 360]) || layers.0.residual_group.blocks.2.mlp.fc2.weight
 | -0.000 | -0.041 |  0.040 |  0.023 | torch.Size([180]) || layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.3.attn_mask
 |  0.999 |  0.959 |  1.038 |  0.016 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.weight
 | -0.001 | -0.039 |  0.039 |  0.020 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.bias
 | -0.002 | -0.081 |  0.081 |  0.023 | torch.Size([225, 6]) || layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.112 |  0.116 |  0.027 | torch.Size([540, 180]) || layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 | -0.040 |  0.043 |  0.020 | torch.Size([540]) || layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.091 |  0.099 |  0.025 | torch.Size([180, 180]) || layers.0.residual_group.blocks.3.attn.proj.weight
 | -0.000 | -0.044 |  0.040 |  0.025 | torch.Size([180]) || layers.0.residual_group.blocks.3.attn.proj.bias
 |  0.983 |  0.952 |  1.035 |  0.014 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.weight
 |  0.001 | -0.046 |  0.042 |  0.018 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.111 |  0.127 |  0.024 | torch.Size([360, 180]) || layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 | -0.028 |  0.032 |  0.011 | torch.Size([360]) || layers.0.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.100 |  0.103 |  0.024 | torch.Size([180, 360]) || layers.0.residual_group.blocks.3.mlp.fc2.weight
 | -0.000 | -0.042 |  0.040 |  0.025 | torch.Size([180]) || layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  0.990 |  0.961 |  1.026 |  0.013 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.weight
 |  0.000 | -0.033 |  0.034 |  0.016 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.065 |  0.093 |  0.022 | torch.Size([225, 6]) || layers.0.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.108 |  0.111 |  0.025 | torch.Size([540, 180]) || layers.0.residual_group.blocks.4.attn.qkv.weight
 | -0.000 | -0.040 |  0.043 |  0.017 | torch.Size([540]) || layers.0.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.106 |  0.094 |  0.024 | torch.Size([180, 180]) || layers.0.residual_group.blocks.4.attn.proj.weight
 | -0.001 | -0.041 |  0.040 |  0.026 | torch.Size([180]) || layers.0.residual_group.blocks.4.attn.proj.bias
 |  0.984 |  0.952 |  1.028 |  0.014 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.weight
 | -0.001 | -0.041 |  0.045 |  0.017 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.101 |  0.131 |  0.024 | torch.Size([360, 180]) || layers.0.residual_group.blocks.4.mlp.fc1.weight
 |  0.001 | -0.040 |  0.041 |  0.014 | torch.Size([360]) || layers.0.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.101 |  0.121 |  0.024 | torch.Size([180, 360]) || layers.0.residual_group.blocks.4.mlp.fc2.weight
 | -0.001 | -0.040 |  0.040 |  0.026 | torch.Size([180]) || layers.0.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.5.attn_mask
 |  0.994 |  0.956 |  1.029 |  0.015 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.weight
 | -0.001 | -0.045 |  0.044 |  0.018 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.092 |  0.087 |  0.025 | torch.Size([225, 6]) || layers.0.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.103 |  0.104 |  0.025 | torch.Size([540, 180]) || layers.0.residual_group.blocks.5.attn.qkv.weight
 |  0.000 | -0.040 |  0.042 |  0.017 | torch.Size([540]) || layers.0.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.108 |  0.100 |  0.024 | torch.Size([180, 180]) || layers.0.residual_group.blocks.5.attn.proj.weight
 | -0.001 | -0.041 |  0.042 |  0.027 | torch.Size([180]) || layers.0.residual_group.blocks.5.attn.proj.bias
 |  0.989 |  0.960 |  1.033 |  0.014 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.weight
 | -0.000 | -0.043 |  0.045 |  0.021 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.114 |  0.111 |  0.025 | torch.Size([360, 180]) || layers.0.residual_group.blocks.5.mlp.fc1.weight
 |  0.001 | -0.039 |  0.037 |  0.014 | torch.Size([360]) || layers.0.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.117 |  0.094 |  0.024 | torch.Size([180, 360]) || layers.0.residual_group.blocks.5.mlp.fc2.weight
 | -0.000 | -0.041 |  0.043 |  0.028 | torch.Size([180]) || layers.0.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.103 |  0.092 |  0.022 | torch.Size([180, 180, 3, 3]) || layers.0.conv.weight
 |  0.003 | -0.060 |  0.061 |  0.030 | torch.Size([180]) || layers.0.conv.bias
 |  0.997 |  0.968 |  1.023 |  0.013 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.weight
 | -0.000 | -0.038 |  0.037 |  0.022 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.064 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.103 |  0.111 |  0.023 | torch.Size([540, 180]) || layers.1.residual_group.blocks.0.attn.qkv.weight
 | -0.000 | -0.040 |  0.038 |  0.016 | torch.Size([540]) || layers.1.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.092 |  0.103 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 | -0.036 |  0.036 |  0.022 | torch.Size([180]) || layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.001 |  0.966 |  1.042 |  0.014 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.weight
 |  0.002 | -0.041 |  0.040 |  0.019 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.096 |  0.106 |  0.025 | torch.Size([360, 180]) || layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.004 | -0.034 |  0.042 |  0.018 | torch.Size([360]) || layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.096 |  0.106 |  0.024 | torch.Size([180, 360]) || layers.1.residual_group.blocks.0.mlp.fc2.weight
 | -0.000 | -0.043 |  0.043 |  0.026 | torch.Size([180]) || layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.1.attn_mask
 |  1.003 |  0.958 |  1.032 |  0.016 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 | -0.047 |  0.044 |  0.027 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.bias
 |  0.001 | -0.071 |  0.078 |  0.022 | torch.Size([225, 6]) || layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.110 |  0.108 |  0.025 | torch.Size([540, 180]) || layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 | -0.044 |  0.048 |  0.021 | torch.Size([540]) || layers.1.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.097 |  0.097 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 | -0.044 |  0.043 |  0.027 | torch.Size([180]) || layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.002 |  0.965 |  1.039 |  0.015 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.weight
 |  0.001 | -0.045 |  0.045 |  0.019 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.109 |  0.107 |  0.025 | torch.Size([360, 180]) || layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.004 | -0.033 |  0.042 |  0.018 | torch.Size([360]) || layers.1.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.095 |  0.099 |  0.024 | torch.Size([180, 360]) || layers.1.residual_group.blocks.1.mlp.fc2.weight
 | -0.000 | -0.044 |  0.043 |  0.027 | torch.Size([180]) || layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  0.996 |  0.958 |  1.026 |  0.013 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.weight
 |  0.001 | -0.038 |  0.043 |  0.022 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.bias
 |  0.001 | -0.071 |  0.063 |  0.021 | torch.Size([225, 6]) || layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.100 |  0.097 |  0.023 | torch.Size([540, 180]) || layers.1.residual_group.blocks.2.attn.qkv.weight
 | -0.001 | -0.045 |  0.050 |  0.016 | torch.Size([540]) || layers.1.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.095 |  0.087 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.2.attn.proj.weight
 | -0.000 | -0.044 |  0.044 |  0.028 | torch.Size([180]) || layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.006 |  0.961 |  1.039 |  0.016 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.weight
 |  0.001 | -0.045 |  0.048 |  0.024 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.108 |  0.104 |  0.025 | torch.Size([360, 180]) || layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.009 | -0.042 |  0.044 |  0.019 | torch.Size([360]) || layers.1.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.097 |  0.102 |  0.025 | torch.Size([180, 360]) || layers.1.residual_group.blocks.2.mlp.fc2.weight
 | -0.000 | -0.044 |  0.044 |  0.028 | torch.Size([180]) || layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.3.attn_mask
 |  1.001 |  0.968 |  1.030 |  0.012 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.weight
 |  0.001 | -0.039 |  0.038 |  0.020 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.065 |  0.077 |  0.021 | torch.Size([225, 6]) || layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.104 |  0.101 |  0.025 | torch.Size([540, 180]) || layers.1.residual_group.blocks.3.attn.qkv.weight
 | -0.001 | -0.041 |  0.040 |  0.017 | torch.Size([540]) || layers.1.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.119 |  0.095 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.3.attn.proj.weight
 | -0.001 | -0.039 |  0.038 |  0.024 | torch.Size([180]) || layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.008 |  0.966 |  1.055 |  0.015 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.weight
 |  0.001 | -0.040 |  0.043 |  0.021 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.108 |  0.104 |  0.025 | torch.Size([360, 180]) || layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.008 | -0.033 |  0.043 |  0.018 | torch.Size([360]) || layers.1.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.103 |  0.108 |  0.024 | torch.Size([180, 360]) || layers.1.residual_group.blocks.3.mlp.fc2.weight
 | -0.000 | -0.044 |  0.044 |  0.028 | torch.Size([180]) || layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  0.999 |  0.966 |  1.030 |  0.015 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.weight
 | -0.000 | -0.043 |  0.045 |  0.027 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.067 |  0.077 |  0.022 | torch.Size([225, 6]) || layers.1.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.099 |  0.097 |  0.024 | torch.Size([540, 180]) || layers.1.residual_group.blocks.4.attn.qkv.weight
 | -0.001 | -0.043 |  0.044 |  0.018 | torch.Size([540]) || layers.1.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.099 |  0.097 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.4.attn.proj.weight
 | -0.000 | -0.044 |  0.041 |  0.028 | torch.Size([180]) || layers.1.residual_group.blocks.4.attn.proj.bias
 |  1.008 |  0.968 |  1.052 |  0.015 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.weight
 | -0.000 | -0.040 |  0.042 |  0.022 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.099 |  0.101 |  0.025 | torch.Size([360, 180]) || layers.1.residual_group.blocks.4.mlp.fc1.weight
 |  0.009 | -0.035 |  0.043 |  0.018 | torch.Size([360]) || layers.1.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.110 |  0.103 |  0.024 | torch.Size([180, 360]) || layers.1.residual_group.blocks.4.mlp.fc2.weight
 | -0.000 | -0.046 |  0.044 |  0.030 | torch.Size([180]) || layers.1.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.5.attn_mask
 |  1.003 |  0.961 |  1.043 |  0.015 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.weight
 |  0.001 | -0.049 |  0.048 |  0.025 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.bias
 |  0.001 | -0.078 |  0.073 |  0.022 | torch.Size([225, 6]) || layers.1.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.103 |  0.099 |  0.024 | torch.Size([540, 180]) || layers.1.residual_group.blocks.5.attn.qkv.weight
 | -0.001 | -0.045 |  0.045 |  0.019 | torch.Size([540]) || layers.1.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.097 |  0.091 |  0.024 | torch.Size([180, 180]) || layers.1.residual_group.blocks.5.attn.proj.weight
 | -0.000 | -0.045 |  0.045 |  0.030 | torch.Size([180]) || layers.1.residual_group.blocks.5.attn.proj.bias
 |  1.011 |  0.967 |  1.049 |  0.016 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.weight
 |  0.002 | -0.042 |  0.044 |  0.022 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.107 |  0.108 |  0.026 | torch.Size([360, 180]) || layers.1.residual_group.blocks.5.mlp.fc1.weight
 |  0.010 | -0.040 |  0.044 |  0.018 | torch.Size([360]) || layers.1.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.099 |  0.097 |  0.024 | torch.Size([180, 360]) || layers.1.residual_group.blocks.5.mlp.fc2.weight
 | -0.000 | -0.045 |  0.047 |  0.030 | torch.Size([180]) || layers.1.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.110 |  0.132 |  0.022 | torch.Size([180, 180, 3, 3]) || layers.1.conv.weight
 | -0.000 | -0.055 |  0.058 |  0.028 | torch.Size([180]) || layers.1.conv.bias
 |  0.996 |  0.977 |  1.012 |  0.007 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.weight
 | -0.000 | -0.032 |  0.029 |  0.013 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.bias
 |  0.001 | -0.070 |  0.072 |  0.021 | torch.Size([225, 6]) || layers.2.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.094 |  0.097 |  0.021 | torch.Size([540, 180]) || layers.2.residual_group.blocks.0.attn.qkv.weight
 |  0.001 | -0.028 |  0.030 |  0.011 | torch.Size([540]) || layers.2.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.094 |  0.092 |  0.022 | torch.Size([180, 180]) || layers.2.residual_group.blocks.0.attn.proj.weight
 | -0.001 | -0.032 |  0.032 |  0.021 | torch.Size([180]) || layers.2.residual_group.blocks.0.attn.proj.bias
 |  1.011 |  0.972 |  1.042 |  0.015 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.weight
 | -0.000 | -0.042 |  0.043 |  0.019 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.122 |  0.109 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.0.mlp.fc1.weight
 |  0.009 | -0.034 |  0.050 |  0.019 | torch.Size([360]) || layers.2.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.087 |  0.106 |  0.022 | torch.Size([180, 360]) || layers.2.residual_group.blocks.0.mlp.fc2.weight
 | -0.001 | -0.040 |  0.040 |  0.025 | torch.Size([180]) || layers.2.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.1.attn_mask
 |  0.997 |  0.968 |  1.025 |  0.010 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.weight
 |  0.001 | -0.040 |  0.039 |  0.020 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.bias
 | -0.001 | -0.063 |  0.071 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.099 |  0.088 |  0.022 | torch.Size([540, 180]) || layers.2.residual_group.blocks.1.attn.qkv.weight
 |  0.000 | -0.040 |  0.042 |  0.016 | torch.Size([540]) || layers.2.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.085 |  0.093 |  0.022 | torch.Size([180, 180]) || layers.2.residual_group.blocks.1.attn.proj.weight
 | -0.001 | -0.038 |  0.039 |  0.025 | torch.Size([180]) || layers.2.residual_group.blocks.1.attn.proj.bias
 |  1.011 |  0.977 |  1.045 |  0.014 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.weight
 |  0.001 | -0.045 |  0.042 |  0.022 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.103 |  0.103 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.1.mlp.fc1.weight
 |  0.007 | -0.039 |  0.045 |  0.021 | torch.Size([360]) || layers.2.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.102 |  0.093 |  0.022 | torch.Size([180, 360]) || layers.2.residual_group.blocks.1.mlp.fc2.weight
 | -0.001 | -0.038 |  0.039 |  0.025 | torch.Size([180]) || layers.2.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.978 |  1.026 |  0.010 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.weight
 |  0.000 | -0.036 |  0.036 |  0.018 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.bias
 | -0.001 | -0.063 |  0.072 |  0.021 | torch.Size([225, 6]) || layers.2.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.096 |  0.091 |  0.022 | torch.Size([540, 180]) || layers.2.residual_group.blocks.2.attn.qkv.weight
 | -0.001 | -0.037 |  0.037 |  0.015 | torch.Size([540]) || layers.2.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.089 |  0.093 |  0.023 | torch.Size([180, 180]) || layers.2.residual_group.blocks.2.attn.proj.weight
 | -0.001 | -0.036 |  0.037 |  0.024 | torch.Size([180]) || layers.2.residual_group.blocks.2.attn.proj.bias
 |  1.010 |  0.975 |  1.041 |  0.013 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.weight
 | -0.000 | -0.041 |  0.041 |  0.018 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.098 |  0.125 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.2.mlp.fc1.weight
 |  0.009 | -0.040 |  0.042 |  0.021 | torch.Size([360]) || layers.2.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.089 |  0.106 |  0.023 | torch.Size([180, 360]) || layers.2.residual_group.blocks.2.mlp.fc2.weight
 | -0.001 | -0.038 |  0.038 |  0.026 | torch.Size([180]) || layers.2.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.3.attn_mask
 |  0.997 |  0.971 |  1.030 |  0.010 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.weight
 |  0.000 | -0.045 |  0.044 |  0.021 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.075 |  0.066 |  0.021 | torch.Size([225, 6]) || layers.2.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.098 |  0.100 |  0.022 | torch.Size([540, 180]) || layers.2.residual_group.blocks.3.attn.qkv.weight
 |  0.000 | -0.041 |  0.039 |  0.016 | torch.Size([540]) || layers.2.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.095 |  0.089 |  0.022 | torch.Size([180, 180]) || layers.2.residual_group.blocks.3.attn.proj.weight
 | -0.001 | -0.039 |  0.038 |  0.026 | torch.Size([180]) || layers.2.residual_group.blocks.3.attn.proj.bias
 |  1.010 |  0.971 |  1.038 |  0.014 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.weight
 | -0.000 | -0.047 |  0.044 |  0.019 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.104 |  0.097 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.3.mlp.fc1.weight
 |  0.007 | -0.037 |  0.042 |  0.022 | torch.Size([360]) || layers.2.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.104 |  0.096 |  0.023 | torch.Size([180, 360]) || layers.2.residual_group.blocks.3.mlp.fc2.weight
 | -0.001 | -0.038 |  0.037 |  0.026 | torch.Size([180]) || layers.2.residual_group.blocks.3.mlp.fc2.bias
 |  0.998 |  0.973 |  1.025 |  0.010 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.weight
 |  0.001 | -0.048 |  0.045 |  0.023 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.073 |  0.080 |  0.022 | torch.Size([225, 6]) || layers.2.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.098 |  0.098 |  0.022 | torch.Size([540, 180]) || layers.2.residual_group.blocks.4.attn.qkv.weight
 |  0.000 | -0.045 |  0.041 |  0.017 | torch.Size([540]) || layers.2.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.088 |  0.088 |  0.022 | torch.Size([180, 180]) || layers.2.residual_group.blocks.4.attn.proj.weight
 | -0.001 | -0.039 |  0.038 |  0.026 | torch.Size([180]) || layers.2.residual_group.blocks.4.attn.proj.bias
 |  1.009 |  0.978 |  1.040 |  0.012 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.weight
 | -0.001 | -0.038 |  0.038 |  0.019 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.107 |  0.102 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.4.mlp.fc1.weight
 |  0.008 | -0.040 |  0.041 |  0.021 | torch.Size([360]) || layers.2.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.105 |  0.098 |  0.023 | torch.Size([180, 360]) || layers.2.residual_group.blocks.4.mlp.fc2.weight
 | -0.001 | -0.039 |  0.038 |  0.026 | torch.Size([180]) || layers.2.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.5.attn_mask
 |  0.996 |  0.973 |  1.016 |  0.009 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.weight
 |  0.000 | -0.031 |  0.037 |  0.014 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.bias
 |  0.001 | -0.067 |  0.075 |  0.022 | torch.Size([225, 6]) || layers.2.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.095 |  0.088 |  0.022 | torch.Size([540, 180]) || layers.2.residual_group.blocks.5.attn.qkv.weight
 |  0.000 | -0.036 |  0.033 |  0.013 | torch.Size([540]) || layers.2.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.078 |  0.080 |  0.021 | torch.Size([180, 180]) || layers.2.residual_group.blocks.5.attn.proj.weight
 | -0.001 | -0.032 |  0.032 |  0.022 | torch.Size([180]) || layers.2.residual_group.blocks.5.attn.proj.bias
 |  1.010 |  0.978 |  1.039 |  0.014 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.weight
 | -0.000 | -0.045 |  0.041 |  0.021 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.100 |  0.113 |  0.024 | torch.Size([360, 180]) || layers.2.residual_group.blocks.5.mlp.fc1.weight
 |  0.009 | -0.040 |  0.046 |  0.023 | torch.Size([360]) || layers.2.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.102 |  0.090 |  0.023 | torch.Size([180, 360]) || layers.2.residual_group.blocks.5.mlp.fc2.weight
 | -0.001 | -0.038 |  0.038 |  0.027 | torch.Size([180]) || layers.2.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.200 |  0.207 |  0.024 | torch.Size([180, 180, 3, 3]) || layers.2.conv.weight
 | -0.000 | -0.048 |  0.049 |  0.020 | torch.Size([180]) || layers.2.conv.bias
 |  1.000 |  0.984 |  1.020 |  0.006 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.weight
 | -0.001 | -0.025 |  0.029 |  0.015 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.062 |  0.068 |  0.021 | torch.Size([225, 6]) || layers.3.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.089 |  0.086 |  0.021 | torch.Size([540, 180]) || layers.3.residual_group.blocks.0.attn.qkv.weight
 | -0.000 | -0.028 |  0.027 |  0.011 | torch.Size([540]) || layers.3.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.091 |  0.093 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.0.attn.proj.weight
 |  0.000 | -0.025 |  0.027 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.0.attn.proj.bias
 |  1.005 |  0.989 |  1.030 |  0.008 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.weight
 |  0.001 | -0.029 |  0.031 |  0.016 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.093 |  0.100 |  0.021 | torch.Size([360, 180]) || layers.3.residual_group.blocks.0.mlp.fc1.weight
 |  0.004 | -0.022 |  0.030 |  0.013 | torch.Size([360]) || layers.3.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.090 |  0.098 |  0.021 | torch.Size([180, 360]) || layers.3.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 | -0.027 |  0.027 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.1.attn_mask
 |  1.001 |  0.992 |  1.019 |  0.005 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.weight
 | -0.000 | -0.028 |  0.030 |  0.015 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.069 |  0.063 |  0.021 | torch.Size([225, 6]) || layers.3.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.101 |  0.095 |  0.021 | torch.Size([540, 180]) || layers.3.residual_group.blocks.1.attn.qkv.weight
 | -0.000 | -0.029 |  0.028 |  0.010 | torch.Size([540]) || layers.3.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.087 |  0.086 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.1.attn.proj.weight
 |  0.000 | -0.026 |  0.027 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.1.attn.proj.bias
 |  1.004 |  0.992 |  1.026 |  0.007 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.weight
 |  0.000 | -0.024 |  0.027 |  0.012 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.086 |  0.089 |  0.021 | torch.Size([360, 180]) || layers.3.residual_group.blocks.1.mlp.fc1.weight
 |  0.004 | -0.017 |  0.024 |  0.009 | torch.Size([360]) || layers.3.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.093 |  0.090 |  0.021 | torch.Size([180, 360]) || layers.3.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 | -0.021 |  0.022 |  0.013 | torch.Size([180]) || layers.3.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.990 |  1.017 |  0.005 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.weight
 |  0.000 | -0.030 |  0.029 |  0.015 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.060 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.090 |  0.100 |  0.021 | torch.Size([540, 180]) || layers.3.residual_group.blocks.2.attn.qkv.weight
 |  0.001 | -0.027 |  0.028 |  0.010 | torch.Size([540]) || layers.3.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.094 |  0.077 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.2.attn.proj.weight
 |  0.001 | -0.027 |  0.028 |  0.018 | torch.Size([180]) || layers.3.residual_group.blocks.2.attn.proj.bias
 |  1.005 |  0.987 |  1.025 |  0.008 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.weight
 | -0.000 | -0.031 |  0.034 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.100 |  0.087 |  0.021 | torch.Size([360, 180]) || layers.3.residual_group.blocks.2.mlp.fc1.weight
 |  0.004 | -0.022 |  0.029 |  0.013 | torch.Size([360]) || layers.3.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.096 |  0.095 |  0.021 | torch.Size([180, 360]) || layers.3.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 | -0.026 |  0.026 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.3.attn_mask
 |  1.001 |  0.986 |  1.018 |  0.005 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.weight
 | -0.000 | -0.028 |  0.024 |  0.014 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.072 |  0.071 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.099 |  0.092 |  0.022 | torch.Size([540, 180]) || layers.3.residual_group.blocks.3.attn.qkv.weight
 | -0.000 | -0.026 |  0.024 |  0.010 | torch.Size([540]) || layers.3.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.082 |  0.086 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.3.attn.proj.weight
 |  0.000 | -0.021 |  0.024 |  0.014 | torch.Size([180]) || layers.3.residual_group.blocks.3.attn.proj.bias
 |  1.003 |  0.990 |  1.016 |  0.006 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.weight
 |  0.001 | -0.023 |  0.021 |  0.012 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.084 |  0.089 |  0.021 | torch.Size([360, 180]) || layers.3.residual_group.blocks.3.mlp.fc1.weight
 |  0.002 | -0.020 |  0.020 |  0.010 | torch.Size([360]) || layers.3.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.102 |  0.080 |  0.021 | torch.Size([180, 360]) || layers.3.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 | -0.020 |  0.019 |  0.013 | torch.Size([180]) || layers.3.residual_group.blocks.3.mlp.fc2.bias
 |  1.001 |  0.985 |  1.018 |  0.007 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.weight
 |  0.000 | -0.030 |  0.026 |  0.016 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.062 |  0.071 |  0.021 | torch.Size([225, 6]) || layers.3.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.087 |  0.090 |  0.021 | torch.Size([540, 180]) || layers.3.residual_group.blocks.4.attn.qkv.weight
 | -0.000 | -0.028 |  0.027 |  0.011 | torch.Size([540]) || layers.3.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.097 |  0.081 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.4.attn.proj.weight
 |  0.001 | -0.026 |  0.026 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.4.attn.proj.bias
 |  1.005 |  0.986 |  1.022 |  0.007 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.weight
 |  0.001 | -0.026 |  0.025 |  0.014 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.089 |  0.096 |  0.021 | torch.Size([360, 180]) || layers.3.residual_group.blocks.4.mlp.fc1.weight
 |  0.003 | -0.020 |  0.027 |  0.012 | torch.Size([360]) || layers.3.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.101 |  0.099 |  0.021 | torch.Size([180, 360]) || layers.3.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 | -0.022 |  0.023 |  0.015 | torch.Size([180]) || layers.3.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.5.attn_mask
 |  1.000 |  0.987 |  1.017 |  0.006 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.weight
 |  0.000 | -0.026 |  0.028 |  0.014 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.070 |  0.074 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.084 |  0.094 |  0.021 | torch.Size([540, 180]) || layers.3.residual_group.blocks.5.attn.qkv.weight
 | -0.000 | -0.027 |  0.031 |  0.011 | torch.Size([540]) || layers.3.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.092 |  0.080 |  0.021 | torch.Size([180, 180]) || layers.3.residual_group.blocks.5.attn.proj.weight
 |  0.000 | -0.025 |  0.026 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.5.attn.proj.bias
 |  1.006 |  0.987 |  1.029 |  0.009 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.weight
 | -0.001 | -0.028 |  0.030 |  0.017 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.104 |  0.091 |  0.022 | torch.Size([360, 180]) || layers.3.residual_group.blocks.5.mlp.fc1.weight
 |  0.004 | -0.024 |  0.034 |  0.014 | torch.Size([360]) || layers.3.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.091 |  0.089 |  0.022 | torch.Size([180, 360]) || layers.3.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 | -0.027 |  0.027 |  0.018 | torch.Size([180]) || layers.3.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.432 |  0.456 |  0.031 | torch.Size([180, 180, 3, 3]) || layers.3.conv.weight
 | -0.000 | -0.028 |  0.031 |  0.015 | torch.Size([180]) || layers.3.conv.bias
 |  1.000 |  0.992 |  1.013 |  0.003 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.weight
 | -0.000 | -0.012 |  0.014 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.062 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.089 |  0.090 |  0.021 | torch.Size([540, 180]) || layers.4.residual_group.blocks.0.attn.qkv.weight
 |  0.000 | -0.013 |  0.013 |  0.004 | torch.Size([540]) || layers.4.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.078 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.0.attn.proj.weight
 | -0.000 | -0.013 |  0.012 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.0.attn.proj.bias
 |  1.001 |  0.992 |  1.014 |  0.004 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.weight
 | -0.001 | -0.017 |  0.012 |  0.007 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.093 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.0.mlp.fc1.weight
 |  0.001 | -0.009 |  0.015 |  0.005 | torch.Size([360]) || layers.4.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.095 |  0.090 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.0.mlp.fc2.weight
 | -0.000 | -0.013 |  0.012 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.1.attn_mask
 |  1.000 |  0.991 |  1.011 |  0.004 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.weight
 | -0.000 | -0.014 |  0.013 |  0.007 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.080 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.092 |  0.097 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.1.attn.qkv.weight
 | -0.000 | -0.017 |  0.016 |  0.005 | torch.Size([540]) || layers.4.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.091 |  0.084 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.1.attn.proj.weight
 |  0.000 | -0.011 |  0.011 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.1.attn.proj.bias
 |  1.002 |  0.990 |  1.021 |  0.004 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.weight
 | -0.000 | -0.014 |  0.015 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.095 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.1.mlp.fc1.weight
 |  0.001 | -0.009 |  0.016 |  0.005 | torch.Size([360]) || layers.4.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.088 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.1.mlp.fc2.weight
 | -0.000 | -0.012 |  0.011 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.992 |  1.008 |  0.003 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.weight
 | -0.000 | -0.013 |  0.011 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.075 |  0.068 |  0.021 | torch.Size([225, 6]) || layers.4.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.095 |  0.097 |  0.021 | torch.Size([540, 180]) || layers.4.residual_group.blocks.2.attn.qkv.weight
 |  0.000 | -0.012 |  0.015 |  0.004 | torch.Size([540]) || layers.4.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.079 |  0.090 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.2.attn.proj.weight
 | -0.000 | -0.013 |  0.011 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.2.attn.proj.bias
 |  1.003 |  0.989 |  1.019 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.weight
 |  0.000 | -0.016 |  0.017 |  0.008 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.087 |  0.088 |  0.021 | torch.Size([360, 180]) || layers.4.residual_group.blocks.2.mlp.fc1.weight
 |  0.001 | -0.010 |  0.021 |  0.006 | torch.Size([360]) || layers.4.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.089 |  0.099 |  0.021 | torch.Size([180, 360]) || layers.4.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 | -0.012 |  0.012 |  0.007 | torch.Size([180]) || layers.4.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.3.attn_mask
 |  1.001 |  0.992 |  1.011 |  0.003 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.weight
 | -0.000 | -0.013 |  0.014 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.088 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.102 |  0.091 |  0.021 | torch.Size([540, 180]) || layers.4.residual_group.blocks.3.attn.qkv.weight
 | -0.000 | -0.013 |  0.015 |  0.005 | torch.Size([540]) || layers.4.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.084 |  0.086 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.3.attn.proj.weight
 | -0.000 | -0.012 |  0.011 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.3.attn.proj.bias
 |  1.003 |  0.994 |  1.026 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.weight
 | -0.000 | -0.016 |  0.015 |  0.007 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.089 |  0.082 |  0.021 | torch.Size([360, 180]) || layers.4.residual_group.blocks.3.mlp.fc1.weight
 |  0.001 | -0.010 |  0.019 |  0.005 | torch.Size([360]) || layers.4.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.091 |  0.096 |  0.021 | torch.Size([180, 360]) || layers.4.residual_group.blocks.3.mlp.fc2.weight
 | -0.000 | -0.012 |  0.010 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.3.mlp.fc2.bias
 |  1.001 |  0.994 |  1.009 |  0.003 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.weight
 |  0.000 | -0.011 |  0.014 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.bias
 |  0.000 | -0.072 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.088 |  0.091 |  0.021 | torch.Size([540, 180]) || layers.4.residual_group.blocks.4.attn.qkv.weight
 | -0.001 | -0.012 |  0.009 |  0.004 | torch.Size([540]) || layers.4.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.083 |  0.087 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.4.attn.proj.weight
 | -0.000 | -0.013 |  0.012 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.4.attn.proj.bias
 |  1.003 |  0.993 |  1.018 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.weight
 | -0.000 | -0.018 |  0.016 |  0.008 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.086 |  0.078 |  0.021 | torch.Size([360, 180]) || layers.4.residual_group.blocks.4.mlp.fc1.weight
 |  0.002 | -0.009 |  0.016 |  0.005 | torch.Size([360]) || layers.4.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.088 |  0.087 |  0.021 | torch.Size([180, 360]) || layers.4.residual_group.blocks.4.mlp.fc2.weight
 | -0.000 | -0.012 |  0.011 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.5.attn_mask
 |  1.000 |  0.991 |  1.011 |  0.003 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.weight
 | -0.000 | -0.010 |  0.008 |  0.004 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.062 |  0.053 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.082 |  0.091 |  0.021 | torch.Size([540, 180]) || layers.4.residual_group.blocks.5.attn.qkv.weight
 | -0.000 | -0.013 |  0.014 |  0.005 | torch.Size([540]) || layers.4.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.085 |  0.097 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.5.attn.proj.weight
 | -0.000 | -0.010 |  0.011 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.5.attn.proj.bias
 |  1.002 |  0.990 |  1.021 |  0.005 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.weight
 | -0.001 | -0.016 |  0.013 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.092 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.5.mlp.fc1.weight
 |  0.001 | -0.016 |  0.016 |  0.005 | torch.Size([360]) || layers.4.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.088 |  0.086 |  0.021 | torch.Size([180, 360]) || layers.4.residual_group.blocks.5.mlp.fc2.weight
 | -0.000 | -0.014 |  0.012 |  0.006 | torch.Size([180]) || layers.4.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.749 |  0.758 |  0.043 | torch.Size([180, 180, 3, 3]) || layers.4.conv.weight
 |  0.001 | -0.028 |  0.031 |  0.016 | torch.Size([180]) || layers.4.conv.bias
 |  1.001 |  0.995 |  1.009 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.weight
 | -0.000 | -0.012 |  0.014 |  0.004 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.065 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.094 |  0.090 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.0.attn.qkv.weight
 |  0.000 | -0.014 |  0.013 |  0.004 | torch.Size([540]) || layers.5.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.083 |  0.089 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.0.attn.proj.weight
 |  0.000 | -0.007 |  0.008 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.0.attn.proj.bias
 |  1.001 |  0.994 |  1.011 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.weight
 |  0.000 | -0.006 |  0.004 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.098 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 | -0.007 |  0.012 |  0.002 | torch.Size([360]) || layers.5.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.084 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 | -0.005 |  0.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.1.attn_mask
 |  1.001 |  0.996 |  1.015 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.weight
 |  0.000 | -0.010 |  0.010 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.060 |  0.066 |  0.021 | torch.Size([225, 6]) || layers.5.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.088 |  0.083 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.1.attn.qkv.weight
 | -0.000 | -0.013 |  0.010 |  0.003 | torch.Size([540]) || layers.5.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.076 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.1.attn.proj.weight
 |  0.000 | -0.007 |  0.007 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.1.attn.proj.bias
 |  1.003 |  0.995 |  1.023 |  0.004 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.weight
 | -0.000 | -0.011 |  0.012 |  0.004 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.087 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.1.mlp.fc1.weight
 |  0.001 | -0.007 |  0.019 |  0.003 | torch.Size([360]) || layers.5.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.081 |  0.091 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 | -0.006 |  0.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  0.994 |  1.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.weight
 | -0.000 | -0.008 |  0.005 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.bias
 | -0.001 | -0.082 |  0.060 |  0.021 | torch.Size([225, 6]) || layers.5.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.097 |  0.089 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.2.attn.qkv.weight
 | -0.000 | -0.009 |  0.009 |  0.002 | torch.Size([540]) || layers.5.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.080 |  0.087 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.2.attn.proj.weight
 |  0.000 | -0.007 |  0.008 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.2.attn.proj.bias
 |  1.002 |  0.994 |  1.013 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.weight
 | -0.000 | -0.008 |  0.011 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.092 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.2.mlp.fc1.weight
 |  0.001 | -0.008 |  0.013 |  0.003 | torch.Size([360]) || layers.5.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.087 |  0.094 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 | -0.006 |  0.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.3.attn_mask
 |  1.000 |  0.995 |  1.009 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.weight
 | -0.000 | -0.005 |  0.008 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.bias
 |  0.001 | -0.059 |  0.074 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.095 |  0.092 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.3.attn.qkv.weight
 |  0.000 | -0.018 |  0.019 |  0.004 | torch.Size([540]) || layers.5.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.076 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.3.attn.proj.weight
 |  0.000 | -0.006 |  0.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.3.attn.proj.bias
 |  1.002 |  0.992 |  1.019 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.weight
 | -0.000 | -0.009 |  0.009 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.086 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.3.mlp.fc1.weight
 |  0.001 | -0.006 |  0.015 |  0.003 | torch.Size([360]) || layers.5.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.105 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  0.997 |  1.009 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.weight
 | -0.000 | -0.007 |  0.010 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.065 |  0.065 |  0.021 | torch.Size([225, 6]) || layers.5.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.085 |  0.093 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.4.attn.qkv.weight
 | -0.000 | -0.010 |  0.010 |  0.003 | torch.Size([540]) || layers.5.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.085 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.4.attn.proj.weight
 |  0.000 | -0.005 |  0.006 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.4.attn.proj.bias
 |  1.004 |  0.993 |  1.021 |  0.005 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.weight
 | -0.000 | -0.016 |  0.016 |  0.006 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.095 |  0.094 |  0.021 | torch.Size([360, 180]) || layers.5.residual_group.blocks.4.mlp.fc1.weight
 |  0.002 | -0.010 |  0.025 |  0.005 | torch.Size([360]) || layers.5.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.083 |  0.090 |  0.021 | torch.Size([180, 360]) || layers.5.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 | -0.008 |  0.009 |  0.003 | torch.Size([180]) || layers.5.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.5.attn_mask
 |  1.000 |  0.995 |  1.008 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.weight
 |  0.000 | -0.007 |  0.007 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.065 |  0.068 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.085 |  0.104 |  0.021 | torch.Size([540, 180]) || layers.5.residual_group.blocks.5.attn.qkv.weight
 |  0.000 | -0.014 |  0.014 |  0.004 | torch.Size([540]) || layers.5.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.087 |  0.091 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.5.attn.proj.weight
 |  0.000 | -0.005 |  0.005 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.5.attn.proj.bias
 |  1.002 |  0.992 |  1.018 |  0.004 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.weight
 | -0.000 | -0.009 |  0.010 |  0.004 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.087 |  0.085 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.5.mlp.fc1.weight
 |  0.001 | -0.012 |  0.019 |  0.003 | torch.Size([360]) || layers.5.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.088 |  0.096 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 | -0.006 |  0.005 |  0.002 | torch.Size([180]) || layers.5.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -1.255 |  1.261 |  0.047 | torch.Size([180, 180, 3, 3]) || layers.5.conv.weight
 |  0.001 | -0.030 |  0.026 |  0.015 | torch.Size([180]) || layers.5.conv.bias
 |  0.426 |  0.276 |  0.702 |  0.052 | torch.Size([180]) || norm.weight
 |  0.027 | -0.484 |  0.640 |  0.060 | torch.Size([180]) || norm.bias
 |  0.000 | -0.103 |  0.083 |  0.017 | torch.Size([180, 180, 3, 3]) || conv_after_body.weight
 | -0.002 | -0.042 |  0.040 |  0.016 | torch.Size([180]) || conv_after_body.bias
 |  0.000 | -0.265 |  0.214 |  0.031 | torch.Size([64, 180, 3, 3]) || conv_before_upsample.0.weight
 | -0.034 | -0.108 |  0.025 |  0.022 | torch.Size([64]) || conv_before_upsample.0.bias
 | -0.000 | -0.325 |  0.231 |  0.033 | torch.Size([256, 64, 3, 3]) || upsample.0.weight
 | -0.002 | -0.120 |  0.111 |  0.035 | torch.Size([256]) || upsample.0.bias
 |  0.000 | -0.044 |  0.051 |  0.009 | torch.Size([1, 64, 3, 3]) || conv_last.weight
 |  0.112 |  0.112 |  0.112 |    nan | torch.Size([1]) || conv_last.bias

